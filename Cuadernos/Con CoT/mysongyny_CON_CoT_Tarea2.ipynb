{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56074,"status":"ok","timestamp":1746721737125,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"5ME2PUEUAOUs","outputId":"3a7b155f-0589-4d42-d0bf-408f0af82dc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Montar directorio de drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"14a8bgNf_ylG","executionInfo":{"status":"ok","timestamp":1746721967749,"user_tz":-120,"elapsed":230622,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"}},"outputId":"5624a678-9c53-4f94-d18e-8e441306c495"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unsloth\n","  Downloading unsloth-2025.4.7-py3-none-any.whl.metadata (46 kB)\n","\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/46.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting unsloth_zoo>=2025.4.4 (from unsloth)\n","  Downloading unsloth_zoo-2025.4.4-py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\n","Collecting xformers>=0.0.27.post2 (from unsloth)\n","  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n","Collecting bitsandbytes (from unsloth)\n","  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n","Collecting tyro (from unsloth)\n","  Downloading tyro-0.9.19-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.51.3)\n","Collecting datasets>=2.16.0 (from unsloth)\n","  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n","Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.0.2)\n","Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.6.0)\n","Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n","  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\n","Collecting protobuf<4.0.0 (from unsloth)\n","  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.30.2)\n","Collecting hf_transfer (from unsloth)\n","  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.33.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n","Collecting xxhash (from datasets>=2.16.0->unsloth)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets>=2.16.0->unsloth)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth)\n","  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n","Collecting cut_cross_entropy (from unsloth_zoo>=2025.4.4->unsloth)\n","  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.4.4->unsloth) (11.2.1)\n","Collecting msgspec (from unsloth_zoo>=2025.4.4->unsloth)\n","  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n","Collecting torch>=2.4.0 (from unsloth)\n","  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n","Collecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n","  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth)\n","  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n","  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting nvidia-nccl-cu12==2.26.2 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n","  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting triton>=3.0.0 (from unsloth)\n","  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth) (75.2.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\n","INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchvision (from unsloth)\n","  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n","Collecting shtab>=1.5.6 (from tyro->unsloth)\n","  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (3.11.15)\n","Collecting hf-xet>=0.1.4 (from huggingface_hub[hf_xet]>=0.30.0->unsloth_zoo>=2025.4.4->unsloth)\n","  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.4.26)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (1.20.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n","Downloading unsloth-2025.4.7-py3-none-any.whl (218 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m218.5/218.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unsloth_zoo-2025.4.4-py3-none-any.whl (129 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tyro-0.9.19-py3-none-any.whl (124 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n","Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n","Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, xxhash, triton, sympy, shtab, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, hf-xet, hf_transfer, fsspec, dill, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, tyro, nvidia-cusolver-cu12, torch, datasets, xformers, torchvision, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n","    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.13.1\n","    Uninstalling sympy-1.13.1:\n","      Successfully uninstalled sympy-1.13.1\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.4\n","    Uninstalling protobuf-5.29.4:\n","      Successfully uninstalled protobuf-5.29.4\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.4.127\n","    Uninstalling nvidia-nvtx-cu12-12.4.127:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.21.5\n","    Uninstalling nvidia-nccl-cu12-2.21.5:\n","      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.6.0+cu124\n","    Uninstalling torch-2.6.0+cu124:\n","      Successfully uninstalled torch-2.6.0+cu124\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.21.0+cu124\n","    Uninstalling torchvision-0.21.0+cu124:\n","      Successfully uninstalled torchvision-0.21.0+cu124\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n","grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n","tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n","ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed bitsandbytes-0.45.5 cut_cross_entropy-25.1.1 datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 hf-xet-1.1.0 hf_transfer-0.1.9 msgspec-0.19.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 protobuf-3.20.3 shtab-1.7.2 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 triton-3.3.0 trl-0.15.2 tyro-0.9.19 unsloth-2025.4.7 unsloth_zoo-2025.4.4 xformers-0.0.30 xxhash-3.5.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]},"id":"831c1fa8247d4811a41e5226947bde55"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/unslothai/unsloth.git\n","  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-8779irwe\n","  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-8779irwe\n","  Resolved https://github.com/unslothai/unsloth.git to commit b9067f98d124173b354808b116f931410e5b699c\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: unsloth\n","  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unsloth: filename=unsloth-2025.4.8-py3-none-any.whl size=263211 sha256=008af0d752b8855241c659d0c045a2c46ebafb8ef0e935786395ac1e26363af2\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-jmevykry/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n","Successfully built unsloth\n","Installing collected packages: unsloth\n","  Attempting uninstall: unsloth\n","    Found existing installation: unsloth 2025.4.7\n","    Uninstalling unsloth-2025.4.7:\n","      Successfully uninstalled unsloth-2025.4.7\n","Successfully installed unsloth-2025.4.8\n"]}],"source":["!pip install unsloth\n","!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5240,"status":"ok","timestamp":1746721972988,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"dhS_9MK-2ikI","outputId":"d82b5fac-0523-407b-d9ba-bd8c95cc5a02"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-dotenv\n","  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n","Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.1.0\n"]}],"source":["!pip install python-dotenv"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3921,"status":"ok","timestamp":1746721976925,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"WfDykUE6n4u_","outputId":"4c205bba-06be-464d-a4d3-70b7a6572a62"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hay 1 GPU(s) disponibles.\n","Vamos a usar la GPU: Tesla T4\n"]}],"source":["import torch\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print('Hay %d GPU(s) disponibles.' % torch.cuda.device_count())\n","    print('Vamos a usar la GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No hay GPU disponible, usando CPU.')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49702,"status":"ok","timestamp":1746722026632,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"z1-ZXv6fBQ8l","outputId":"9215134f-5506-4cdb-8c59-b613e09105dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"]}],"source":["from unsloth import FastLanguageModel, is_bfloat16_supported\n","import torch\n","from trl import SFTTrainer\n","\n","from huggingface_hub import login\n","from transformers import TrainingArguments, EarlyStoppingCallback, AutoModelForCausalLM, AutoTokenizer\n","from datasets import load_dataset\n","import wandb\n","import pandas as pd\n","from datasets import Dataset\n","from dotenv import load_dotenv\n","import os\n","from sklearn.utils import resample\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, average_precision_score, accuracy_score, precision_recall_fscore_support, precision_score, recall_score"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Op1WzBwFJb3W","executionInfo":{"status":"ok","timestamp":1746722026633,"user_tz":-120,"elapsed":2,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"}}},"outputs":[],"source":["path = '/content/drive/Othercomputers/Mi PC/TFG/MySonGyny/MySonGyny-2025'\n","#path = '../'\n","#path = '/home/lobezno/Documentos'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RX5Y2kTD2IPR"},"outputs":[],"source":["load_dotenv(f'{path}/cuadernos/.env')  # Carga las variables del .env\n","\n","HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n","WANDB_TOKEN = os.getenv(\"WANDB_TOKEN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZpMlasSAadz"},"outputs":[],"source":["hf_token = HUGGINGFACE_TOKEN\n","login(hf_token)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213},"executionInfo":{"elapsed":4359,"status":"ok","timestamp":1746530369511,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"xcVPd0h9Ajz2","outputId":"eced1d03-8693-4cbb-b164-ecd99c43ca0f"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matorojaen\u001b[0m (\u001b[33matorojaen-university-of-huelva\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.19.10"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20250506_111927-c6f61n2p</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2/runs/c6f61n2p?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3' target=\"_blank\">charmed-energy-5</a></strong> to <a href='https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3' target=\"_blank\">https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2/runs/c6f61n2p?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3' target=\"_blank\">https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2/runs/c6f61n2p?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Do NOT share these links with anyone. They can be used to claim your runs."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["wb_token = WANDB_TOKEN\n","\n","wandb.login(key=wb_token)\n","run = wandb.init(\n","    project='Fine-tune-con-CoT-Tarea-2',\n","    job_type=\"training\",\n","    anonymous=\"allow\"\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"WEvLF7Pg_pC-","executionInfo":{"status":"ok","timestamp":1746722026635,"user_tz":-120,"elapsed":1,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"}}},"outputs":[],"source":["COLUMN_ID = 'id'\n","COLUMN_LETRA = 'lyrics'\n","COLUMN_ETIQUETA = 'label'\n","COLUNM_ETIQUETALETRA = 'labelconletras'\n","COLUMN_RAZONAMIENTO = 'reasoning'\n","tipo_CoT = 'GPT4o'\n","model_name = 'DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit'\n","csv_train_filepath = f'{path}/data/train_data/task2_GPT4o_dataReasoning.csv'\n","tipo_prompt = 0\n","#csv_test_filepath = f'{path}/data/train_data/testData.csv'"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3726,"status":"ok","timestamp":1746722030362,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"MH0L24sv_pC_","outputId":"78056c01-f433-474d-e870-d72d363cfde9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cargando datos desde: /content/drive/Othercomputers/Mi PC/TFG/MySonGyny/MySonGyny-2025/data/train_data/task2_GPT4o_dataReasoning.csv\n","Columnas encontradas en el CSV: ['id', 'lyrics', 'reasoning', 'label', 'labelconletras']\n","Total de filas vÃ¡lidas antes de dividir: 1168\n","DivisiÃ³n completada:\n","  - TamaÃ±o del conjunto de Entrenamiento: 653\n","  - TamaÃ±o del conjunto de ValidaciÃ³n:  281\n","  - TamaÃ±o del conjunto de Test:         234\n","\n","--- DistribuciÃ³n de Clases ---\n","Conjunto de Entrenamiento:\n","labelconletras\n","NR    294\n","S     243\n","V      72\n","H      44\n","Name: count, dtype: int64\n","\n","Conjunto de ValidaciÃ³n:\n","labelconletras\n","NR    127\n","S     105\n","V      31\n","H      18\n","Name: count, dtype: int64\n","\n","Conjunto de Test:\n","labelconletras\n","NR    105\n","S      87\n","V      26\n","H      16\n","Name: count, dtype: int64\n","-----------------------------\n","\n","Conjunto de Entrenamiento nuevo:\n","labelconletras\n","NR    294\n","S     243\n","V     198\n","H     156\n","Name: count, dtype: int64\n","------------------------------------------\n","\n","Datasets creados exitosamente.\n","Columnas en trainData: ['id', 'lyrics', 'reasoning', 'label', 'labelconletras']\n"]}],"source":["print(f\"Cargando datos desde: {csv_train_filepath}\")\n","try:\n","    # 1. Cargar el DataFrame separado\n","    df = pd.read_csv(csv_train_filepath, encoding='utf-8', encoding_errors=\"replace\", sep=',')\n","    #test_df = pd.read_csv(csv_test_filepath, encoding='utf-8', encoding_errors=\"replace\", sep=',')\n","    print(f\"Columnas encontradas en el CSV: {df.columns.tolist()}\")\n","\n","    # 2. Dividir los datos\n","    print(f\"Total de filas vÃ¡lidas antes de dividir: {len(df)}\")\n","    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[COLUMN_ETIQUETA])\n","    train_df, valid_df = train_test_split(train_df, test_size=0.3, random_state=42, stratify=train_df[COLUMN_ETIQUETA])\n","    print(f\"DivisiÃ³n completada:\")\n","    print(f\"  - TamaÃ±o del conjunto de Entrenamiento: {len(train_df)}\")\n","    print(f\"  - TamaÃ±o del conjunto de ValidaciÃ³n:  {len(valid_df)}\")\n","    print(f\"  - TamaÃ±o del conjunto de Test:         {len(test_df)}\")\n","\n","     # --- COMPROBACIÃ“N DE PROPORCIONES DE CLASE ---\n","    print(\"\\n--- DistribuciÃ³n de Clases ---\")\n","    print(\"Conjunto de Entrenamiento:\")\n","    print(train_df[COLUNM_ETIQUETALETRA].value_counts(normalize=False)) # Conteos absolutos\n","\n","    print(\"\\nConjunto de ValidaciÃ³n:\")\n","    print(valid_df[COLUNM_ETIQUETALETRA].value_counts(normalize=False))\n","\n","    print(\"\\nConjunto de Test:\")\n","    print(test_df[COLUNM_ETIQUETALETRA].value_counts(normalize=False))\n","    print(\"-----------------------------\")\n","\n","    nr_class_df = train_df[train_df[COLUMN_ETIQUETA] == 0]\n","    s_class_df = train_df[train_df[COLUMN_ETIQUETA] == 1]\n","    v_class_df = train_df[train_df[COLUMN_ETIQUETA] == 2]\n","    h_class_df = train_df[train_df[COLUMN_ETIQUETA] == 3]\n","\n","    #----PARA HACER OVERSAMPLING---\n","    v_oversampled_df = resample(v_class_df,\n","                                       replace=True,     # Permitir duplicados\n","                                       n_samples=198,\n","                                       random_state=42)\n","    h_oversampled_df = resample(h_class_df,\n","                                       replace=True,     # Permitir duplicados\n","                                       n_samples=156,\n","                                       random_state=42)\n","\n","    train_df_balanced = pd.concat([nr_class_df, s_class_df, v_oversampled_df, h_oversampled_df])\n","\n","    #----PARA HACER UNDERSAMPLING---\n","    #majority_undersampled_df = resample(majority_class_df,\n","    #                                replace=False,  # Sin duplicados\n","    #                                n_samples=600,  # Mismo tamaÃ±o que minoritaria\n","    #                                random_state=42)\n","\n","    #train_df_balanced = pd.concat([majority_undersampled_df, minority_oversampled_df])\n","\n","\n","    # Mezclar el DataFrame resultante para que las clases no estÃ©n agrupadas\n","    train_df_balanced = train_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","    print(\"\\nConjunto de Entrenamiento nuevo:\")\n","    print(train_df_balanced[COLUNM_ETIQUETALETRA].value_counts(normalize=False))\n","    print(\"------------------------------------------\")\n","\n","    # 3. Convertir a datasets.Dataset\n","    trainData = Dataset.from_pandas(train_df_balanced)\n","    validData = Dataset.from_pandas(valid_df)\n","    testData = Dataset.from_pandas(test_df) # Guarda el de test tambiÃ©n\n","\n","    print(\"\\nDatasets creados exitosamente.\")\n","    print(f\"Columnas en trainData: {trainData.column_names}\")\n","except FileNotFoundError:\n","    print(f\"Error: Archivo no encontrado en '{csv_train_filepath}'\")\n","except Exception as e:\n","    print(f\"Error inesperado durante la carga o divisiÃ³n de datos: {e}\")\n","    import traceback\n","    traceback.print_exc()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"B1doBf3zD3Te","executionInfo":{"status":"ok","timestamp":1746722030363,"user_tz":-120,"elapsed":3,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"}}},"outputs":[],"source":["max_seq_length = 2048\n","dtype = None\n","load_in_4bit = True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":414,"referenced_widgets":["fec76fe1626f4faa986607fecd5bb6f9","43c2dcb9dfa24d08b220c0a6148f77c1","a5c6ab15b4fa485496d010863539ec52","76c064c70b9a4872a30eae66f02f4753","38270cf0ec3c43608ee6e443dcd0a893","c6ea58ddcce84a25ad9af005b2510cb2","609cfffdb0604df29ff4e1fd71b6c578","720228dc54bc46d1b29205410850b033","89e21db494784a13bb3c0f1ac3cd3f8d","bf56cd4e1d6f4d8f907d57c0914fbfa2","40a0b050829940c3b593ab6ff2e5d1e7","9803dd27aef04ed3b9824bb7102886ea","23d53214d58c41eea95355ae35a56eda","b780a36a3ea246aa9f8dc6dc4100b367","563e94e1db65459b86db501ab5ad29de","55194d41ce7242e294584b09aaa33859","ca65e1341deb42c98db598f9134c3303","9c8b77fa1841456c81f72ecb197b63fb","e3c2d518fee34df187e83afc3224fd3c","e8cc49716c2e44a0bc81aa3e7759f6dd","17bfd2e0cb3d459e9ad58834b3f6869e","9cbb6745209b4b4a8872a3a803c330f3","a15419a9a4e04d92a5ff9b0208c94d48","2359a187b3394c09b8031707aa974ebc","29da4326c15d4b92b83870960d387010","e5871d00596346c8bda863a97e106fc5","e30074f251ee42fca775950cb0073988","b6c9ebf42670480b93b95056cef6983a","eb8490d21bac4a75bb5a777e8d22bfab","0a07b601e5694cf48f25a316f76d7a76","253ed70d5b4149cdb58619b1974ed1ee","6f587a743ecb412f8a8baf700a648848","7f8950f8e14f47bf834f00f1feef2f4a","57058c2b65a1490782c98279fa518a09","0752a00183c64d9b86c2979d73ee25e4","7dd73dba05f94fbf99f849e20259cbb6","d71bc31b01364c7ebb6af35fa6ac2de4","a410f703ec494b24a0d22fcad7d2c718","0dae58497ab641b09eaa2a253dd393c3","ff2ba07063ee427299a1430f9fbfc6ea","c2c15a0b46be43daa8c8e45225bf9f09","1096ee5ce1ef4f74a4b1cc9143127403","e685393f4d6e46b6bfeb08a3b2674e8b","c997e3afe6af48d78a43bcb0f9d805b0","62cd6f4b08cc4de88d8da6bb118f9c22","44b050dc6ac842feb9d7e5836f95429b","a087c2a202114633b1f5327c0e968737","de6f8f182c4d4ea0843cf4a03f4e9771","4f107ec4d633447a9326fd7e6c8fb813","0ccbee7735904b5d82f197b6d0747c98","0f2510c730e24e52941ce325080592cb","8daa6bf63e6f4fd3abfd1a047f5857a1","ab656ff5575940d3a30c8cf7e239631a","d261bc71b45d40df94e1ff0038ffc45d","898c7aa44a61421896f9dc52b32c4f6c","9111563aceda495ea3373f406c42e52d","75a8313258614396be18ace8509582b5","4135157c1d804e94947ee0b8eed4b9ed","532da8e4c46145e5b3c334d787e73184","d48e0838e8944a1099df1bf49af9902e","2a91de26d6aa49fdb29ac9f4200fc25f","6fac066a886d4b6d9c00314b04424706","79b6995f9e5c4d8c8bdaf6df1c75c29a","af57b503abee4a7b843a1e531c1aefbd","c902d3d624e24f629a77ac7a0de612bc","7dc9c7909dd048dd81d9db1a5adc0bbd","3a4809f0001646c0ab767ef0bd017fef","db309406b1254fc6811f02a2aaf0a057","f56a778c3d2a45d08a34489d1c21d7fc","cba192c28db44cec9434691a77dfe79c","b6fbfbed0211459a8fc0d9d2b1d607c2","d331a9a675a74c7faee91bad2659246c","9394d1bd90514806b1d36bbd8d7226d9","deb48c3656054abd8bb022564fd2940c","5656321d931f460cbf9bb816755dda4c","cf589c973cfe4d92a527243bcd7ac737","0824a865c0ea4f52a763742f6ced64c1","e8fc7a0dd21840c4aa7e95fd37d312c8","b1249e86d93043d38854703cd1289dfa","4c610cbdd3ef4ab380561ff181c694c6","35e8c3ae84ee4722bcc932a21bbae8a0","9f5f356f76e146b4aaf4d97ce97a6270","fa23568eac23446aa17e36b670db2ce8","178246b4898f4474beaf28a8f944e7cd","f90974ee917b4ac6b32f8eec0dea6b39","8bfddbe72ee94433b3674a5c27ea8ceb","0efa7482772f4b74b52701c14de3176c","db0a2ba6f43343449e2b9dce9611fb28","d8ce0dad3c8548708cd7722ea2e256df","8d75a97d43b7449ebb6de38c73df4cf7","9031da16fc8241c8bcc5ff9faa1c25e6","c442ee20120d411aac058cb14ecd1d50","8d13ce5ab8424161aa3b0f5a5e3da0e1","9928e6506d144835bb23099ac4273c5e","59ac5eb554214e56a88de281fef2728f","43d7def3f91447a8a159198a79a56d2a","7b293025a01c487184e3923b9750153e","12cd7047a5c54d6486e83e2c0a01f015","daa4842fa31b4ae2b339c24e668e9e19"]},"executionInfo":{"elapsed":117795,"status":"ok","timestamp":1746530586491,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"RjPCizQ6EQ9E","outputId":"000ff8fa-1d5b-4465-d26a-3f95bed2a8d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["==((====))==  Unsloth 2025.4.8: Fast Qwen2 patching. Transformers: 4.51.3.\n","   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fec76fe1626f4faa986607fecd5bb6f9","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/182k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9803dd27aef04ed3b9824bb7102886ea","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a15419a9a4e04d92a5ff9b0208c94d48","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57058c2b65a1490782c98279fa518a09","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.35G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62cd6f4b08cc4de88d8da6bb118f9c22","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9111563aceda495ea3373f406c42e52d","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a4809f0001646c0ab767ef0bd017fef","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/6.78k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8fc7a0dd21840c4aa7e95fd37d312c8","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d8ce0dad3c8548708cd7722ea2e256df","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = f\"unsloth/{model_name}\",  # Nombre del modelo en Hugging Face (modelo LLaMA distilado y mÃ¡s ligero)\n","    max_seq_length = max_seq_length,                      # Longitud mÃ¡xima de secuencia que puede procesar (por ejemplo 2048 o 4096 tokens)\n","    dtype = dtype,                                        # Tipo de precisiÃ³n para los cÃ¡lculos (por ejemplo: torch.float16 o torch.bfloat16)\n",")"]},{"cell_type":"markdown","metadata":{"id":"57hgk8HW_pC_"},"source":["# Prueba con el modelo sin entrenar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otsU1wn1n4vB"},"outputs":[],"source":["promp_style = \"\"\"### Instruccion\n","            Analiza la siguiente letra de canciÃ³n y determina si contiene contenido misÃ³gino. EvalÃºa si incluye lenguaje, actitudes o mensajes que:\n","              - Degraden o deshumanicen a las mujeres.\n","              - Menosprecien a las mujeres de manera explÃ­cita o implÃ­cita.\n","              - Refuercen estereotipos negativos o daÃ±inos sobre las mujeres.\n","              - Promuevan violencia fÃ­sica, emocional o sexual contra las mujeres.\n","\n","            Primero, piensa paso a paso tu razonamiento.\n","            DESPUÃ‰S de tu razonamiento interno, responde ÃšNICA Y EXCLUSIVAMENTE con \"1\" si la letra es misÃ³gina o con \"0\" si la letra no es misÃ³gina. Tu respuesta final debe ser solo un dÃ­gito.\n","\n","            ### Letra:\n","            {}\n","\n","            ### Respuesta: <think>\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znSFjuBfF_Ei"},"outputs":[],"source":["cancion = \"\"\"a de nuevo solos tÃº y yo, un lago y una canciÃ³n echo de menos oÃ­r tu voz, una estrella te eclipsÃ³ los momentos que no volverÃ¡n a sentir tu piel en mis brazos te tuve ayer hay tantas cosas que te quiero decir, acÃ©rcate a veces siento al despertar como un susurro, tu calor ella no deja de pensar que un dÃ­a te encontrarÃ¡ cÃ³geme no me dejes marchar, quiero sentarme a tus pies en mis brazos te tuve ayer hay tantas cosas que te quiero decir, dime por que sÃ³lo tÃº, tÃº y yo una guitarra, el lago y una canciÃ³n sÃ³lo tÃº, tÃº y yo ahora ya te puedo decir Â¡adiÃ³-oh-oh-os! ah-ah-ah-ah ah-ah-ah-ah de nuevo solos tÃº y yo un lago y una canciÃ³n echo de menos oÃ­r tu voz una estrella te eclipsÃ³\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUNhw6tAGXhK"},"outputs":[],"source":["import re\n","# Prepara el modelo para la inferencia (modo generaciÃ³n, sin entrenamiento)\n","FastLanguageModel.for_inference(model)\n","\n","# Formatea el prompt con el estilo deseado, insertando la variable 'cancion'\n","# Se crea una lista con un solo string para que el tokenizer procese un batch de tamaÃ±o 1\n","# Luego se convierte el texto a tensores de PyTorch y se pasa a la GPU\n","inputs = tokenizer([promp_style.format(cancion)], return_tensors=\"pt\").to('cuda')\n","\n","# Genera texto a partir del prompt\n","outputs = model.generate(\n","    input_ids=inputs.input_ids,           # IDs de entrada tokenizados\n","    attention_mask=inputs.attention_mask, # MÃ¡scara de atenciÃ³n para que el modelo ignore los tokens de relleno\n","    max_new_tokens=2,                  # NÃºmero mÃ¡ximo de tokens nuevos a generar\n","    use_cache=True,                       # Usa cachÃ© para acelerar la generaciÃ³n\n","    eos_token_id=tokenizer.eos_token_id,\n",")\n","\n","# Decodifica SOLO los tokens generados (excluye el prompt)\n","prompt_length = inputs.input_ids.shape[1]\n","generated_tokens = outputs[:, prompt_length:]\n","\n","# skip_special_tokens=True es VITAL para eliminar <|begin_of_sentence|>, etc.\n","# clean_up_tokenization_spaces=True puede ayudar con espacios extra.\n","decoded_outputs = tokenizer.batch_decode(\n","    generated_tokens,\n","    skip_especial_tokens=True,\n","    clean_up_tokenization_spaces=True\n",")\n","\n","if not decoded_outputs:\n","    print(\"Error: La decodificaciÃ³n no produjo ninguna salida.\")\n","    final_answer = None # O manejar el error\n","else:\n","    # Puede que necesites ajustar esto si la salida siempre estÃ¡ fragmentada en la lista\n","    # Por ejemplo, si siempre es el penÃºltimo o Ãºltimo elemento no vacÃ­o:\n","    # response_text = \"\".join([s for s in decoded_outputs if s.strip()]) # Une todo lo no vacÃ­o\n","    response_text = decoded_outputs[0].strip() # Toma el primer resultado decodificado y quita espacios extra\n","    print(f\"--- Texto Generado Decodificado (limpio) ---\\n{response_text}\\n------------------------------------------\")\n","\n","    final_answer = None\n","    found_method = \"None\"\n","\n","    # Prioridad 1: Buscar despuÃ©s de </think>\n","    # Busca la etiqueta literal \"</think>\" seguida de cualquier cantidad de espacio en blanco (\\s*)\n","    # y luego captura el primer dÃ­gito que encuentre ([01]).\n","    match_think = re.search(r\"</think>\\s*([01])\", response_text)\n","    if match_think:\n","        final_answer = match_think.group(1)\n","        found_method = \"After </think>\"\n","\n","    # Prioridad 2: Buscar despuÃ©s de \"Respuesta:\" (si la Prioridad 1 fallÃ³)\n","    # Esto es por si el modelo *genera* \"Respuesta:\" en su salida, aunque es menos probable con el prompt actual.\n","    if final_answer is None:\n","        # Busca \"Respuesta\" (case-insensitive) seguido de ':' opcional, espacios, y captura el dÃ­gito.\n","        match_respuesta = re.search(r\"Respuesta:?\\s*([01])\", response_text, re.IGNORECASE)\n","        if match_respuesta:\n","            final_answer = match_respuesta.group(1)\n","            found_method = \"After 'Respuesta:' (in output)\" # Indica que se encontrÃ³ en la salida\n","\n","    # Prioridad 3: Fallbacks (si las Prioridades 1 y 2 fallaron)\n","    if final_answer is None:\n","        print(\"INFO: No se encontrÃ³ patrÃ³n '</think> [0/1]' ni 'Respuesta: [0/1]'. Intentando fallbacks...\")\n","        # Fallback A: Ãšltimo dÃ­gito no-espacio\n","        cleaned_end_text = response_text.rstrip()\n","        if cleaned_end_text and cleaned_end_text[-1] in ('0', '1'):\n","            final_answer = cleaned_end_text[-1]\n","            found_method = \"Fallback: Last non-whitespace character\"\n","        # Fallback B: Primer dÃ­gito (solo si Fallback A fallÃ³)\n","        elif response_text and response_text.startswith(('0', '1')):\n","             # Usamos el texto ya limpiado con strip() al principio\n","             final_answer = response_text[0]\n","             found_method = \"Fallback: First character\"\n","\n","    # Resultado Final\n","    if final_answer is None:\n","        final_answer = '?' # O None\n","        found_method = \"Failed to find digit\"\n","        print(\"ERROR: No se pudo extraer '0' o '1' de forma fiable.\")\n","        # Considera registrar 'response_text' completo para depuraciÃ³n\n","    elif final_answer not in ('0', '1'): # Doble chequeo por si acaso\n","         print(f\"ERROR INESPERADO: Se extrajo algo que no es '0' o '1': {final_answer}\")\n","         final_answer = '?'\n","         found_method += \" (Extraction Error)\"\n","\n","\n","print(f\"\\nExtraction Method Used: {found_method}\")\n","print(f\"Respuesta Final Procesada: {final_answer}\")\n","# Decodifica los tokens generados a texto legible\n","#response = tokenizer.batch_decode(outputs)"]},{"cell_type":"markdown","metadata":{"id":"vfeNoP-WIr_A"},"source":["# AHORA EMPIEZA EL FINE TUNING"]},{"cell_type":"markdown","metadata":{"id":"MBzYEoW5WjG_"},"source":["# LLAMADO \"ALPACA PROMPT\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ek7kbrURoRDa"},"outputs":[],"source":["prompts_train = [ \"\"\"\n","            ### Instruccion\n","            Analiza la siguiente letra de canciÃ³n y clasifÃ­cala en UNA de las siguientes cuatro categorÃ­as: SexualizaciÃ³n, Odio, Violencia, o No relacionado.\n","\n","            EvalÃºa si la letra contiene predominantemente:\n","              - Violencia: Descripciones explÃ­citas de agresiÃ³n fÃ­sica, uso de armas, peleas, asesinatos, daÃ±o fÃ­sico, amenazas directas, o glorificaciÃ³n de la violencia fÃ­sica.\n","              - Odio: Expresiones de hostilidad intensa, desprecio profundo, animadversiÃ³n, lenguaje deshumanizante o extremadamente denigrante hacia individuos o grupos (por cualquier motivo: rivalidad, raza, gÃ©nero, etc.), o incitaciÃ³n al rencor/venganza.\n","              - Sexualizacion: ReducciÃ³n de personas a objetos sexuales, enfoque excesivo en atributos fÃ­sicos de forma despersonalizada, descripciones sexuales explÃ­citas con fin de objetivar/degradar, o lenguaje sexualmente sugerente de manera cosificadora.\n","              - No relacionado: Si la letra NO contiene predominantemente ninguno de los elementos anteriores (puede ser romÃ¡ntica, triste, festiva, narrativa sin estos enfoques, etc.).\n","\n","            Piensa cuidadosamente tu respuesta paso a paso, considerando cada categorÃ­a. Determina cuÃ¡l es la categorÃ­a principal o mÃ¡s definitoria si hubiera solapamiento leve. Responde Ãºnicamente con la etiqueta de la categorÃ­a elegida: \"Violencia\", \"Odio\", \"Sexualizacion\", o \"No relacionado\". No proporciones ninguna explicaciÃ³n ni texto adicional fuera de la etiqueta final.\n","\n","            ### Letra:\n","            {lyrics}\n","\n","            ### Respuesta:\n","            <think> {reasoning} </think>{label}\"\"\",\n","          ]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1746530596895,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"pPxmib2oK4Kd","outputId":"ba5906b8-ad7d-44d5-abd0-6c9ac1e15ecb"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<ï½œendâ–ofâ–sentenceï½œ>'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["EOS_TOKEN = tokenizer.eos_token\n","EOS_TOKEN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUohMs_SLUDy"},"outputs":[],"source":["label_map = {\n","    0: \"No relacionado\",\n","    1: \"Sexualizacion\",\n","    2: \"Violencia\",\n","    3: \"Odio\"\n","}\n","\n","def formatting_prompts(examples):\n","    # Extrae las listas de letras y etiquetas desde el diccionario de ejemplos\n","    lyrics_list = examples[COLUMN_LETRA]      # Lista de letras (inputs)\n","    labels_list  = examples[COLUMN_ETIQUETA]      # Lista de etiquetas o salidas esperadas\n","    reasonings_list = examples[COLUMN_RAZONAMIENTO]\n","\n","    texts = []  # AquÃ­ se guardarÃ¡n los prompts formateados\n","\n","\n","    # Itera sobre cada par input/output\n","    for lyric, label, reasoning in zip(lyrics_list, labels_list, reasonings_list):\n","        label_str = label_map[label]\n","        # Formatea el prompt usando la plantilla\n","        text = prompts_train[tipo_prompt].format(\n","            lyrics=str(lyric),\n","            reasoning=str(reasoning),\n","            label=label_str\n","            ) + EOS_TOKEN  # AÃ±ade token de finalizaciÃ³n\n","\n","        texts.append(text)  # Guarda el texto formateado\n","\n","    # Devuelve un diccionario con el texto ya preparado para el entrenamiento\n","    return {\n","        \"text\": texts\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1746530602577,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"ao_bD7U-_pDA","outputId":"161fe151-9344-4815-93b3-3476fcbd07a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Formateando datasets para entrenamiento CoT...\n","Columnas en trainData antes de mapear: ['id', 'lyrics', 'reasoning', 'label', 'labelconletras']\n","Columnas en validData antes de mapear: ['id', 'lyrics', 'reasoning', 'label', 'labelconletras', '__index_level_0__']\n"]}],"source":["print(\"\\nFormateando datasets para entrenamiento CoT...\")\n","print(f\"Columnas en trainData antes de mapear: {trainData.column_names}\")\n","print(f\"Columnas en validData antes de mapear: {validData.column_names}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"executionInfo":{"elapsed":187,"status":"ok","timestamp":1746530603840,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"S4quwbYvMOO0","outputId":"f49e993a-905a-4d7e-9dc7-4cf0e5eea38f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"752ea53897ca407b8ac5587b9c9d90b9","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/891 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d197e2553bb47b6bc7e4efea21c90f2","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/281 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n            ### Instruccion\\n            Analiza la siguiente letra de canciÃ³n y clasifÃ­cala en UNA de las siguientes cuatro categorÃ­as: SexualizaciÃ³n, Odio, Violencia, o No relacionado.\\n\\n            EvalÃºa si la letra contiene predominantemente:\\n              - Violencia: Descripciones explÃ­citas de agresiÃ³n fÃ­sica, uso de armas, peleas, asesinatos, daÃ±o fÃ­sico, amenazas directas, o glorificaciÃ³n de la violencia fÃ­sica.\\n              - Odio: Expresiones de hostilidad intensa, desprecio profundo, animadversiÃ³n, lenguaje deshumanizante o extremadamente denigrante hacia individuos o grupos (por cualquier motivo: rivalidad, raza, gÃ©nero, etc.), o incitaciÃ³n al rencor/venganza.\\n              - Sexualizacion: ReducciÃ³n de personas a objetos sexuales, enfoque excesivo en atributos fÃ­sicos de forma despersonalizada, descripciones sexuales explÃ­citas con fin de objetivar/degradar, o lenguaje sexualmente sugerente de manera cosificadora.\\n              - No relacionado: Si la letra NO contiene predominantemente ninguno de los elementos anteriores (puede ser romÃ¡ntica, triste, festiva, narrativa sin estos enfoques, etc.).\\n\\n            Piensa cuidadosamente tu respuesta paso a paso, considerando cada categorÃ­a. Determina cuÃ¡l es la categorÃ­a principal o mÃ¡s definitoria si hubiera solapamiento leve. Responde Ãºnicamente con la etiqueta de la categorÃ­a elegida: \"Violencia\", \"Odio\", \"Sexualizacion\", o \"No relacionado\". No proporciones ninguna explicaciÃ³n ni texto adicional fuera de la etiqueta final.\\n\\n            ### Letra:\\n            a a ti te quiero mujer no le hace que seas pasiada te quiero porque te quiero porque me nace del alma tu no sabias querer porque eras mujer pasiada y te burrlabas de mi cuando de amores te acraba pero, llegaste a saber que con mi amor no jugabas y con el tiempo supistes lo mucho que tu me amaba tu despresiabas me amor cuando en tus brasos lloraba pero llegaste a quererme asi como yo yo deseaba\\n\\n            ### Respuesta:\\n            <think> 1. AnÃ¡lisis de Componentes: La frase describe una relaciÃ³n entre dos personas. Palabras y frases clave incluyen \"mujer pasiada\", \"te burlabas de mÃ­\", \"no sabÃ­as querer\", \"despreciabas mi amor\", y \"lloraba en tus brazos\". Estas expresiones sugieren conflictos en la relaciÃ³n y sentimientos de menosprecio.\\n\\n2. InterpretaciÃ³n y ConnotaciÃ³n: La expresiÃ³n \"mujer pasiada\" podrÃ­a implicar que la mujer en cuestiÃ³n tiene mÃºltiples relaciones, lo que lleva a un conflicto. \"Te burlabas de mÃ­\" y \"despreciabas mi amor\" indican una dinÃ¡mica de desdÃ©n y rechazo. El narrador describe una transiciÃ³n desde el menosprecio hacia un eventual reconocimiento y reciprocidad del amor.\\n\\n3. EvaluaciÃ³n contra S (Sexualization): No hay indicios claros de lenguaje o insinuaciones de naturaleza sexual en la letra. La frase se centra mÃ¡s en el conflicto emocional y el cambio en la relaciÃ³n afectiva, sin descripciones sexuales.\\n\\n4. EvaluaciÃ³n contra V (Violence): Aunque no hay amenazas fÃ­sicas explÃ­citas, el lenguaje sugiere un tipo de violencia emocional o psicolÃ³gica. Frases como \"te burlabas de mÃ­\" y \"despreciabas mi amor\" implican un menosprecio emocional significativo. La idea de \"llorar en tus brazos\" denota sufrimiento emocional, que puede considerarse una forma de agresiÃ³n psicolÃ³gica.\\n\\n5. EvaluaciÃ³n contra H (Hate): No se evidencia hostilidad activa o desprecio hacia un grupo especÃ­fico. El enfoque estÃ¡ en la interacciÃ³n y el conflicto entre dos individuos, sin extenderse a una expresiÃ³n de odio mÃ¡s generalizada.\\n\\n6. SÃ­ntesis y JustificaciÃ³n Final: La letra refleja una dinÃ¡mica de poder emocional donde el narrador siente que ha sido objeto de burla y desprecio, lo que conlleva sufrimiento emocional. Aunque no hay violencia fÃ­sica, la violencia emocional se manifiesta a travÃ©s de las expresiones de menosprecio y el impacto psicolÃ³gico en el narrador. Este tipo de agresiÃ³n justifica la clasificaciÃ³n bajo la etiqueta de violencia. </think>Violencia<ï½œendâ–ofâ–sentenceï½œ>'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["dataset_finetune = trainData.map(formatting_prompts, batched = True, remove_columns=trainData.column_names) # remove_columns para dejar solo 'text'\n","eval_finetune = validData.map(formatting_prompts, batched=True, remove_columns=validData.column_names) # remove_columns para dejar solo 'text'\n","dataset_finetune[\"text\"][0]"]},{"cell_type":"markdown","metadata":{"id":"VK2eEmIhJDai"},"source":["# Calculo longitud de los prompts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9q54WStlr81"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","token_lengths = [len(tokenizer.encode(text, add_special_tokens=False)) for text in dataset_finetune[\"text\"]]\n","print(f\"Se calcularon las longitudes de {len(token_lengths)} prompts formateados.\")\n","\n","if not token_lengths:\n","    print(\"La lista de longitudes estÃ¡ vacÃ­a. Verifica el dataset y la columna 'text'.\")\n","else:\n","    # --- Ahora puedes proceder con el anÃ¡lisis ---\n","\n","    # 1. Encontrar la longitud mÃ¡xima\n","    max_len = max(token_lengths)\n","    print(f\"Longitud mÃ¡xima encontrada (incl. special tokens): {max_len}\")\n","\n","    # 2. Visualizar la distribuciÃ³n (como en tu ejemplo anterior)\n","    plt.figure(figsize=(12, 7))\n","    plt.hist(token_lengths, bins=50, alpha=0.7, label=f'DistribuciÃ³n (MÃ¡x={max_len})')\n","\n","    plt.axvline(1024, color='red', linestyle='dashed', linewidth=1, label='Max Length = 1024')\n","    plt.axvline(2048, color='orange', linestyle='dashed', linewidth=1, label='Max Length = 2048')\n","    plt.axvline(4096, color='green', linestyle='dashed', linewidth=1, label='Max Length = 4096')\n","    plt.axvline(8192, color='blue', linestyle='dashed', linewidth=1, label='Max Length = 8192')\n","\n","\n","    plt.xlabel('NÃºmero de Tokens (incl. special tokens)')\n","    plt.ylabel('NÃºmero de Ejemplos')\n","    plt.title(f'DistribuciÃ³n de Longitud de Tokens del Prompt Formateado\\n(Modelo: {tokenizer.name_or_path})') # Usa el nombre del tokenizer\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    print(f\"Porcentaje de canciones conS mÃ¡s de 1024 tokens: {sum(l > 1024 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")\n","    print(f\"Porcentaje de canciones con mÃ¡s de 2048 tokens: {sum(l > 2048 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")\n","    print(f\"Porcentaje de canciones con mÃ¡s de 4096 tokens: {sum(l > 4096 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")\n","    print(f\"Porcentaje de canciones con mÃ¡s de 8192 tokens: {sum(l > 8192 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"355czYm-o-QH"},"source":["# ELIMINO LOS PROMPTS QUE SUPEREN LOS 2048 TOKENS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gG983_Pfoc7B"},"outputs":[],"source":["def limit_tokens(example):\n","  return len(tokenizer.encode(str(example['text']), add_special_tokens=True)) <= max_seq_length"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":154},"executionInfo":{"elapsed":4943,"status":"ok","timestamp":1746530616762,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"5x5k-ByEopGc","outputId":"5e3b6da2-3b4e-4b5d-fb72-09efb51d567e"},"outputs":[{"name":"stdout","output_type":"stream","text":["TamaÃ±o original de dataset_finetune: 891\n","TamaÃ±o original de eval_finetune: 281\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af3859103d4d46e9be244b29180ba784","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/891 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59a1ed7493684ff4a51c739086dd90af","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/281 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["TamaÃ±o filtrado de dataset_finetune: 844\n","TamaÃ±o filtrado de eval_finetune: 263\n"]}],"source":["print(f\"TamaÃ±o original de dataset_finetune: {len(dataset_finetune)}\")\n","print(f\"TamaÃ±o original de eval_finetune: {len(eval_finetune)}\")\n","dataset_finetune = dataset_finetune.filter(limit_tokens, batched=False)\n","eval_finetune = eval_finetune.filter(limit_tokens, batched=False)\n","print(f\"TamaÃ±o filtrado de dataset_finetune: {len(dataset_finetune)}\")\n","print(f\"TamaÃ±o filtrado de eval_finetune: {len(eval_finetune)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6I0nyDAVo7J_"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","token_lengths = [len(tokenizer.encode(text, add_special_tokens=False)) for text in dataset_finetune[\"text\"]]\n","print(f\"Se calcularon las longitudes de {len(token_lengths)} prompts formateados.\")\n","\n","if not token_lengths:\n","    print(\"La lista de longitudes estÃ¡ vacÃ­a. Verifica el dataset y la columna 'text'.\")\n","else:\n","    # --- Ahora puedes proceder con el anÃ¡lisis ---\n","\n","    # 1. Encontrar la longitud mÃ¡xima\n","    max_len = max(token_lengths)\n","    print(f\"Longitud mÃ¡xima encontrada (incl. special tokens): {max_len}\")\n","\n","    # 2. Visualizar la distribuciÃ³n (como en tu ejemplo anterior)\n","    plt.figure(figsize=(12, 7))\n","    plt.hist(token_lengths, bins=50, alpha=0.7, label=f'DistribuciÃ³n (MÃ¡x={max_len})')\n","\n","    plt.axvline(1024, color='red', linestyle='dashed', linewidth=1, label='Max Length = 1024')\n","    plt.axvline(2048, color='orange', linestyle='dashed', linewidth=1, label='Max Length = 2048')\n","    plt.axvline(4096, color='green', linestyle='dashed', linewidth=1, label='Max Length = 4096')\n","    plt.axvline(8192, color='blue', linestyle='dashed', linewidth=1, label='Max Length = 8192')\n","\n","\n","    plt.xlabel('NÃºmero de Tokens (incl. special tokens)')\n","    plt.ylabel('NÃºmero de Ejemplos')\n","    plt.title(f'DistribuciÃ³n de Longitud de Tokens del Prompt Formateado\\n(Modelo: {tokenizer.name_or_path})') # Usa el nombre del tokenizer\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    print(f\"Porcentaje de canciones con mÃ¡s de 1024 tokens: {sum(l > 1024 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")\n","    print(f\"Porcentaje de canciones con mÃ¡s de 2048 tokens: {sum(l > 2048 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")\n","    print(f\"Porcentaje de canciones con mÃ¡s de 4096 tokens: {sum(l > 4096 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")\n","    print(f\"Porcentaje de canciones con mÃ¡s de 8192 tokens: {sum(l > 8192 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"VJR6rC2uibOw"},"source":["# Parametros del train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1746530625939,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"sXMS7Nai_pDB","outputId":"85fa64a5-d627-4606-fee5-8058d0974fe2"},"outputs":[{"name":"stdout","output_type":"stream","text":["ConfiguraciÃ³n de Entrenamiento:\n","  - Per Device Batch Size: 1\n","  - Gradient Accumulation Steps: 16\n","  - Effective Batch Size: 16\n","  - Num Train Examples: 843\n","  - Epochs: 8\n","  - Steps per Epoch: ~52\n","  - Total Steps: ~416\n","  - Warmup Steps: 41\n","  - Eval/Log/Save Steps: 25\n","  - LoRA R: 16\n","  - LoRA Alpha: 32\n"]}],"source":["# --- Ajustes para 14GB VRAM y cÃ¡lculo de pasos ---\n","per_device_batch = 1\n","gradient_accum_steps = 16 # Si da OOM, bajar a 4.\n","effective_batch_size = per_device_batch * gradient_accum_steps # = 8\n","epochs = 8\n","num_train_examples = 843 # TamaÃ±o de tu dataset de entrenamiento\n","steps_per_epoch = num_train_examples // effective_batch_size # ~177\n","total_steps = steps_per_epoch * epochs # ~296\n","warmup_steps_calculated = int(total_steps * 0.10) # ~30 (10% warmup)\n","# Evaluar/Loguear/Guardar cada 25 pasos\n","eval_logging_save_steps = 25\n","lora_r = 16\n","lora_alpha = 32\n","learning_rate = 1e-5\n","\n","print(f\"ConfiguraciÃ³n de Entrenamiento:\")\n","print(f\"  - Per Device Batch Size: {per_device_batch}\")\n","print(f\"  - Gradient Accumulation Steps: {gradient_accum_steps}\")\n","print(f\"  - Effective Batch Size: {effective_batch_size}\")\n","print(f\"  - Num Train Examples: {num_train_examples}\")\n","print(f\"  - Epochs: {epochs}\")\n","print(f\"  - Steps per Epoch: ~{steps_per_epoch}\")\n","print(f\"  - Total Steps: ~{total_steps}\")\n","print(f\"  - Warmup Steps: {warmup_steps_calculated}\")\n","print(f\"  - Eval/Log/Save Steps: {eval_logging_save_steps}\")\n","print(f\"  - LoRA R: {lora_r}\")\n","print(f\"  - LoRA Alpha: {lora_alpha}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7243,"status":"ok","timestamp":1746530634769,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"aZLxKZeJNOpW","outputId":"9ffbe42e-bde1-4c01-a969-a440291dd6c9"},"outputs":[{"name":"stderr","output_type":"stream","text":["Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","Unsloth 2025.4.8 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"]}],"source":["# Aplica LoRA al modelo base para permitir entrenamiento eficiente\n","model_lora = FastLanguageModel.get_peft_model(\n","    model,                          # Modelo base al que se le quiere aplicar LoRA\n","    r=lora_r,                           # Rango bajo de adaptaciÃ³n (mÃ¡s bajo = menos parÃ¡metros entrenables)\n","    target_modules=[               # Capas especÃ­ficas del modelo donde se insertarÃ¡n los adaptadores LoRA\n","        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",        # Proyecciones de atenciÃ³n\n","        \"gate_proj\", \"up_proj\", \"down_proj\"           # Capas del MLP\n","    ],\n","    lora_alpha=lora_alpha,                 # Escala del adaptador LoRA (afecta la fuerza de la adaptaciÃ³n)\n","    lora_dropout=0.05,                # Dropout en LoRA (0 = no dropout, Ãºtil para tener resultados estables)\n","    bias=\"none\",                   # No se entrena el sesgo de las capas originales\n","    use_gradient_checkpointing=\"unsloth\",  # Reduce el uso de memoria en GPU durante el entrenamiento\n","    random_state=3407,             # Fijar semilla aleatoria para reproducibilidad\n","    use_rslora=False,              # Si se quiere usar una versiÃ³n mÃ¡s avanzada de LoRA (aquÃ­ se desactiva)\n","    loftq_config=None              # Solo se usa si se quiere aplicar cuantizaciÃ³n LoFTQ (aquÃ­ no se usa)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"elapsed":1840,"status":"ok","timestamp":1746530636612,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"z1ydFsfzOueV","outputId":"67a559c5-0f3c-4f2d-ca10-83c338f59e15"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23cb2bb61b594145bcf39d1b84af562c","version_major":2,"version_minor":0},"text/plain":["Unsloth: Tokenizing [\"text\"]:   0%|          | 0/844 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d785ec90a0d47aa84b818431fa1cbc6","version_major":2,"version_minor":0},"text/plain":["Unsloth: Tokenizing [\"text\"]:   0%|          | 0/263 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#Inicializar el fine tuning trainer\n","trainer = SFTTrainer(\n","    model = model_lora,                 # El modelo con adaptadores LoRA listos para entrenar\n","    tokenizer = tokenizer,              # El tokenizer asociado al modelo\n","    train_dataset = dataset_finetune,   # Dataset ya formateado con prompts\n","    eval_dataset = eval_finetune,\n","    dataset_text_field = \"text\",        # Campo del dataset que contiene el texto de entrada y salida\n","    max_seq_length = max_seq_length,    # Longitud mÃ¡xima de secuencia para cada ejemplo\n","    dataset_num_proc = 1,               # NÃºmero de procesos paralelos para preparar el dataset\n","    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n","    args = TrainingArguments(\n","        per_device_train_batch_size = per_device_batch,       # TamaÃ±o de batch por GPU (se combina con gradient_accumulation_steps)\n","        gradient_accumulation_steps = gradient_accum_steps,       # Acumula gradientes durante 4 pasos antes de actualizar pesos â†’ batch efectivo = 8\n","        #max_steps=60,\n","        num_train_epochs = epochs,                  # NÃºmero de Ã©pocas completas sobre el dataset\n","        warmup_steps = warmup_steps_calculated,                      # Pasos de warmup donde la LR va subiendo poco a poco\n","        learning_rate = learning_rate,                  # Tasa de aprendizaje (bastante buena para LoRA)\n","        fp16 = not is_bfloat16_supported(),    # Usa FP16 si bfloat16 no estÃ¡ soportado\n","        bf16 = is_bfloat16_supported(),        # Usa BF16 si tu GPU lo soporta (mejor para Ampere y posteriores)\n","        logging_steps = eval_logging_save_steps,                    # Registra logs cada 10 pasos\n","        #logging_steps = 1,\n","        optim = \"adamw_8bit\",                  # Optimizador AdamW en 8 bits (menos memoria, ideal para GPUs pequeÃ±as)\n","        weight_decay = 0.01,                   # RegularizaciÃ³n para evitar overfitting\n","        lr_scheduler_type = \"linear\",          # Scheduler lineal de tasa de aprendizaje\n","        seed = 3407,                           # Semilla para reproducibilidad\n","        output_dir = f\"{path}/cuadernos/outputs\",                # Carpeta donde se guardan los checkpoints\n","        eval_strategy=\"steps\",\n","        eval_steps=eval_logging_save_steps,\n","        save_strategy=\"steps\",\n","        save_steps=eval_logging_save_steps,\n","        save_total_limit=4,                                   # Guardar solo los Ãºltimos 2\n","        load_best_model_at_end=True,                          # Carga el mejor modelo segÃºn eval_loss\n","        metric_for_best_model=\"eval_loss\",                    # Usa loss para decidir el \"mejor\"\n","        greater_is_better=False,                              # Menor loss es mejor\n","        report_to=\"wandb\",\n","    ),\n",")"]},{"cell_type":"markdown","metadata":{"id":"JGpCfVNfifkF"},"source":["#Iniciando training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":834},"executionInfo":{"elapsed":30715341,"status":"ok","timestamp":1746561351955,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"9JUZ4b6aQriG","outputId":"7d2a78a3-6cd7-4f07-f4ab-dee0af4185cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Iniciando entrenamiento...\n"]},{"name":"stderr","output_type":"stream","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 844 | Num Epochs = 8 | Total steps = 416\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n","\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n"," \"-____-\"     Trainable parameters = 68,812,800/14,000,000,000 (0.49% trained)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Unsloth: Will smartly offload gradients to save VRAM!\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='416' max='416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [416/416 8:30:43, Epoch 7/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>2.190000</td>\n","      <td>2.160296</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>2.103900</td>\n","      <td>1.901212</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>1.777100</td>\n","      <td>1.556898</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.465400</td>\n","      <td>1.297387</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>1.293300</td>\n","      <td>1.203350</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.215800</td>\n","      <td>1.157684</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>1.181900</td>\n","      <td>1.135752</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.186800</td>\n","      <td>1.120890</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>1.163900</td>\n","      <td>1.109648</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.146200</td>\n","      <td>1.101332</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>1.137500</td>\n","      <td>1.094796</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.127600</td>\n","      <td>1.090002</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>1.130000</td>\n","      <td>1.086341</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.114400</td>\n","      <td>1.083882</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>1.138600</td>\n","      <td>1.082101</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.110600</td>\n","      <td>1.081092</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n","Using gradient accumulation will be very slightly less accurate.\n","Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"]},{"name":"stdout","output_type":"stream","text":["Entrenamiento completado.\n"]}],"source":["# --- Ejecutar Entrenamiento ---\n","print(\"\\nIniciando entrenamiento...\")\n","trainer_stats = trainer.train()\n","print(\"Entrenamiento completado.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"elapsed":3602,"status":"ok","timestamp":1746561355558,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"E-O6TMCrITD-","outputId":"2a675c30-d1ff-46b8-d392-555a2534f31c"},"outputs":[{"name":"stderr","output_type":"stream","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 844 | Num Epochs = 8 | Total steps = 416\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n","\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n"," \"-____-\"     Trainable parameters = 68,812,800/14,000,000,000 (0.49% trained)\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='416' max='416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [416/416 : < :, Epoch 7/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["trainer_stats = trainer.train(resume_from_checkpoint = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":763},"executionInfo":{"elapsed":1295,"status":"ok","timestamp":1746561356854,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"YFud0-khQvsB","outputId":"9b36bbc1-0f34-4d35-f743-a7dca452287c"},"outputs":[{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–ˆâ–†â–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–ˆâ–‚â–â–â–‚â–â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–</td></tr><tr><td>eval/samples_per_second</td><td>â–â–†â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡</td></tr><tr><td>eval/steps_per_second</td><td>â–â–†â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆâ–‡</td></tr><tr><td>train/epoch</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–‡â–ˆâ–…â–ƒâ–â–â–‚â–â–‚â–â–â–â–â–â–â–</td></tr><tr><td>train/learning_rate</td><td>â–…â–ˆâ–‡â–‡â–‡â–†â–…â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–‡â–…â–ƒâ–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.08109</td></tr><tr><td>eval/runtime</td><td>310.4012</td></tr><tr><td>eval/samples_per_second</td><td>0.847</td></tr><tr><td>eval/steps_per_second</td><td>0.847</td></tr><tr><td>total_flos</td><td>7.92954674284376e+17</td></tr><tr><td>train/epoch</td><td>7.85308</td></tr><tr><td>train/global_step</td><td>416</td></tr><tr><td>train/grad_norm</td><td>0.1919</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1106</td></tr><tr><td>train_loss</td><td>0</td></tr><tr><td>train_runtime</td><td>0.3866</td></tr><tr><td>train_samples_per_second</td><td>17464.218</td></tr><tr><td>train_steps_per_second</td><td>1075.994</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">charmed-energy-5</strong> at: <a href='https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2/runs/c6f61n2p?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3' target=\"_blank\">https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2/runs/c6f61n2p?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3</a><br> View project at: <a href='https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3' target=\"_blank\">https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20250506_111927-c6f61n2p/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"ubt8DzWPijDT"},"source":["#Guardando el modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQe8V1Qjk_Xi"},"outputs":[],"source":["final_save_path = f'{path}/models/Model_{model_name}-CoT_{tipo_CoT}-R_{lora_r}-Alpha_{lora_alpha}-LR_{learning_rate}-Tarea_2'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11081,"status":"ok","timestamp":1746561367947,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"pry08J4u0vfN","outputId":"9d624d24-fb72-4f0d-812c-91fef2ffeb89"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Guardando el mejor modelo en: /content/drive/Othercomputers/Mi PC/TFG/MySonGyny/MySonGyny-2025/models/Model_DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit-CoT_GPT4o-R_16-Alpha_32-LR_1e-05-Tarea_2\n","Modelo y tokenizer guardados.\n"]}],"source":["\n","print(f\"\\nGuardando el mejor modelo en: {final_save_path}\")\n","trainer.save_model(final_save_path)\n","tokenizer.save_pretrained(final_save_path)\n","print(\"Modelo y tokenizer guardados.\")"]},{"cell_type":"markdown","metadata":{"id":"HNsaiyuOila2"},"source":["# Probando el modelo entrenado :)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"1hVnVadOOqTC","executionInfo":{"status":"ok","timestamp":1746722102998,"user_tz":-120,"elapsed":8,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"}}},"outputs":[],"source":["final_save_path = f'{path}/models/Model-DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-CoT-GPT4o-R-16-Alpha-32-LR-2e-05-Tarea-2'"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"bkM2Q93s74fB","colab":{"base_uri":"https://localhost:8080/","height":499},"executionInfo":{"status":"error","timestamp":1746722809086,"user_tz":-120,"elapsed":27298,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"}},"outputId":"ad9ddada-9abd-410e-9f96-dc96533441cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.4.8: Fast Llama patching. Transformers: 4.51.3.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"error","ename":"HFValidationError","evalue":"Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/Othercomputers/Mi PC/TFG/MySonGyny/MySonGyny-2025/models/Model-DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-CoT-GPT4o-R-16-Alpha-32-LR-2e-05-Tarea-2'. Use `repo_type` argument if needed.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-519023891ab2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_save_path\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0;31m# Nombre del modelo en Hugging Face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m                      \u001b[0;31m# Longitud mÃ¡xima de secuencia que puede procesar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m                                        \u001b[0;31m# Tipo de precisiÃ³n para los cÃ¡lculos (torch.float16 o torch.bfloat16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;31m# From https://github.com/huggingface/peft/issues/184\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# Now add PEFT adapters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             model = PeftModel.from_pretrained(\n\u001b[0m\u001b[1;32m    434\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0mold_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             )\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         load_result = model.load_adapter(\n\u001b[0m\u001b[1;32m    542\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mload_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m         \u001b[0madapters_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_peft_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhf_hub_download_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;31m# load the weights into the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py\u001b[0m in \u001b[0;36mload_peft_weights\u001b[0;34m(model_id, device, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mhub_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hub_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_safetensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         has_remote_safetensors_file = file_exists(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhub_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/Othercomputers/Mi PC/TFG/MySonGyny/MySonGyny-2025/models/Model-DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-CoT-GPT4o-R-16-Alpha-32-LR-2e-05-Tarea-2'. Use `repo_type` argument if needed."]}],"source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = final_save_path,                         # Nombre del modelo en Hugging Face\n","    max_seq_length = 2048,                      # Longitud mÃ¡xima de secuencia que puede procesar\n","    dtype = dtype,                                        # Tipo de precisiÃ³n para los cÃ¡lculos (torch.float16 o torch.bfloat16)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NnSc7zDBsoFI","executionInfo":{"status":"aborted","timestamp":1746722031450,"user_tz":-120,"elapsed":0,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"}}},"outputs":[],"source":["prompts_test = [ \"\"\"\n","            ### Instruccion\n","            Analiza la siguiente letra de canciÃ³n y clasifÃ­cala en UNA de las siguientes cuatro categorÃ­as: SexualizaciÃ³n, Odio, Violencia, o No relacionado.\n","\n","            EvalÃºa si la letra contiene predominantemente:\n","              - Violencia: Descripciones explÃ­citas de agresiÃ³n fÃ­sica, uso de armas, peleas, asesinatos, daÃ±o fÃ­sico, amenazas directas, o glorificaciÃ³n de la violencia fÃ­sica.\n","              - Odio: Expresiones de hostilidad intensa, desprecio profundo, animadversiÃ³n, lenguaje deshumanizante o extremadamente denigrante hacia individuos o grupos (por cualquier motivo: rivalidad, raza, gÃ©nero, etc.), o incitaciÃ³n al rencor/venganza.\n","              - Sexualizacion: ReducciÃ³n de personas a objetos sexuales, enfoque excesivo en atributos fÃ­sicos de forma despersonalizada, descripciones sexuales explÃ­citas con fin de objetivar/degradar, o lenguaje sexualmente sugerente de manera cosificadora.\n","              - No relacionado: Si la letra NO contiene predominantemente ninguno de los elementos anteriores (puede ser romÃ¡ntica, triste, festiva, narrativa sin estos enfoques, etc.).\n","\n","            Piensa cuidadosamente tu respuesta paso a paso, considerando cada categorÃ­a. Determina cuÃ¡l es la categorÃ­a principal o mÃ¡s definitoria si hubiera solapamiento leve. Responde Ãºnicamente con la etiqueta de la categorÃ­a elegida: \"Violencia\", \"Odio\", \"Sexualizacion\", o \"No relacionado\". No proporciones ninguna explicaciÃ³n ni texto adicional fuera de la etiqueta final.\n","\n","            ### Letra:\n","            {lyrics}\n","\n","            ### Respuesta:\n","            <think>\"\"\"\n","          ]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":49,"status":"ok","timestamp":1746563026762,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"MscjKW9x_pDC","outputId":"94ba4b8b-bb3a-4236-da5d-e48690371d64"},"outputs":[{"data":{"text/plain":["Qwen2ForCausalLM(\n","  (model): Qwen2Model(\n","    (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n","    (layers): ModuleList(\n","      (0-4): 5 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear(\n","            (base_layer): Linear(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (5-6): 2 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear(\n","            (base_layer): Linear(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (7-22): 16 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (23): Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (24-25): 2 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (26): Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear(\n","            (base_layer): Linear(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (27-42): 16 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (43-46): 4 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear(\n","            (base_layer): Linear(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (47): Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","    )\n","    (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",")"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["# --- Asegurar que el modelo estÃ© en modo evaluaciÃ³n e inferencia ---\n","model.eval() # Pone el modelo en modo evaluaciÃ³n (desactiva dropout, etc.)\n","FastLanguageModel.for_inference(model) # Prepara el modelo Unsloth para inferencia rÃ¡pida"]},{"cell_type":"markdown","metadata":{"id":"B6DgHs_EKK4K"},"source":["#Prueba para ver los FP y FN"]},{"cell_type":"markdown","metadata":{"id":"JS_D54YZKQlC"},"source":["#Prueba para ejecutar solo 40 canciones"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1746563033084,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"36YvpbvGDNVI","outputId":"7ba48aa1-8b27-4b6e-b12d-598bdf9093f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Preparando evaluaciÃ³n con un subconjunto ---\n","Seleccionados 6 ejemplos con etiqueta '1'.\n","Seleccionados 6 ejemplos con etiqueta '0'.\n","Total ejemplos en el subconjunto de evaluaciÃ³n: 12\n"]}],"source":["import random\n","# --- SELECCIÃ“N DEL SUBCONJUNTO DE testData ---\n","\n","num_samples_label_1 = 6  # NÃºmero de ejemplos misÃ³ginos a seleccionar\n","num_samples_label_0 = 6  # NÃºmero de ejemplos no misÃ³ginos a seleccionar\n","\n","# 1. Separar los ejemplos por etiqueta\n","examples_label_1 = [ex for ex in testData if str(ex[COLUMN_ETIQUETA]).strip() == '2']\n","examples_label_0 = [ex for ex in testData if str(ex[COLUMN_ETIQUETA]).strip() == '3']\n","\n","# 2. Asegurarse de no pedir mÃ¡s ejemplos de los que hay\n","num_samples_label_1 = min(num_samples_label_1, len(examples_label_1))\n","num_samples_label_0 = min(num_samples_label_0, len(examples_label_0))\n","\n","# 3. Seleccionar aleatoriamente los ejemplos deseados\n","random.seed(42) # Para reproducibilidad\n","selected_examples_1 = random.sample(examples_label_1, num_samples_label_1)\n","selected_examples_0 = random.sample(examples_label_0, num_samples_label_0)\n","\n","# 4. Combinar los ejemplos seleccionados en un nuevo dataset para evaluar\n","sampled_testData = selected_examples_1 + selected_examples_0\n","random.shuffle(sampled_testData) # Mezclar las clases 0 y 1 para la evaluaciÃ³n\n","\n","testData = sampled_testData\n","print(f\"--- Preparando evaluaciÃ³n con un subconjunto ---\")\n","print(f\"Seleccionados {len(selected_examples_1)} ejemplos con etiqueta '1'.\")\n","print(f\"Seleccionados {len(selected_examples_0)} ejemplos con etiqueta '0'.\")\n","print(f\"Total ejemplos en el subconjunto de evaluaciÃ³n: {len(sampled_testData)}\")"]},{"cell_type":"markdown","metadata":{"id":"1tZaUff7Kiho"},"source":["#Ejecutar evaluaciÃ³n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1098031,"status":"ok","timestamp":1746564132748,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"z-HKvebm_pDC","outputId":"0c9bd0a9-7e9e-4dd6-fc92-b752b6c2e746"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Iniciando evaluaciÃ³n en el Test Set (12 ejemplos) ---\n"]},{"name":"stderr","output_type":"stream","text":["Evaluando Test Set:   8%|â–Š         | 1/12 [00:55<10:08, 55.27s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0460\n","True label Violencia\n","Salida\n","1. AnÃ¡lisis de Componentes: La frase clave es \"invito a las putas\" y \"el que no sabÃ­a de mÃ­, me escucha y dice quien es ese\". Estas expresiones contienen palabras como \"putas\", que es una forma despectiva y vulgar de referirse a mujeres, y \"dice quien es ese\", que sugiere una actitud de menosprecio o discriminaciÃ³n hacia alguien desconocido.\n","\n","2. InterpretaciÃ³n y ConnotaciÃ³n: La palabra \"putas\" es una expresiÃ³n grosera y despectiva que implica una falta de respeto hacia mujeres, reduciÃ©ndolas a un estereotipo negativo. AdemÃ¡s, la frase \"dice quien es ese\" puede interpretarse como una forma de burla o discriminaciÃ³n hacia alguien que no es conocido, reflejando una actitud de superioridad o indiferencia hacia ellos.\n","\n","3. EvaluaciÃ³n contra S (Sexualization): Aunque hay una insinuaciÃ³n de invitaciÃ³n a relaciones sexuales implÃ­cita en \"invito a las putas\", el contexto general de la canciÃ³n no estÃ¡ centrado en contenido sexual. MÃ¡s bien, esta parte del anÃ¡lisis se centra en la naturaleza despectiva y discriminatoria de la frase.\n","\n","4. EvaluaciÃ³n contra V (Violence): No hay referencias directas a agresiÃ³n fÃ­sica, amenazas o actos violentos en la frase analizada. El tono es mÃ¡s bien de desdÃ©n y discriminaciÃ³n, pero no se menciona violencia fÃ­sica.\n","\n","5. EvaluaciÃ³n contra H (Hate): La frase contiene elementos de odio y discriminaciÃ³n. La palabra \"putas\" es una expresiÃ³n despectiva y discriminatoria hacia mujeres, mientras que \"dice quien es ese\" sugiere una actitud de indiferencia o desprecio hacia aquellos que no son conocidos. Ambas expresiones reflejan una actitud de hostilidad y desprecio hacia otros individuos, lo cual es consistente con la categorÃ­a de odio.\n","\n","6. SÃ­ntesis y JustificaciÃ³n Final: La frase analizada contiene elementos de odio debido a su uso de lenguaje despectivo y discriminatorio hacia mujeres y hacia aquellos que no son conocidos. La palabra \"putas\" es una expresiÃ³n grosera y despectiva que demuestra una falta de respeto hacia mujeres, mientras que \"dice quien es ese\" expresa una actitud de indiferencia o desprecio hacia desconocidos. Ambas expresiones reflejan una actitud de hostilidad y discriminaciÃ³n, lo cual justifica la clasificaciÃ³n bajo la categorÃ­a de odio. </think>Odio\n","-----------------------------------------------------------------------\n","ID T2_TRAIN_0701\n","True label Violencia\n","Salida\n","que se metiÃ³, lo mataron y a los otros les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un\n","---Reintento 1/4 para ID: T2_TRAIN_0701---\n","-----------------------------------------------------------------------\n","ID T2_TRAIN_0701\n","True label Violencia\n","Salida\n","que se metiÃ³, lo mataron y a los otros les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un\n","---Reintento 2/4 para ID: T2_TRAIN_0701---\n","-----------------------------------------------------------------------\n","ID T2_TRAIN_0701\n","True label Violencia\n","Salida\n","que se metiÃ³, lo mataron y a los otros les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un\n","---Reintento 3/4 para ID: T2_TRAIN_0701---\n","-----------------------------------------------------------------------\n","ID T2_TRAIN_0701\n","True label Violencia\n","Salida\n","que se metiÃ³, lo mataron y a los otros les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un\n","---Reintento 4/4 para ID: T2_TRAIN_0701---\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  17%|â–ˆâ–‹        | 2/12 [09:26<53:52, 323.28s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0701\n","True label Violencia\n","Salida\n","que se metiÃ³, lo mataron y a los otros les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  25%|â–ˆâ–ˆâ–Œ       | 3/12 [10:09<29:20, 195.66s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0839\n","True label Violencia\n","Salida\n","1. AnÃ¡lisis de Componentes: La frase clave es \"ahora si voy palante, ahora si voy paencima.\" El tÃ©rmino \"paencima\" puede interpretarse literalmente como \"hacia arriba\" o figurativamente como \"muy lejos\" o \"de gran altura.\" Sin embargo, en este contexto, podrÃ­a tener una connotaciÃ³n de dominancia o superioridad.\n","\n","2. InterpretaciÃ³n y ConnotaciÃ³n: La expresiÃ³n \"voy paencima\" sugiere una posiciÃ³n de ventaja o control sobre otra persona. En el contexto de la canciÃ³n, esto se refiere a la relaciÃ³n entre dos personas, donde uno estÃ¡ en una posiciÃ³n de poder o control sobre el otro. Esto implica una dinÃ¡mica de poder desequilibrada, lo cual puede ser visto como una forma de violencia emocional o psicolÃ³gica.\n","\n","3. EvaluaciÃ³n contra S (Sexualization): Aunque hay referencias a amor y atracciÃ³n, la frase no presenta elementos de sexualizaciÃ³n. No hay insinuaciones sexuales ni descripciones de naturaleza sexual.\n","\n","4. EvaluaciÃ³n contra V (Violence): La frase \"voy paencima\" sugiere una posiciÃ³n de dominancia o control, lo cual puede implicar una forma de violencia emocional o psicolÃ³gica. La idea de \"estar encima\" de alguien puede representar una dinÃ¡mica de poder abusiva, que es consistente con la categorÃ­a de violencia.\n","\n","5. EvaluaciÃ³n contra H (Hate): No hay expresiones de odio o discriminaciÃ³n en la frase. La canciÃ³n parece centrarse en el amor y la atracciÃ³n, sin mostrar hostilidad o desprecio hacia individuos o grupos.\n","\n","6. SÃ­ntesis y JustificaciÃ³n Final: La frase \"voy paencima\" se ajusta a la categorÃ­a de violencia debido a su connotaciÃ³n de dominancia y control sobre otra persona. Este tipo de dinÃ¡mica de poder desequilibrada es una forma de violencia emocional o psicolÃ³gica, lo cual justifica la asignaciÃ³n de la etiqueta 'V'. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  33%|â–ˆâ–ˆâ–ˆâ–      | 4/12 [11:10<18:59, 142.43s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0726\n","True label Odio\n","Salida\n","1. AnÃ¡lisis de Componentes: La frase contiene varias expresiones clave como \"estoy solo otra vez\", \"ahogado en mi soledad\", \"no es novedad\", \"antes ya me habÃ­an dejado\", \"antes ya me habÃ­a enborrachado\", \"quiero saber si alguien puede salvarme\", \"si hay una mujer que me saque de esta perra soledad\", \"quiero gritar porque no puedo mÃ¡s\", \"tu adios me ba matar\", \"regresa por piedad\", \"me encontraras en el mismo lugar\", \"si quieres regresar ahi te esperare\". Estas frases reflejan emociones de tristeza, soledad, depresiÃ³n y un deseo de compaÃ±Ã­a.\n","\n","2. InterpretaciÃ³n y ConnotaciÃ³n: Las palabras \"solo\", \"soledad\" y \"ahogado\" sugieren una sensaciÃ³n de aislamiento y angustia emocional. \"Me habÃ­an dejado\" y \"me habÃ­a enborrachado\" indican un patrÃ³n de fracaso en relaciones pasadas y posible autoabuso. \"Quiero gritar porque no puedo mÃ¡s\" expresa un estado de agotamiento emocional. \"Tu adios me ba matar\" podrÃ­a interpretarse literalmente o metafÃ³ricamente, sugiriendo que el impacto del adiÃ³s ha sido tan fuerte que casi mata al hablante. \"Regresa por piedad\" muestra una sÃºplica desesperada, mientras que \"si quieres regresar ahi te esperare\" refuerza el anhelo de compaÃ±Ã­a y estabilidad emocional.\n","\n","3. EvaluaciÃ³n contra S (Sexualization): No hay referencias o insinuaciones de naturaleza sexual en la letra. El enfoque estÃ¡ en el dolor emocional y la necesidad de compaÃ±Ã­a, no en temas sexuales.\n","\n","4. EvaluaciÃ³n contra V (Violence): Aunque hay una declaraciÃ³n de \"tu adios me ba matar\", esto se interpreta mÃ¡s bien como una metÃ¡fora poÃ©tica para expresar el impacto emocional devastador del adiÃ³s, en lugar de una amenaza real de violencia fÃ­sica. No hay menciones de agresiÃ³n o actos violentos.\n","\n","5. EvaluaciÃ³n contra H (Hate): No hay expresiones de odio, discriminaciÃ³n o hostilidad hacia individuos o grupos. La letra se centra en el sufrimiento personal y la bÃºsqueda de consuelo, sin dirigir sentimientos negativos hacia otros.\n","\n","6. SÃ­ntesis y JustificaciÃ³n Final: La letra pertenece a la categorÃ­a 'NR' debido a que no presenta elementos de sexualizaciÃ³n ('S'), violencia ('V') o odio ('H'). En cambio, se enfoca en el dolor emocional, la soledad y la sÃºplica de ayuda, lo cual es consistente con la categorÃ­a 'NR'. </think>No relacionado\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 5/12 [12:14<13:18, 114.09s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0403\n","True label Violencia\n","Salida\n","1. AnÃ¡lisis de Componentes: La letra describe una escena urbana donde Pedro Navaja, un personaje conocido por sus actos violentos, se enfrenta a una mujer que tambiÃ©n porta un arma. Se mencionan detalles sobre cÃ³mo ambos portan armas (puÃ±ales y revÃ³lveres) y cÃ³mo Pedro Navaja utiliza estas armas contra la mujer, culminando en un homicidio. AdemÃ¡s, se hace referencia a la violencia cotidiana en el barrio, con expresiones como \"el que no corre, vuela\" y \"la vida te da sorpresas\".\n","\n","2. InterpretaciÃ³n y ConnotaciÃ³n: El texto presenta una historia cargada de violencia y peligro. Pedro Navaja es retratado como un criminal que no duda en usar su arma para resolver conflictos, y la mujer tambiÃ©n resulta ser una figura violenta que intenta defenderse. La frase \"Â¡guiso fÃ¡cil!\" sugiere una falta de empatÃ­a y una actitud de indiferencia hacia la vÃ­ctima. Las referencias a la violencia en el barrio (\"cuidao en la acera\") y la metÃ¡fora de Kafka refuerzan la idea de una existencia marcada por la violencia y la fatalidad.\n","\n","3. EvaluaciÃ³n contra S (Sexualization): Aunque hay una interacciÃ³n entre hombres y mujeres, la letra no contiene insinuaciones sexuales ni se centra en aspectos de naturaleza sexual. Los encuentros entre los personajes estÃ¡n centrados en la confrontaciÃ³n y la violencia, no en relaciones Ã­ntimas o seducciÃ³n.\n","\n","4. EvaluaciÃ³n contra V (Violence): La letra estÃ¡ cargada de violencia. Pedro Navaja es descrito como un hombre que porta un puÃ±al y no duda en usarlo contra la mujer. La escena incluye un homicidio perpetrado con un puÃ±al, y la mujer tambiÃ©n responde con un disparo de revÃ³lver. AdemÃ¡s, el contexto general del barrio y las expresiones utilizadas (\"el que no corre, vuela\") refuerzan la percepciÃ³n de un ambiente violento y peligroso.\n","\n","5. EvaluaciÃ³n contra H (Hate): Aunque hay tensiones entre los personajes, no hay evidencias claras de odio basado en caracterÃ­sticas demogrÃ¡ficas como raza, gÃ©nero u origen Ã©tnico. La violencia parece estar motivada por el conflicto personal y situacional, no por sentimientos de hostilidad generalizados hacia un grupo especÃ­fico.\n","\n","6. SÃ­ntesis y JustificaciÃ³n Final: La letra pertenece a la categorÃ­a 'V' debido a su contenido extremadamente violento. La presencia constante de armas, el homicidio cometido por Pedro Navaja y la defensa violenta de la mujer son ejemplos claros de violencia. La narrativa refuerza la idea de un mundo lleno de peligros y amenazas, justificando asÃ­ la clasificaciÃ³n bajo la etiqueta de violencia. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 6/12 [13:22<09:49, 98.22s/it] "]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0453\n","True label Odio\n","Salida\n","coger un aviÃ³n pa llega a mi velocidad tu gente, par de dÃ­a esperando, y mira quÃ© asquerosidad\n","\n","            ### Respuesta:\n","            <think> 1. AnÃ¡lisis de Componentes: La frase incluye varias expresiones clave como \"yo te mato con lo tuyo, ratÃ³n, pero con limpieza de tu boca\", \"te entrÃ³ a mordÃ­a\", \"te asesino a sangre frÃ­a\", \"el que gana es el que goza y yo sigo con mi alegrÃ­a\", y \"yo te matÃ© encima de una pista\". Estas expresiones sugieren actos violentos, amenazas y referencias a la muerte.\n","\n","2. InterpretaciÃ³n y ConnotaciÃ³n: Las frases analizadas presentan un tono agresivo y hostil. \"Matar\" y \"asesinar\" son palabras fuertes que indican intencionalidad violenta. \"Te asesino a sangre frÃ­a\" sugiere una falta de empatÃ­a y una acciÃ³n premeditada. \"El que gana es el que goza y yo sigo con mi alegrÃ­a\" implica una justicia personal basada en la superioridad y la victoria sobre el otro. \"Yo te matÃ© encima de una pista\" refuerza la idea de dominancia y control.\n","\n","3. EvaluaciÃ³n contra S (Sexualization): Aunque hay referencias a relaciones personales y situaciones Ã­ntimas (\"tu mujer te acepta do hijo ajeno\"), estas no constituyen la base de la letra. El Ã©nfasis estÃ¡ en la violencia y las amenazas, no en insinuaciones sexuales.\n","\n","4. EvaluaciÃ³n contra V (Violence): Las expresiones analizadas claramente apuntan a la violencia. Frases como \"yo te mato con lo tuyo, ratÃ³n, pero con limpieza de tu boca\" y \"te asesino a sangre frÃ­a\" son ejemplos claros de amenazas de agresiÃ³n fÃ­sica. AdemÃ¡s, \"yo te matÃ© encima de una pista\" sugiere un acto de dominancia y control, que es consistente con la categorÃ­a de violencia.\n","\n","5. EvaluaciÃ³n contra H (Hate): Aunque hay cierto desprecio y hostilidad hacia el otro (\"lo lapicista\", \"volviste pa la suciedad\"), la letra no expresa odio en el sentido de hostilidad generalizada hacia un grupo o individuo por motivos de raza, gÃ©nero u otra discriminaciÃ³n. El foco estÃ¡ mÃ¡s en la competencia personal y la superioridad individual.\n","\n","6. SÃ­ntesis y JustificaciÃ³n Final: La letra pertenece a la categorÃ­a 'V' debido a las mÃºltiples expresiones de amenazas y actos violentos. Las frases analizadas reflejan un tono agresivo y hostil, con referencias directas a la muerte y la dominancia. Esto se alinea con la definiciÃ³n de violencia proporcionada, que incluye amenazas y glorificaciones de agresiÃ³n fÃ­sica. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 7/12 [14:14<06:56, 83.27s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0776\n","True label Odio\n","Salida\n","1. AnÃ¡lisis de Componentes: La frase clave es \"juegas al hombre fatal\" y \"sabes jugar a matar\". Estas expresiones implican un juego peligroso y potencialmente mortal, sugiriendo una dinÃ¡mica de riesgo y control emocional. El tÃ©rmino \"fatal\" implica resultados negativos o mortales, mientras que \"jugar a matar\" puede interpretarse literalmente o metafÃ³ricamente como una relaciÃ³n tÃ³xica o destructiva.\n","\n","2. InterpretaciÃ³n y ConnotaciÃ³n: La frase parece insinuar que el hablante estÃ¡ involucrado en una relaciÃ³n donde uno de los participantes ejerce control o manipulaciÃ³n emocional, similar a un juego de muerte. Esto puede connotar una dinÃ¡mica de poder desequilibrada, donde el otro individuo es tratado como si estuviera jugando con su vida o su bienestar emocional.\n","\n","3. EvaluaciÃ³n contra S (Sexualization): Aunque la frase podrÃ­a tener ciertos tonos seductores debido a la palabra \"fatal\", no hay referencias claras o explÃ­citas a contenido sexual. El contexto parece estar centrado mÃ¡s en el conflicto emocional que en la sexualizaciÃ³n.\n","\n","4. EvaluaciÃ³n contra V (Violence): La frase \"juegas al hombre fatal\" y \"sabes jugar a matar\" sugieren una dinÃ¡mica de control y posible abuso emocional. La metÃ¡fora del juego a matar puede indicar una situaciÃ³n violenta o tÃ³xica, aunque no necesariamente fÃ­sica. Sin embargo, la intenciÃ³n puede ser interpretar una violencia emocional o psicolÃ³gica, que tambiÃ©n cae bajo la categorÃ­a de violencia en este contexto.\n","\n","5. EvaluaciÃ³n contra H (Hate): No hay evidencias claras de odio en la frase proporcionada. Las palabras no contienen expresiones de hostilidad, discriminaciÃ³n o desprecio hacia individuos o grupos especÃ­ficos.\n","\n","6. SÃ­ntesis y JustificaciÃ³n Final: La frase se centra en una dinÃ¡mica de control y manipulaciÃ³n emocional, representada mediante la metÃ¡fora de un juego fatal. Aunque no hay violencia fÃ­sica explicita, la referencia a \"jugar a matar\" sugiere una violencia emocional o psicolÃ³gica, lo cual justifica la categorizaciÃ³n bajo 'V' por implicar un tipo de violencia no fÃ­sica pero significativa en el contexto de la letra. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 8/12 [14:58<04:43, 70.86s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_1130\n","True label Odio\n","Salida\n","1. AnÃ¡lisis de Componentes: La frase describe un conflicto emocional entre dos personas. Palabras clave incluyen \"tu amante\", \"tu amor\", \"tu vida\", \"tu retrato\", \"mala mujer\", \"matarlo\", \"llevarlo hasta tus pies\", \"me quedo y te tengo que matar\", \"te olvidas de mi nombre\". Estas palabras sugieren un contexto de traiciÃ³n y venganza.\n","\n","2. InterpretaciÃ³n y ConnotaciÃ³n: El habla estÃ¡ cargada de emociones negativas como rabia, dolor y frustraciÃ³n. Se menciona la intenciÃ³n de \"matar\" a alguien, lo cual es una expresiÃ³n clara de violencia. AdemÃ¡s, hay una referencia a \"mala mujer\", lo cual puede implicar juicios de valor negativos sobre la otra persona.\n","\n","3. EvaluaciÃ³n contra S (Sexualization): No hay indicios de contenido sexual en la frase. Las referencias son mÃ¡s bien emocionales y violentas, sin insinuaciones o descripciones de naturaleza sexual.\n","\n","4. EvaluaciÃ³n contra V (Violence): La frase contiene varias indicaciones de violencia potencial. La repetida menciÃ³n de querer \"matar\" a alguien es un ejemplo claro de violencia verbal y emocional. AdemÃ¡s, la idea de \"llevarlo hasta tus pies\" implica una acciÃ³n de dominancia o castigo, refuerzando la connotaciÃ³n de violencia.\n","\n","5. EvaluaciÃ³n contra H (Hate): Aunque hay un tono de hostilidad y desprecio (\"mala mujer\"), la frase no expresa odio en el sentido de hostilidad generalizada o discriminaciÃ³n hacia un grupo especÃ­fico. El odio aquÃ­ es mÃ¡s personal y centrado en una situaciÃ³n particular de traiciÃ³n.\n","\n","6. SÃ­ntesis y JustificaciÃ³n Final: La frase pertenece a la categorÃ­a 'V' debido a las mÃºltiples referencias a actos violentos, especialmente la intenciÃ³n de \"matar\" a alguien. Esto es un ejemplo directo de violencia verbal y emocional, que domina el anÃ¡lisis de la frase. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 9/12 [15:47<03:11, 63.92s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0069\n","True label Violencia\n","Salida\n","1. AnÃ¡lisis de Componentes: La frase analizada describe emocionalmente el proceso de pÃ©rdida de amor y dolor emocional. Palabras clave incluyen \"no vi las flores marchitar\", \"frÃ­o en tus ojos\", \"amor\", \"llorÃ©\", \"perderte\", \"jurÃ©\", \"negar\", \"pasiÃ³n\", \"esperando\" y \"nadie te ha amado\". Estas palabras sugieren sentimientos de tristeza, arrepentimiento y deseo de reconciliaciÃ³n.\n","\n","2. InterpretaciÃ³n y ConnotaciÃ³n: El contexto de la frase implica una historia de amor que terminÃ³ mal, donde el hablante expresa su pena y arrepentimiento por haber permitido que el amor terminara. La repeticiÃ³n de \"como yo, como yo nadie te ha amado\" refuerza la idea de que el hablante ha puesto todo su corazÃ³n en el amor, pero ahora estÃ¡ solo y herido. Sin embargo, no hay insinuaciones de agresiÃ³n, hostilidad o contenido sexual.\n","\n","3. EvaluaciÃ³n contra S (Sexualization): No hay referencias a actos sexuales, lenguaje erÃ³tico o insinuaciones de naturaleza sexual. Las expresiones se centran en el amor y el dolor emocional, sin ningÃºn componente de sexualizaciÃ³n.\n","\n","4. EvaluaciÃ³n contra V (Violence): Aunque se menciona el dolor emocional y el arrepentimiento, no hay descripciones de agresiÃ³n fÃ­sica, amenazas o actos violentos. La frase se enfoca en el impacto emocional del amor perdido, no en la violencia fÃ­sica o verbal.\n","\n","5. EvaluaciÃ³n contra H (Hate): No hay expresiones de hostilidad, discriminaciÃ³n o desprecio hacia individuos o grupos. La letra se centra en el amor y el dolor personal, sin mostrar odio hacia otra persona o grupo.\n","\n","6. SÃ­ntesis y JustificaciÃ³n Final: Dada la ausencia de elementos de sexualizaciÃ³n, violencia o odio en la frase, la letra pertenece claramente a la categorÃ­a 'NR' (No Related). La letra se centra en el dolor emocional y el arrepentimiento por el amor perdido, sin contener contenido sensible segÃºn las definiciones proporcionadas. </think>No relacionado\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 10/12 [16:34<01:57, 58.58s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0051\n","True label Odio\n","Salida\n","1. AnÃ¡lisis de Componentes: La frase contiene varias expresiones clave: \"me desespera\", \"me mata\", \"me enloquece\", \"hasta la vida diera por vencer el miedo de besarla a usted\". Estas frases sugieren emociones intensas y posibles actos de autolesividad o agresiÃ³n emocional.\n","\n","2. InterpretaciÃ³n y ConnotaciÃ³n: Las palabras \"me desespera\" y \"me mata\" pueden interpretarse literalmente o metafÃ³ricamente. En contexto, parecen indicar un estado emocional extremo, quizÃ¡s refiriÃ©ndose a la intensidad del amor o atracciÃ³n. Sin embargo, tambiÃ©n podrÃ­an insinuar un impacto negativo en la salud mental del hablante, tal vez inducido por la relaciÃ³n con otra persona.\n","\n","3. EvaluaciÃ³n contra S (Sexualization): No hay indicios claros de contenido sexual en la frase. Aunque se menciona \"el miedo de besarla a usted\", esto estÃ¡ centrado en el deseo y la ansiedad emocional, no en un contexto sexual explÃ­cito.\n","\n","4. EvaluaciÃ³n contra V (Violence): La frase utiliza expresiones como \"me desespera\", \"me mata\", y \"me enloquece\", que pueden implicar un impacto emocional violento o extremo. \"Me mata\" puede interpretarse literalmente, lo cual implica un acto de violencia real, aunque probablemente sea una metÃ¡fora poÃ©tica para expresar la intensidad emocional.\n","\n","5. EvaluaciÃ³n contra H (Hate): No hay evidencias de odio o discriminaciÃ³n en la frase. El tono es mÃ¡s bien de desesperaciÃ³n personal y afecto intenso hacia otra persona, sin mostrar hostilidad hacia individuos o grupos especÃ­ficos.\n","\n","6. SÃ­ntesis y JustificaciÃ³n Final: La frase presenta elementos que podrÃ­an caer bajo la categorÃ­a de violencia debido a las expresiones extremadas como \"me desespera\", \"me mata\", y \"me enloquece\". Aunque estas palabras pueden ser interpretadas metafÃ³ricamente, su naturaleza potencialmente violenta justifica la asignaciÃ³n a esta categorÃ­a. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 11/12 [17:19<00:54, 54.45s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0675\n","True label Odio\n","Salida\n","1. AnÃ¡lisis de Componentes: La frase incluye varias expresiones clave como \"mi negra\", \"una pociÃ³n\", \"mÃ¡gica ilusiÃ³n\", \"un toloache\" y \"vÃ¡monos\". Estas palabras sugieren un contexto de bÃºsqueda emocional y posible consumo de sustancias. \"Toloache\" es una bebida tradicional en algunas culturas, pero tambiÃ©n puede asociarse con efectos psicoactivos.\n","\n","2. InterpretaciÃ³n y ConnotaciÃ³n: El uso de \"mi negra\" podrÃ­a implicar una relaciÃ³n Ã­ntima o afectiva, mientras que \"una pociÃ³n\" y \"mÃ¡gica ilusiÃ³n\" sugieren un intento de encontrar consuelo o escape emocional. \"VÃ¡monos\" implica una urgencia o deseo de acciÃ³n inmediata, posiblemente relacionada con el consumo de sustancias para alcanzar ese estado deseado.\n","\n","3. EvaluaciÃ³n contra S (Sexualization): Aunque hay referencias a \"mi negra\", no hay insinuaciones claras de naturaleza sexual. Las frases parecen centradas en el deseo emocional y el consumo de sustancias mÃ¡s que en interacciones sexuales.\n","\n","4. EvaluaciÃ³n contra V (Violence): No hay indicios de agresiÃ³n, amenazas o actos violentos en la frase. El tono es mÃ¡s bien de buscar consuelo y escape, sin mencionar violencia.\n","\n","5. EvaluaciÃ³n contra H (Hate): No hay expresiones de hostilidad, discriminaciÃ³n o desprecio hacia individuos o grupos. La frase parece estar centrada en el deseo personal y no en el odio hacia otros.\n","\n","6. SÃ­ntesis y JustificaciÃ³n Final: La frase carece de elementos de sexualizaciÃ³n, violencia u odio. Se centra en el deseo de encontrar consuelo emocional y escape a travÃ©s del consumo de sustancias, lo cual no cae bajo las categorÃ­as de S, V o H. Por lo tanto, corresponde a la etiqueta 'NR' debido a la ausencia de contenido sensible en las categorÃ­as analizadas. </think>No relacionado\n"]},{"name":"stderr","output_type":"stream","text":["Evaluando Test Set: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [18:17<00:00, 91.50s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0159\n","True label Violencia\n","Salida\n","1. AnÃ¡lisis de Componentes: La frase incluye varias expresiones clave como \"yo te voy a raptar\", \"va a tocarme arrestarte\", \"un ak en la nalga va a tatuarse\", \"ella e la preferida\", \"bandida\", \"stripper\", y \"tre letra\". Estas palabras y frases sugieren referencias a secuestros, actos de posesiÃ³n, violencia, y comportamientos asociados con la cultura delincuencial.\n","\n","2. InterpretaciÃ³n y ConnotaciÃ³n: Las expresiones analizadas tienen connotaciones claras de violencia y hostilidad. \"Raptar\" implica secuestro, mientras que \"va a tocarme arrestarte\" sugiere un acto de dominancia o agresiÃ³n. \"Un ak en la nalga va a tatuarse\" puede interpretarse como una amenaza de usar un arma contra alguien. AdemÃ¡s, \"bandida\" y \"tre letra\" son tÃ©rminos comunes en la cultura del crimen organizado, refiriÃ©ndose a actividades ilegales y peligrosas.\n","\n","3. EvaluaciÃ³n contra S (Sexualization): Aunque hay menciones de \"stripper\" y \"ropa interior\", estas no constituyen la parte central de la letra. El contexto general de la frase no se centra en la sexualizaciÃ³n, sino mÃ¡s bien en la violencia y el crimen organizado.\n","\n","4. EvaluaciÃ³n contra V (Violence): La frase contiene mÃºltiples indicadores de violencia. \"Yo te voy a raptar\" y \"va a tocarme arrestarte\" son claramente amenazantes y sugieren actos violentos. \"Un ak en la nalga va a tatuarse\" tambiÃ©n implica el uso de armas y la intimidaciÃ³n. AdemÃ¡s, \"bandida\" y \"tre letra\" estÃ¡n ligados a la cultura delictuosa, refuerzando la presencia de violencia en la letra.\n","\n","5. EvaluaciÃ³n contra H (Hate): No hay evidencias claras de odio dirigido a individuos o grupos especÃ­ficos en la frase. Las expresiones se centran mÃ¡s en actos violentos y criminales en general, sin apuntar a un grupo determinado.\n","\n","6. SÃ­ntesis y JustificaciÃ³n Final: La frase pertenece a la categorÃ­a 'V' debido a su contenido explÃ­cito de violencia. Las expresiones como \"yo te voy a raptar\", \"va a tocarme arrestarte\", y \"un ak en la nalga va a tatuarse\" son claras referencias a actos violentos y amenazas. La combinaciÃ³n de estos elementos justifica la clasificaciÃ³n bajo la etiqueta de violencia. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# --- Listas para guardar resultados ---\n","true_labels = []\n","predicted_labels = []\n","failed_extractions_count = 0\n","test_results_details = []\n","\n","from tqdm import tqdm\n","import re\n","print(f\"\\n--- Iniciando evaluaciÃ³n en el Test Set ({len(testData)} ejemplos) ---\")\n","\n","for example in tqdm(testData, desc=\"Evaluando Test Set\"):\n","    try:\n","        lyrics = str(example[COLUMN_LETRA])\n","        true_label = str(example[COLUNM_ETIQUETALETRA]).strip()\n","\n","        if true_label == 'NR':\n","            text_label = 'No relacionado'\n","        elif true_label == 'V':\n","            text_label = 'Violencia'\n","        elif true_label == 'H':\n","            text_label = 'Odio'\n","        elif true_label == 'S':\n","            text_label = 'Sexualizacion'\n","\n","        # 1. Formatear el prompt de inferencia\n","        input_text = prompts_test[tipo_prompt].format(lyrics=str(lyrics))\n","\n","        # 2. Tokenizar\n","        inputs = tokenizer([input_text], return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(device)\n","\n","        pred_label = None\n","        retries = 0\n","\n","        while retries <= 4 and pred_label not in [\"NR\", \"V\", \"H\", \"S\"]:\n","            if retries > 0:\n","                print(f'---Reintento {retries}/4 para ID: {example[COLUMN_ID]}---')\n","            # 3. Generar la salida del modelo\n","            with torch.no_grad():\n","                outputs = model.generate(\n","                    input_ids=inputs.input_ids,\n","                    attention_mask=inputs.attention_mask,\n","                    temperature = 0.6,\n","                    max_new_tokens=1024,  # Suficiente para CoT + </think> + label\n","                    eos_token_id=tokenizer.eos_token_id,\n","                    pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n","                    do_sample=False,     # Forzar salida determinista para evaluaciÃ³n\n","                    use_cache=True,\n","                    repetition_penalty=1.1,\n","                )\n","\n","            # 4. Decodificar SOLO los tokens generados\n","            prompt_length = inputs.input_ids.shape[1]\n","            generated_ids = outputs[0, prompt_length:]\n","            decoded_output = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n","            print(\"-----------------------------------------------------------------------\")\n","            print(\"ID \"+example[COLUMN_ID])\n","            print(\"True label \" + text_label)\n","            print(\"Salida\\n\" + decoded_output)\n","            target_labels = [\"Sexualizacion\", \"Odio\", \"Violencia\", \"No relacionado\"]\n","            regex_pattern = r\"(?is)<\\/think>\\s*(.*)\"\n","            #regex_pattern = r\"(?i)</think>\\s*(\" + \"|\".join(target_labels) + r\")\\s*$\"\n","            match = re.search(regex_pattern, decoded_output)\n","            if match:\n","                pred_label = match.group(1).strip().capitalize()\n","            if pred_label not in target_labels:\n","                pred_label = None  # Forzar otro intento\n","                if retries == 4:\n","                    pred_label = 'No relacionado'\n","                retries += 1\n","                continue  # Ir al siguiente intento\n","\n","\n","            if pred_label == 'No relacionado':\n","                pred_label = 'NR'\n","            elif pred_label == 'Violencia':\n","                pred_label = 'V'\n","            elif pred_label == 'Odio':\n","                pred_label = 'H'\n","            elif pred_label == 'Sexualizacion':\n","                pred_label = 'S'\n","\n","            # 6. Almacenar resultados si la extracciÃ³n fue vÃ¡lida\n","            if pred_label in ('NR', 'V', 'H', 'S'):\n","                true_labels.append(true_label)\n","                predicted_labels.append(pred_label)\n","                test_results_details.append({\n","                    'id': example[COLUMN_ID],\n","                    'lyrics': lyrics,\n","                    'true_label': true_label,\n","                    'predicted_label': pred_label,\n","                    'full_output_decoded': decoded_output # Guardar salida completa para anÃ¡lisis\n","                })\n","\n","    except Exception as e:\n","        print(f\"\\nError procesando un ejemplo durante la evaluaciÃ³n: {e}\")\n","        failed_extractions_count += 1 # Contarlo como fallo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1438,"status":"ok","timestamp":1746564153656,"user":{"displayName":"Antonio Toro JaÃ©n","userId":"09221236473454991270"},"user_tz":-120},"id":"1GQ4MA8Y_pDC","outputId":"8789b724-2ffb-4f59-8449-fdcc2fab25ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Resultados de la EvaluaciÃ³n (Test Set) ---\n","Total de ejemplos evaluados: 11\n","Ejemplos donde fallÃ³ la extracciÃ³n de la etiqueta: 0\n","\n","Accuracy: 0.2727\n","F1 Score (Macro): 0.1667\n","F1 Score (Weighted): 0.2273\n","\n","Reporte de ClasificaciÃ³n Detallado:\n","                     precision    recall  f1-score   support\n","\n","No relacionado (NR)       0.00      0.00      0.00         0\n","  Sexualizacion (S)       0.00      0.00      0.00         0\n","      Violencia (V)       0.43      0.60      0.50         5\n","           Odio (H)       0.00      0.00      0.00         6\n","\n","           accuracy                           0.27        11\n","          macro avg       0.11      0.15      0.12        11\n","       weighted avg       0.19      0.27      0.23        11\n","\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 800x600 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","# --- Calcular y mostrar mÃ©tricas ---\n","print(f\"\\n--- Resultados de la EvaluaciÃ³n (Test Set) ---\")\n","print(f\"Total de ejemplos evaluados: {len(true_labels)}\")\n","print(f\"Ejemplos donde fallÃ³ la extracciÃ³n de la etiqueta: {failed_extractions_count}\")\n","\n","class_labels = ['NR', 'S', 'V', 'H']\n","class_labels_display = [\"No relacionado (NR)\", \"Sexualizacion (S)\", \"Violencia (V)\", \"Odio (H)\"]\n","\n","if not true_labels or not predicted_labels:\n","    print(\"\\nNo se pudieron calcular las mÃ©tricas (no hay predicciones vÃ¡lidas).\")\n","else:\n","    # Calcular mÃ©tricas usando sklearn\n","    accuracy = accuracy_score(true_labels, predicted_labels)\n","    f1_macro = f1_score(true_labels, predicted_labels, average='macro', zero_division=0)\n","    f1_weighted = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n","    report = classification_report(true_labels, predicted_labels, labels=class_labels, target_names=class_labels_display, zero_division=0)\n","    conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=class_labels)\n","\n","    print(f\"\\nAccuracy: {accuracy:.4f}\")\n","    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n","    print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n","    print(\"\\nReporte de ClasificaciÃ³n Detallado:\")\n","    print(report)\n","\n","    # Grafico de la matriz de confusiÃ³n\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Oranges', xticklabels=class_labels_display, yticklabels=class_labels_display)\n","    plt.ylabel('Etiqueta verdadera')\n","    plt.xlabel('Etiqueta predecida')\n","    plt.title('Matriz de confusiÃ³n Tarea 2')\n","    matriz_path = f\"{path}/matrices/prueba.png\"\n","    plt.savefig(matriz_path)\n","    plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["57hgk8HW_pC_","MBzYEoW5WjG_","VK2eEmIhJDai","355czYm-o-QH","VJR6rC2uibOw","JGpCfVNfifkF"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0752a00183c64d9b86c2979d73ee25e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dae58497ab641b09eaa2a253dd393c3","placeholder":"â€‹","style":"IPY_MODEL_ff2ba07063ee427299a1430f9fbfc6ea","value":"model-00003-of-00003.safetensors:â€‡100%"}},"0824a865c0ea4f52a763742f6ced64c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a07b601e5694cf48f25a316f76d7a76":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ccbee7735904b5d82f197b6d0747c98":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dae58497ab641b09eaa2a253dd393c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0efa7482772f4b74b52701c14de3176c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f2510c730e24e52941ce325080592cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1096ee5ce1ef4f74a4b1cc9143127403":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12cd7047a5c54d6486e83e2c0a01f015":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"178246b4898f4474beaf28a8f944e7cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17bfd2e0cb3d459e9ad58834b3f6869e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2359a187b3394c09b8031707aa974ebc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6c9ebf42670480b93b95056cef6983a","placeholder":"â€‹","style":"IPY_MODEL_eb8490d21bac4a75bb5a777e8d22bfab","value":"model-00002-of-00003.safetensors:â€‡100%"}},"23d53214d58c41eea95355ae35a56eda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca65e1341deb42c98db598f9134c3303","placeholder":"â€‹","style":"IPY_MODEL_9c8b77fa1841456c81f72ecb197b63fb","value":"model-00001-of-00003.safetensors:â€‡100%"}},"253ed70d5b4149cdb58619b1974ed1ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"29da4326c15d4b92b83870960d387010":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a07b601e5694cf48f25a316f76d7a76","max":4967447568,"min":0,"orientation":"horizontal","style":"IPY_MODEL_253ed70d5b4149cdb58619b1974ed1ee","value":4967447568}},"2a91de26d6aa49fdb29ac9f4200fc25f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35e8c3ae84ee4722bcc932a21bbae8a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0efa7482772f4b74b52701c14de3176c","placeholder":"â€‹","style":"IPY_MODEL_db0a2ba6f43343449e2b9dce9611fb28","value":"â€‡11.4M/11.4Mâ€‡[00:01&lt;00:00,â€‡6.65MB/s]"}},"38270cf0ec3c43608ee6e443dcd0a893":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a4809f0001646c0ab767ef0bd017fef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db309406b1254fc6811f02a2aaf0a057","IPY_MODEL_f56a778c3d2a45d08a34489d1c21d7fc","IPY_MODEL_cba192c28db44cec9434691a77dfe79c"],"layout":"IPY_MODEL_b6fbfbed0211459a8fc0d9d2b1d607c2"}},"40a0b050829940c3b593ab6ff2e5d1e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4135157c1d804e94947ee0b8eed4b9ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_79b6995f9e5c4d8c8bdaf6df1c75c29a","max":236,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af57b503abee4a7b843a1e531c1aefbd","value":236}},"43c2dcb9dfa24d08b220c0a6148f77c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6ea58ddcce84a25ad9af005b2510cb2","placeholder":"â€‹","style":"IPY_MODEL_609cfffdb0604df29ff4e1fd71b6c578","value":"model.safetensors.index.json:â€‡100%"}},"43d7def3f91447a8a159198a79a56d2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44b050dc6ac842feb9d7e5836f95429b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ccbee7735904b5d82f197b6d0747c98","placeholder":"â€‹","style":"IPY_MODEL_0f2510c730e24e52941ce325080592cb","value":"Loadingâ€‡checkpointâ€‡shards:â€‡100%"}},"4c610cbdd3ef4ab380561ff181c694c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f90974ee917b4ac6b32f8eec0dea6b39","max":11422778,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8bfddbe72ee94433b3674a5c27ea8ceb","value":11422778}},"4f107ec4d633447a9326fd7e6c8fb813":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"532da8e4c46145e5b3c334d787e73184":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c902d3d624e24f629a77ac7a0de612bc","placeholder":"â€‹","style":"IPY_MODEL_7dc9c7909dd048dd81d9db1a5adc0bbd","value":"â€‡236/236â€‡[00:00&lt;00:00,â€‡28.4kB/s]"}},"55194d41ce7242e294584b09aaa33859":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"563e94e1db65459b86db501ab5ad29de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17bfd2e0cb3d459e9ad58834b3f6869e","placeholder":"â€‹","style":"IPY_MODEL_9cbb6745209b4b4a8872a3a803c330f3","value":"â€‡4.94G/4.94Gâ€‡[00:23&lt;00:00,â€‡555MB/s]"}},"5656321d931f460cbf9bb816755dda4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57058c2b65a1490782c98279fa518a09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0752a00183c64d9b86c2979d73ee25e4","IPY_MODEL_7dd73dba05f94fbf99f849e20259cbb6","IPY_MODEL_d71bc31b01364c7ebb6af35fa6ac2de4"],"layout":"IPY_MODEL_a410f703ec494b24a0d22fcad7d2c718"}},"59ac5eb554214e56a88de281fef2728f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"609cfffdb0604df29ff4e1fd71b6c578":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62cd6f4b08cc4de88d8da6bb118f9c22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_44b050dc6ac842feb9d7e5836f95429b","IPY_MODEL_a087c2a202114633b1f5327c0e968737","IPY_MODEL_de6f8f182c4d4ea0843cf4a03f4e9771"],"layout":"IPY_MODEL_4f107ec4d633447a9326fd7e6c8fb813"}},"6f587a743ecb412f8a8baf700a648848":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fac066a886d4b6d9c00314b04424706":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"720228dc54bc46d1b29205410850b033":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75a8313258614396be18ace8509582b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a91de26d6aa49fdb29ac9f4200fc25f","placeholder":"â€‹","style":"IPY_MODEL_6fac066a886d4b6d9c00314b04424706","value":"generation_config.json:â€‡100%"}},"76c064c70b9a4872a30eae66f02f4753":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf56cd4e1d6f4d8f907d57c0914fbfa2","placeholder":"â€‹","style":"IPY_MODEL_40a0b050829940c3b593ab6ff2e5d1e7","value":"â€‡182k/182kâ€‡[00:00&lt;00:00,â€‡739kB/s]"}},"79b6995f9e5c4d8c8bdaf6df1c75c29a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b293025a01c487184e3923b9750153e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7dc9c7909dd048dd81d9db1a5adc0bbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7dd73dba05f94fbf99f849e20259cbb6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2c15a0b46be43daa8c8e45225bf9f09","max":4350046915,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1096ee5ce1ef4f74a4b1cc9143127403","value":4350046915}},"7f8950f8e14f47bf834f00f1feef2f4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"898c7aa44a61421896f9dc52b32c4f6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89e21db494784a13bb3c0f1ac3cd3f8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8bfddbe72ee94433b3674a5c27ea8ceb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d13ce5ab8424161aa3b0f5a5e3da0e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d75a97d43b7449ebb6de38c73df4cf7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9928e6506d144835bb23099ac4273c5e","placeholder":"â€‹","style":"IPY_MODEL_59ac5eb554214e56a88de281fef2728f","value":"special_tokens_map.json:â€‡100%"}},"8daa6bf63e6f4fd3abfd1a047f5857a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9031da16fc8241c8bcc5ff9faa1c25e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_43d7def3f91447a8a159198a79a56d2a","max":472,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b293025a01c487184e3923b9750153e","value":472}},"9111563aceda495ea3373f406c42e52d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75a8313258614396be18ace8509582b5","IPY_MODEL_4135157c1d804e94947ee0b8eed4b9ed","IPY_MODEL_532da8e4c46145e5b3c334d787e73184"],"layout":"IPY_MODEL_d48e0838e8944a1099df1bf49af9902e"}},"9394d1bd90514806b1d36bbd8d7226d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9803dd27aef04ed3b9824bb7102886ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23d53214d58c41eea95355ae35a56eda","IPY_MODEL_b780a36a3ea246aa9f8dc6dc4100b367","IPY_MODEL_563e94e1db65459b86db501ab5ad29de"],"layout":"IPY_MODEL_55194d41ce7242e294584b09aaa33859"}},"9928e6506d144835bb23099ac4273c5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c8b77fa1841456c81f72ecb197b63fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9cbb6745209b4b4a8872a3a803c330f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f5f356f76e146b4aaf4d97ce97a6270":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a087c2a202114633b1f5327c0e968737":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8daa6bf63e6f4fd3abfd1a047f5857a1","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab656ff5575940d3a30c8cf7e239631a","value":3}},"a15419a9a4e04d92a5ff9b0208c94d48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2359a187b3394c09b8031707aa974ebc","IPY_MODEL_29da4326c15d4b92b83870960d387010","IPY_MODEL_e5871d00596346c8bda863a97e106fc5"],"layout":"IPY_MODEL_e30074f251ee42fca775950cb0073988"}},"a410f703ec494b24a0d22fcad7d2c718":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5c6ab15b4fa485496d010863539ec52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_720228dc54bc46d1b29205410850b033","max":182401,"min":0,"orientation":"horizontal","style":"IPY_MODEL_89e21db494784a13bb3c0f1ac3cd3f8d","value":182401}},"ab656ff5575940d3a30c8cf7e239631a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af57b503abee4a7b843a1e531c1aefbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b1249e86d93043d38854703cd1289dfa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa23568eac23446aa17e36b670db2ce8","placeholder":"â€‹","style":"IPY_MODEL_178246b4898f4474beaf28a8f944e7cd","value":"tokenizer.json:â€‡100%"}},"b6c9ebf42670480b93b95056cef6983a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6fbfbed0211459a8fc0d9d2b1d607c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b780a36a3ea246aa9f8dc6dc4100b367":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3c2d518fee34df187e83afc3224fd3c","max":4941037119,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e8cc49716c2e44a0bc81aa3e7759f6dd","value":4941037119}},"bf56cd4e1d6f4d8f907d57c0914fbfa2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2c15a0b46be43daa8c8e45225bf9f09":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c442ee20120d411aac058cb14ecd1d50":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12cd7047a5c54d6486e83e2c0a01f015","placeholder":"â€‹","style":"IPY_MODEL_daa4842fa31b4ae2b339c24e668e9e19","value":"â€‡472/472â€‡[00:00&lt;00:00,â€‡57.1kB/s]"}},"c6ea58ddcce84a25ad9af005b2510cb2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c902d3d624e24f629a77ac7a0de612bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c997e3afe6af48d78a43bcb0f9d805b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca65e1341deb42c98db598f9134c3303":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cba192c28db44cec9434691a77dfe79c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf589c973cfe4d92a527243bcd7ac737","placeholder":"â€‹","style":"IPY_MODEL_0824a865c0ea4f52a763742f6ced64c1","value":"â€‡6.78k/6.78kâ€‡[00:00&lt;00:00,â€‡861kB/s]"}},"cf589c973cfe4d92a527243bcd7ac737":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d261bc71b45d40df94e1ff0038ffc45d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d331a9a675a74c7faee91bad2659246c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d48e0838e8944a1099df1bf49af9902e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d71bc31b01364c7ebb6af35fa6ac2de4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e685393f4d6e46b6bfeb08a3b2674e8b","placeholder":"â€‹","style":"IPY_MODEL_c997e3afe6af48d78a43bcb0f9d805b0","value":"â€‡4.35G/4.35Gâ€‡[00:19&lt;00:00,â€‡221MB/s]"}},"d8ce0dad3c8548708cd7722ea2e256df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d75a97d43b7449ebb6de38c73df4cf7","IPY_MODEL_9031da16fc8241c8bcc5ff9faa1c25e6","IPY_MODEL_c442ee20120d411aac058cb14ecd1d50"],"layout":"IPY_MODEL_8d13ce5ab8424161aa3b0f5a5e3da0e1"}},"daa4842fa31b4ae2b339c24e668e9e19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db0a2ba6f43343449e2b9dce9611fb28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db309406b1254fc6811f02a2aaf0a057":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d331a9a675a74c7faee91bad2659246c","placeholder":"â€‹","style":"IPY_MODEL_9394d1bd90514806b1d36bbd8d7226d9","value":"tokenizer_config.json:â€‡100%"}},"de6f8f182c4d4ea0843cf4a03f4e9771":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d261bc71b45d40df94e1ff0038ffc45d","placeholder":"â€‹","style":"IPY_MODEL_898c7aa44a61421896f9dc52b32c4f6c","value":"â€‡3/3â€‡[00:04&lt;00:00,â€‡â€‡1.60s/it]"}},"deb48c3656054abd8bb022564fd2940c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e30074f251ee42fca775950cb0073988":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3c2d518fee34df187e83afc3224fd3c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5871d00596346c8bda863a97e106fc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f587a743ecb412f8a8baf700a648848","placeholder":"â€‹","style":"IPY_MODEL_7f8950f8e14f47bf834f00f1feef2f4a","value":"â€‡4.97G/4.97Gâ€‡[00:47&lt;00:00,â€‡369MB/s]"}},"e685393f4d6e46b6bfeb08a3b2674e8b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8cc49716c2e44a0bc81aa3e7759f6dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8fc7a0dd21840c4aa7e95fd37d312c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b1249e86d93043d38854703cd1289dfa","IPY_MODEL_4c610cbdd3ef4ab380561ff181c694c6","IPY_MODEL_35e8c3ae84ee4722bcc932a21bbae8a0"],"layout":"IPY_MODEL_9f5f356f76e146b4aaf4d97ce97a6270"}},"eb8490d21bac4a75bb5a777e8d22bfab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f56a778c3d2a45d08a34489d1c21d7fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_deb48c3656054abd8bb022564fd2940c","max":6782,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5656321d931f460cbf9bb816755dda4c","value":6782}},"f90974ee917b4ac6b32f8eec0dea6b39":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa23568eac23446aa17e36b670db2ce8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fec76fe1626f4faa986607fecd5bb6f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_43c2dcb9dfa24d08b220c0a6148f77c1","IPY_MODEL_a5c6ab15b4fa485496d010863539ec52","IPY_MODEL_76c064c70b9a4872a30eae66f02f4753"],"layout":"IPY_MODEL_38270cf0ec3c43608ee6e443dcd0a893"}},"ff2ba07063ee427299a1430f9fbfc6ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}