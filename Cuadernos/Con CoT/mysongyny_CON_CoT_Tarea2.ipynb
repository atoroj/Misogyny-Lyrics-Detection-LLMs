{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56074,"status":"ok","timestamp":1746721737125,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"5ME2PUEUAOUs","outputId":"3a7b155f-0589-4d42-d0bf-408f0af82dc2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Montar directorio de drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"14a8bgNf_ylG","executionInfo":{"status":"ok","timestamp":1746721967749,"user_tz":-120,"elapsed":230622,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"}},"outputId":"5624a678-9c53-4f94-d18e-8e441306c495"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting unsloth\n","  Downloading unsloth-2025.4.7-py3-none-any.whl.metadata (46 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/46.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting unsloth_zoo>=2025.4.4 (from unsloth)\n","  Downloading unsloth_zoo-2025.4.4-py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\n","Collecting xformers>=0.0.27.post2 (from unsloth)\n","  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n","Collecting bitsandbytes (from unsloth)\n","  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n","Collecting tyro (from unsloth)\n","  Downloading tyro-0.9.19-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: transformers!=4.47.0,>=4.46.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.51.3)\n","Collecting datasets>=2.16.0 (from unsloth)\n","  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n","Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.0.2)\n","Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.6.0)\n","Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9 (from unsloth)\n","  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\n","Collecting protobuf<4.0.0 (from unsloth)\n","  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.30.2)\n","Collecting hf_transfer (from unsloth)\n","  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.33.1)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->unsloth)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->unsloth) (2.32.3)\n","Collecting xxhash (from datasets>=2.16.0->unsloth)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets>=2.16.0->unsloth)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth)\n","  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,>=4.46.1->unsloth) (0.21.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (13.9.4)\n","Collecting cut_cross_entropy (from unsloth_zoo>=2025.4.4->unsloth)\n","  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.4.4->unsloth) (11.2.1)\n","Collecting msgspec (from unsloth_zoo>=2025.4.4->unsloth)\n","  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n","Collecting torch>=2.4.0 (from unsloth)\n","  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n","Collecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n","  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n","Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth)\n","  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n","  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n","Collecting nvidia-nccl-cu12==2.26.2 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n","Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n","  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.4.0->unsloth)\n","  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n","Collecting triton>=3.0.0 (from unsloth)\n","  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth) (75.2.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\n","INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n","Collecting torchvision (from unsloth)\n","  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n","Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n","Collecting shtab>=1.5.6 (from tyro->unsloth)\n","  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (3.11.15)\n","Collecting hf-xet>=0.1.4 (from huggingface_hub[hf_xet]>=0.30.0->unsloth_zoo>=2025.4.4->unsloth)\n","  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->unsloth) (2025.4.26)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (2.19.1)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->unsloth) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->unsloth) (1.20.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,<=0.15.2,>=0.7.9->unsloth) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->unsloth) (1.17.0)\n","Downloading unsloth-2025.4.7-py3-none-any.whl (218 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.5/218.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading unsloth_zoo-2025.4.4-py3-none-any.whl (129 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m69.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tyro-0.9.19-py3-none-any.whl (124 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n","Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n","Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, xxhash, triton, sympy, shtab, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, hf-xet, hf_transfer, fsspec, dill, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, multiprocess, tyro, nvidia-cusolver-cu12, torch, datasets, xformers, torchvision, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n","  Attempting uninstall: nvidia-cusparselt-cu12\n","    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n","    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n","      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.13.1\n","    Uninstalling sympy-1.13.1:\n","      Successfully uninstalled sympy-1.13.1\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 5.29.4\n","    Uninstalling protobuf-5.29.4:\n","      Successfully uninstalled protobuf-5.29.4\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.4.127\n","    Uninstalling nvidia-nvtx-cu12-12.4.127:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.21.5\n","    Uninstalling nvidia-nccl-cu12-2.21.5:\n","      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.6.0+cu124\n","    Uninstalling torch-2.6.0+cu124:\n","      Successfully uninstalled torch-2.6.0+cu124\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.21.0+cu124\n","    Uninstalling torchvision-0.21.0+cu124:\n","      Successfully uninstalled torchvision-0.21.0+cu124\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n","grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n","tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n","ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed bitsandbytes-0.45.5 cut_cross_entropy-25.1.1 datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 hf-xet-1.1.0 hf_transfer-0.1.9 msgspec-0.19.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 protobuf-3.20.3 shtab-1.7.2 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 triton-3.3.0 trl-0.15.2 tyro-0.9.19 unsloth-2025.4.7 unsloth_zoo-2025.4.4 xformers-0.0.30 xxhash-3.5.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["google"]},"id":"831c1fa8247d4811a41e5226947bde55"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/unslothai/unsloth.git\n","  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-8779irwe\n","  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-8779irwe\n","  Resolved https://github.com/unslothai/unsloth.git to commit b9067f98d124173b354808b116f931410e5b699c\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: unsloth\n","  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unsloth: filename=unsloth-2025.4.8-py3-none-any.whl size=263211 sha256=008af0d752b8855241c659d0c045a2c46ebafb8ef0e935786395ac1e26363af2\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-jmevykry/wheels/d1/17/05/850ab10c33284a4763b0595cd8ea9d01fce6e221cac24b3c01\n","Successfully built unsloth\n","Installing collected packages: unsloth\n","  Attempting uninstall: unsloth\n","    Found existing installation: unsloth 2025.4.7\n","    Uninstalling unsloth-2025.4.7:\n","      Successfully uninstalled unsloth-2025.4.7\n","Successfully installed unsloth-2025.4.8\n"]}],"source":["!pip install unsloth\n","!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5240,"status":"ok","timestamp":1746721972988,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"dhS_9MK-2ikI","outputId":"d82b5fac-0523-407b-d9ba-bd8c95cc5a02"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-dotenv\n","  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n","Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n","Installing collected packages: python-dotenv\n","Successfully installed python-dotenv-1.1.0\n"]}],"source":["!pip install python-dotenv"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3921,"status":"ok","timestamp":1746721976925,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"WfDykUE6n4u_","outputId":"4c205bba-06be-464d-a4d3-70b7a6572a62"},"outputs":[{"output_type":"stream","name":"stdout","text":["Hay 1 GPU(s) disponibles.\n","Vamos a usar la GPU: Tesla T4\n"]}],"source":["import torch\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print('Hay %d GPU(s) disponibles.' % torch.cuda.device_count())\n","    print('Vamos a usar la GPU:', torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print('No hay GPU disponible, usando CPU.')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49702,"status":"ok","timestamp":1746722026632,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"z1-ZXv6fBQ8l","outputId":"9215134f-5506-4cdb-8c59-b613e09105dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","🦥 Unsloth Zoo will now patch everything to make training faster!\n"]}],"source":["from unsloth import FastLanguageModel, is_bfloat16_supported\n","import torch\n","from trl import SFTTrainer\n","\n","from huggingface_hub import login\n","from transformers import TrainingArguments, EarlyStoppingCallback, AutoModelForCausalLM, AutoTokenizer\n","from datasets import load_dataset\n","import wandb\n","import pandas as pd\n","from datasets import Dataset\n","from dotenv import load_dotenv\n","import os\n","from sklearn.utils import resample\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, f1_score, average_precision_score, accuracy_score, precision_recall_fscore_support, precision_score, recall_score"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Op1WzBwFJb3W","executionInfo":{"status":"ok","timestamp":1746722026633,"user_tz":-120,"elapsed":2,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"}}},"outputs":[],"source":["path = '/content/drive/Othercomputers/Mi PC/TFG/MySonGyny/MySonGyny-2025'\n","#path = '../'\n","#path = '/home/lobezno/Documentos'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RX5Y2kTD2IPR"},"outputs":[],"source":["load_dotenv(f'{path}/cuadernos/.env')  # Carga las variables del .env\n","\n","HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n","WANDB_TOKEN = os.getenv(\"WANDB_TOKEN\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZpMlasSAadz"},"outputs":[],"source":["hf_token = HUGGINGFACE_TOKEN\n","login(hf_token)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":213},"executionInfo":{"elapsed":4359,"status":"ok","timestamp":1746530369511,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"xcVPd0h9Ajz2","outputId":"eced1d03-8693-4cbb-b164-ecd99c43ca0f"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33matorojaen\u001b[0m (\u001b[33matorojaen-university-of-huelva\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.19.10"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20250506_111927-c6f61n2p</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2/runs/c6f61n2p?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3' target=\"_blank\">charmed-energy-5</a></strong> to <a href='https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3' target=\"_blank\">https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2/runs/c6f61n2p?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3' target=\"_blank\">https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2/runs/c6f61n2p?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Do NOT share these links with anyone. They can be used to claim your runs."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["wb_token = WANDB_TOKEN\n","\n","wandb.login(key=wb_token)\n","run = wandb.init(\n","    project='Fine-tune-con-CoT-Tarea-2',\n","    job_type=\"training\",\n","    anonymous=\"allow\"\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"WEvLF7Pg_pC-","executionInfo":{"status":"ok","timestamp":1746722026635,"user_tz":-120,"elapsed":1,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"}}},"outputs":[],"source":["COLUMN_ID = 'id'\n","COLUMN_LETRA = 'lyrics'\n","COLUMN_ETIQUETA = 'label'\n","COLUNM_ETIQUETALETRA = 'labelconletras'\n","COLUMN_RAZONAMIENTO = 'reasoning'\n","tipo_CoT = 'GPT4o'\n","model_name = 'DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit'\n","csv_train_filepath = f'{path}/data/train_data/task2_GPT4o_dataReasoning.csv'\n","tipo_prompt = 0\n","#csv_test_filepath = f'{path}/data/train_data/testData.csv'"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3726,"status":"ok","timestamp":1746722030362,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"MH0L24sv_pC_","outputId":"78056c01-f433-474d-e870-d72d363cfde9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cargando datos desde: /content/drive/Othercomputers/Mi PC/TFG/MySonGyny/MySonGyny-2025/data/train_data/task2_GPT4o_dataReasoning.csv\n","Columnas encontradas en el CSV: ['id', 'lyrics', 'reasoning', 'label', 'labelconletras']\n","Total de filas válidas antes de dividir: 1168\n","División completada:\n","  - Tamaño del conjunto de Entrenamiento: 653\n","  - Tamaño del conjunto de Validación:  281\n","  - Tamaño del conjunto de Test:         234\n","\n","--- Distribución de Clases ---\n","Conjunto de Entrenamiento:\n","labelconletras\n","NR    294\n","S     243\n","V      72\n","H      44\n","Name: count, dtype: int64\n","\n","Conjunto de Validación:\n","labelconletras\n","NR    127\n","S     105\n","V      31\n","H      18\n","Name: count, dtype: int64\n","\n","Conjunto de Test:\n","labelconletras\n","NR    105\n","S      87\n","V      26\n","H      16\n","Name: count, dtype: int64\n","-----------------------------\n","\n","Conjunto de Entrenamiento nuevo:\n","labelconletras\n","NR    294\n","S     243\n","V     198\n","H     156\n","Name: count, dtype: int64\n","------------------------------------------\n","\n","Datasets creados exitosamente.\n","Columnas en trainData: ['id', 'lyrics', 'reasoning', 'label', 'labelconletras']\n"]}],"source":["print(f\"Cargando datos desde: {csv_train_filepath}\")\n","try:\n","    # 1. Cargar el DataFrame separado\n","    df = pd.read_csv(csv_train_filepath, encoding='utf-8', encoding_errors=\"replace\", sep=',')\n","    #test_df = pd.read_csv(csv_test_filepath, encoding='utf-8', encoding_errors=\"replace\", sep=',')\n","    print(f\"Columnas encontradas en el CSV: {df.columns.tolist()}\")\n","\n","    # 2. Dividir los datos\n","    print(f\"Total de filas válidas antes de dividir: {len(df)}\")\n","    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[COLUMN_ETIQUETA])\n","    train_df, valid_df = train_test_split(train_df, test_size=0.3, random_state=42, stratify=train_df[COLUMN_ETIQUETA])\n","    print(f\"División completada:\")\n","    print(f\"  - Tamaño del conjunto de Entrenamiento: {len(train_df)}\")\n","    print(f\"  - Tamaño del conjunto de Validación:  {len(valid_df)}\")\n","    print(f\"  - Tamaño del conjunto de Test:         {len(test_df)}\")\n","\n","     # --- COMPROBACIÓN DE PROPORCIONES DE CLASE ---\n","    print(\"\\n--- Distribución de Clases ---\")\n","    print(\"Conjunto de Entrenamiento:\")\n","    print(train_df[COLUNM_ETIQUETALETRA].value_counts(normalize=False)) # Conteos absolutos\n","\n","    print(\"\\nConjunto de Validación:\")\n","    print(valid_df[COLUNM_ETIQUETALETRA].value_counts(normalize=False))\n","\n","    print(\"\\nConjunto de Test:\")\n","    print(test_df[COLUNM_ETIQUETALETRA].value_counts(normalize=False))\n","    print(\"-----------------------------\")\n","\n","    nr_class_df = train_df[train_df[COLUMN_ETIQUETA] == 0]\n","    s_class_df = train_df[train_df[COLUMN_ETIQUETA] == 1]\n","    v_class_df = train_df[train_df[COLUMN_ETIQUETA] == 2]\n","    h_class_df = train_df[train_df[COLUMN_ETIQUETA] == 3]\n","\n","    #----PARA HACER OVERSAMPLING---\n","    v_oversampled_df = resample(v_class_df,\n","                                       replace=True,     # Permitir duplicados\n","                                       n_samples=198,\n","                                       random_state=42)\n","    h_oversampled_df = resample(h_class_df,\n","                                       replace=True,     # Permitir duplicados\n","                                       n_samples=156,\n","                                       random_state=42)\n","\n","    train_df_balanced = pd.concat([nr_class_df, s_class_df, v_oversampled_df, h_oversampled_df])\n","\n","    #----PARA HACER UNDERSAMPLING---\n","    #majority_undersampled_df = resample(majority_class_df,\n","    #                                replace=False,  # Sin duplicados\n","    #                                n_samples=600,  # Mismo tamaño que minoritaria\n","    #                                random_state=42)\n","\n","    #train_df_balanced = pd.concat([majority_undersampled_df, minority_oversampled_df])\n","\n","\n","    # Mezclar el DataFrame resultante para que las clases no estén agrupadas\n","    train_df_balanced = train_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","    print(\"\\nConjunto de Entrenamiento nuevo:\")\n","    print(train_df_balanced[COLUNM_ETIQUETALETRA].value_counts(normalize=False))\n","    print(\"------------------------------------------\")\n","\n","    # 3. Convertir a datasets.Dataset\n","    trainData = Dataset.from_pandas(train_df_balanced)\n","    validData = Dataset.from_pandas(valid_df)\n","    testData = Dataset.from_pandas(test_df) # Guarda el de test también\n","\n","    print(\"\\nDatasets creados exitosamente.\")\n","    print(f\"Columnas en trainData: {trainData.column_names}\")\n","except FileNotFoundError:\n","    print(f\"Error: Archivo no encontrado en '{csv_train_filepath}'\")\n","except Exception as e:\n","    print(f\"Error inesperado durante la carga o división de datos: {e}\")\n","    import traceback\n","    traceback.print_exc()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"B1doBf3zD3Te","executionInfo":{"status":"ok","timestamp":1746722030363,"user_tz":-120,"elapsed":3,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"}}},"outputs":[],"source":["max_seq_length = 2048\n","dtype = None\n","load_in_4bit = True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":414,"referenced_widgets":["fec76fe1626f4faa986607fecd5bb6f9","43c2dcb9dfa24d08b220c0a6148f77c1","a5c6ab15b4fa485496d010863539ec52","76c064c70b9a4872a30eae66f02f4753","38270cf0ec3c43608ee6e443dcd0a893","c6ea58ddcce84a25ad9af005b2510cb2","609cfffdb0604df29ff4e1fd71b6c578","720228dc54bc46d1b29205410850b033","89e21db494784a13bb3c0f1ac3cd3f8d","bf56cd4e1d6f4d8f907d57c0914fbfa2","40a0b050829940c3b593ab6ff2e5d1e7","9803dd27aef04ed3b9824bb7102886ea","23d53214d58c41eea95355ae35a56eda","b780a36a3ea246aa9f8dc6dc4100b367","563e94e1db65459b86db501ab5ad29de","55194d41ce7242e294584b09aaa33859","ca65e1341deb42c98db598f9134c3303","9c8b77fa1841456c81f72ecb197b63fb","e3c2d518fee34df187e83afc3224fd3c","e8cc49716c2e44a0bc81aa3e7759f6dd","17bfd2e0cb3d459e9ad58834b3f6869e","9cbb6745209b4b4a8872a3a803c330f3","a15419a9a4e04d92a5ff9b0208c94d48","2359a187b3394c09b8031707aa974ebc","29da4326c15d4b92b83870960d387010","e5871d00596346c8bda863a97e106fc5","e30074f251ee42fca775950cb0073988","b6c9ebf42670480b93b95056cef6983a","eb8490d21bac4a75bb5a777e8d22bfab","0a07b601e5694cf48f25a316f76d7a76","253ed70d5b4149cdb58619b1974ed1ee","6f587a743ecb412f8a8baf700a648848","7f8950f8e14f47bf834f00f1feef2f4a","57058c2b65a1490782c98279fa518a09","0752a00183c64d9b86c2979d73ee25e4","7dd73dba05f94fbf99f849e20259cbb6","d71bc31b01364c7ebb6af35fa6ac2de4","a410f703ec494b24a0d22fcad7d2c718","0dae58497ab641b09eaa2a253dd393c3","ff2ba07063ee427299a1430f9fbfc6ea","c2c15a0b46be43daa8c8e45225bf9f09","1096ee5ce1ef4f74a4b1cc9143127403","e685393f4d6e46b6bfeb08a3b2674e8b","c997e3afe6af48d78a43bcb0f9d805b0","62cd6f4b08cc4de88d8da6bb118f9c22","44b050dc6ac842feb9d7e5836f95429b","a087c2a202114633b1f5327c0e968737","de6f8f182c4d4ea0843cf4a03f4e9771","4f107ec4d633447a9326fd7e6c8fb813","0ccbee7735904b5d82f197b6d0747c98","0f2510c730e24e52941ce325080592cb","8daa6bf63e6f4fd3abfd1a047f5857a1","ab656ff5575940d3a30c8cf7e239631a","d261bc71b45d40df94e1ff0038ffc45d","898c7aa44a61421896f9dc52b32c4f6c","9111563aceda495ea3373f406c42e52d","75a8313258614396be18ace8509582b5","4135157c1d804e94947ee0b8eed4b9ed","532da8e4c46145e5b3c334d787e73184","d48e0838e8944a1099df1bf49af9902e","2a91de26d6aa49fdb29ac9f4200fc25f","6fac066a886d4b6d9c00314b04424706","79b6995f9e5c4d8c8bdaf6df1c75c29a","af57b503abee4a7b843a1e531c1aefbd","c902d3d624e24f629a77ac7a0de612bc","7dc9c7909dd048dd81d9db1a5adc0bbd","3a4809f0001646c0ab767ef0bd017fef","db309406b1254fc6811f02a2aaf0a057","f56a778c3d2a45d08a34489d1c21d7fc","cba192c28db44cec9434691a77dfe79c","b6fbfbed0211459a8fc0d9d2b1d607c2","d331a9a675a74c7faee91bad2659246c","9394d1bd90514806b1d36bbd8d7226d9","deb48c3656054abd8bb022564fd2940c","5656321d931f460cbf9bb816755dda4c","cf589c973cfe4d92a527243bcd7ac737","0824a865c0ea4f52a763742f6ced64c1","e8fc7a0dd21840c4aa7e95fd37d312c8","b1249e86d93043d38854703cd1289dfa","4c610cbdd3ef4ab380561ff181c694c6","35e8c3ae84ee4722bcc932a21bbae8a0","9f5f356f76e146b4aaf4d97ce97a6270","fa23568eac23446aa17e36b670db2ce8","178246b4898f4474beaf28a8f944e7cd","f90974ee917b4ac6b32f8eec0dea6b39","8bfddbe72ee94433b3674a5c27ea8ceb","0efa7482772f4b74b52701c14de3176c","db0a2ba6f43343449e2b9dce9611fb28","d8ce0dad3c8548708cd7722ea2e256df","8d75a97d43b7449ebb6de38c73df4cf7","9031da16fc8241c8bcc5ff9faa1c25e6","c442ee20120d411aac058cb14ecd1d50","8d13ce5ab8424161aa3b0f5a5e3da0e1","9928e6506d144835bb23099ac4273c5e","59ac5eb554214e56a88de281fef2728f","43d7def3f91447a8a159198a79a56d2a","7b293025a01c487184e3923b9750153e","12cd7047a5c54d6486e83e2c0a01f015","daa4842fa31b4ae2b339c24e668e9e19"]},"executionInfo":{"elapsed":117795,"status":"ok","timestamp":1746530586491,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"RjPCizQ6EQ9E","outputId":"000ff8fa-1d5b-4465-d26a-3f95bed2a8d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["==((====))==  Unsloth 2025.4.8: Fast Qwen2 patching. Transformers: 4.51.3.\n","   \\\\   /|    NVIDIA L4. Num GPUs = 1. Max memory: 22.161 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n","\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fec76fe1626f4faa986607fecd5bb6f9","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/182k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9803dd27aef04ed3b9824bb7102886ea","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a15419a9a4e04d92a5ff9b0208c94d48","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"57058c2b65a1490782c98279fa518a09","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/4.35G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"62cd6f4b08cc4de88d8da6bb118f9c22","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9111563aceda495ea3373f406c42e52d","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a4809f0001646c0ab767ef0bd017fef","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/6.78k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8fc7a0dd21840c4aa7e95fd37d312c8","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d8ce0dad3c8548708cd7722ea2e256df","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = f\"unsloth/{model_name}\",  # Nombre del modelo en Hugging Face (modelo LLaMA distilado y más ligero)\n","    max_seq_length = max_seq_length,                      # Longitud máxima de secuencia que puede procesar (por ejemplo 2048 o 4096 tokens)\n","    dtype = dtype,                                        # Tipo de precisión para los cálculos (por ejemplo: torch.float16 o torch.bfloat16)\n",")"]},{"cell_type":"markdown","metadata":{"id":"57hgk8HW_pC_"},"source":["# Prueba con el modelo sin entrenar"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"otsU1wn1n4vB"},"outputs":[],"source":["promp_style = \"\"\"### Instruccion\n","            Analiza la siguiente letra de canción y determina si contiene contenido misógino. Evalúa si incluye lenguaje, actitudes o mensajes que:\n","              - Degraden o deshumanicen a las mujeres.\n","              - Menosprecien a las mujeres de manera explícita o implícita.\n","              - Refuercen estereotipos negativos o dañinos sobre las mujeres.\n","              - Promuevan violencia física, emocional o sexual contra las mujeres.\n","\n","            Primero, piensa paso a paso tu razonamiento.\n","            DESPUÉS de tu razonamiento interno, responde ÚNICA Y EXCLUSIVAMENTE con \"1\" si la letra es misógina o con \"0\" si la letra no es misógina. Tu respuesta final debe ser solo un dígito.\n","\n","            ### Letra:\n","            {}\n","\n","            ### Respuesta: <think>\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"znSFjuBfF_Ei"},"outputs":[],"source":["cancion = \"\"\"a de nuevo solos tú y yo, un lago y una canción echo de menos oír tu voz, una estrella te eclipsó los momentos que no volverán a sentir tu piel en mis brazos te tuve ayer hay tantas cosas que te quiero decir, acércate a veces siento al despertar como un susurro, tu calor ella no deja de pensar que un día te encontrará cógeme no me dejes marchar, quiero sentarme a tus pies en mis brazos te tuve ayer hay tantas cosas que te quiero decir, dime por que sólo tú, tú y yo una guitarra, el lago y una canción sólo tú, tú y yo ahora ya te puedo decir ¡adió-oh-oh-os! ah-ah-ah-ah ah-ah-ah-ah de nuevo solos tú y yo un lago y una canción echo de menos oír tu voz una estrella te eclipsó\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YUNhw6tAGXhK"},"outputs":[],"source":["import re\n","# Prepara el modelo para la inferencia (modo generación, sin entrenamiento)\n","FastLanguageModel.for_inference(model)\n","\n","# Formatea el prompt con el estilo deseado, insertando la variable 'cancion'\n","# Se crea una lista con un solo string para que el tokenizer procese un batch de tamaño 1\n","# Luego se convierte el texto a tensores de PyTorch y se pasa a la GPU\n","inputs = tokenizer([promp_style.format(cancion)], return_tensors=\"pt\").to('cuda')\n","\n","# Genera texto a partir del prompt\n","outputs = model.generate(\n","    input_ids=inputs.input_ids,           # IDs de entrada tokenizados\n","    attention_mask=inputs.attention_mask, # Máscara de atención para que el modelo ignore los tokens de relleno\n","    max_new_tokens=2,                  # Número máximo de tokens nuevos a generar\n","    use_cache=True,                       # Usa caché para acelerar la generación\n","    eos_token_id=tokenizer.eos_token_id,\n",")\n","\n","# Decodifica SOLO los tokens generados (excluye el prompt)\n","prompt_length = inputs.input_ids.shape[1]\n","generated_tokens = outputs[:, prompt_length:]\n","\n","# skip_special_tokens=True es VITAL para eliminar <|begin_of_sentence|>, etc.\n","# clean_up_tokenization_spaces=True puede ayudar con espacios extra.\n","decoded_outputs = tokenizer.batch_decode(\n","    generated_tokens,\n","    skip_especial_tokens=True,\n","    clean_up_tokenization_spaces=True\n",")\n","\n","if not decoded_outputs:\n","    print(\"Error: La decodificación no produjo ninguna salida.\")\n","    final_answer = None # O manejar el error\n","else:\n","    # Puede que necesites ajustar esto si la salida siempre está fragmentada en la lista\n","    # Por ejemplo, si siempre es el penúltimo o último elemento no vacío:\n","    # response_text = \"\".join([s for s in decoded_outputs if s.strip()]) # Une todo lo no vacío\n","    response_text = decoded_outputs[0].strip() # Toma el primer resultado decodificado y quita espacios extra\n","    print(f\"--- Texto Generado Decodificado (limpio) ---\\n{response_text}\\n------------------------------------------\")\n","\n","    final_answer = None\n","    found_method = \"None\"\n","\n","    # Prioridad 1: Buscar después de </think>\n","    # Busca la etiqueta literal \"</think>\" seguida de cualquier cantidad de espacio en blanco (\\s*)\n","    # y luego captura el primer dígito que encuentre ([01]).\n","    match_think = re.search(r\"</think>\\s*([01])\", response_text)\n","    if match_think:\n","        final_answer = match_think.group(1)\n","        found_method = \"After </think>\"\n","\n","    # Prioridad 2: Buscar después de \"Respuesta:\" (si la Prioridad 1 falló)\n","    # Esto es por si el modelo *genera* \"Respuesta:\" en su salida, aunque es menos probable con el prompt actual.\n","    if final_answer is None:\n","        # Busca \"Respuesta\" (case-insensitive) seguido de ':' opcional, espacios, y captura el dígito.\n","        match_respuesta = re.search(r\"Respuesta:?\\s*([01])\", response_text, re.IGNORECASE)\n","        if match_respuesta:\n","            final_answer = match_respuesta.group(1)\n","            found_method = \"After 'Respuesta:' (in output)\" # Indica que se encontró en la salida\n","\n","    # Prioridad 3: Fallbacks (si las Prioridades 1 y 2 fallaron)\n","    if final_answer is None:\n","        print(\"INFO: No se encontró patrón '</think> [0/1]' ni 'Respuesta: [0/1]'. Intentando fallbacks...\")\n","        # Fallback A: Último dígito no-espacio\n","        cleaned_end_text = response_text.rstrip()\n","        if cleaned_end_text and cleaned_end_text[-1] in ('0', '1'):\n","            final_answer = cleaned_end_text[-1]\n","            found_method = \"Fallback: Last non-whitespace character\"\n","        # Fallback B: Primer dígito (solo si Fallback A falló)\n","        elif response_text and response_text.startswith(('0', '1')):\n","             # Usamos el texto ya limpiado con strip() al principio\n","             final_answer = response_text[0]\n","             found_method = \"Fallback: First character\"\n","\n","    # Resultado Final\n","    if final_answer is None:\n","        final_answer = '?' # O None\n","        found_method = \"Failed to find digit\"\n","        print(\"ERROR: No se pudo extraer '0' o '1' de forma fiable.\")\n","        # Considera registrar 'response_text' completo para depuración\n","    elif final_answer not in ('0', '1'): # Doble chequeo por si acaso\n","         print(f\"ERROR INESPERADO: Se extrajo algo que no es '0' o '1': {final_answer}\")\n","         final_answer = '?'\n","         found_method += \" (Extraction Error)\"\n","\n","\n","print(f\"\\nExtraction Method Used: {found_method}\")\n","print(f\"Respuesta Final Procesada: {final_answer}\")\n","# Decodifica los tokens generados a texto legible\n","#response = tokenizer.batch_decode(outputs)"]},{"cell_type":"markdown","metadata":{"id":"vfeNoP-WIr_A"},"source":["# AHORA EMPIEZA EL FINE TUNING"]},{"cell_type":"markdown","metadata":{"id":"MBzYEoW5WjG_"},"source":["# LLAMADO \"ALPACA PROMPT\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ek7kbrURoRDa"},"outputs":[],"source":["prompts_train = [ \"\"\"\n","            ### Instruccion\n","            Analiza la siguiente letra de canción y clasifícala en UNA de las siguientes cuatro categorías: Sexualización, Odio, Violencia, o No relacionado.\n","\n","            Evalúa si la letra contiene predominantemente:\n","              - Violencia: Descripciones explícitas de agresión física, uso de armas, peleas, asesinatos, daño físico, amenazas directas, o glorificación de la violencia física.\n","              - Odio: Expresiones de hostilidad intensa, desprecio profundo, animadversión, lenguaje deshumanizante o extremadamente denigrante hacia individuos o grupos (por cualquier motivo: rivalidad, raza, género, etc.), o incitación al rencor/venganza.\n","              - Sexualizacion: Reducción de personas a objetos sexuales, enfoque excesivo en atributos físicos de forma despersonalizada, descripciones sexuales explícitas con fin de objetivar/degradar, o lenguaje sexualmente sugerente de manera cosificadora.\n","              - No relacionado: Si la letra NO contiene predominantemente ninguno de los elementos anteriores (puede ser romántica, triste, festiva, narrativa sin estos enfoques, etc.).\n","\n","            Piensa cuidadosamente tu respuesta paso a paso, considerando cada categoría. Determina cuál es la categoría principal o más definitoria si hubiera solapamiento leve. Responde únicamente con la etiqueta de la categoría elegida: \"Violencia\", \"Odio\", \"Sexualizacion\", o \"No relacionado\". No proporciones ninguna explicación ni texto adicional fuera de la etiqueta final.\n","\n","            ### Letra:\n","            {lyrics}\n","\n","            ### Respuesta:\n","            <think> {reasoning} </think>{label}\"\"\",\n","          ]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1746530596895,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"pPxmib2oK4Kd","outputId":"ba5906b8-ad7d-44d5-abd0-6c9ac1e15ecb"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'<｜end▁of▁sentence｜>'"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["EOS_TOKEN = tokenizer.eos_token\n","EOS_TOKEN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUohMs_SLUDy"},"outputs":[],"source":["label_map = {\n","    0: \"No relacionado\",\n","    1: \"Sexualizacion\",\n","    2: \"Violencia\",\n","    3: \"Odio\"\n","}\n","\n","def formatting_prompts(examples):\n","    # Extrae las listas de letras y etiquetas desde el diccionario de ejemplos\n","    lyrics_list = examples[COLUMN_LETRA]      # Lista de letras (inputs)\n","    labels_list  = examples[COLUMN_ETIQUETA]      # Lista de etiquetas o salidas esperadas\n","    reasonings_list = examples[COLUMN_RAZONAMIENTO]\n","\n","    texts = []  # Aquí se guardarán los prompts formateados\n","\n","\n","    # Itera sobre cada par input/output\n","    for lyric, label, reasoning in zip(lyrics_list, labels_list, reasonings_list):\n","        label_str = label_map[label]\n","        # Formatea el prompt usando la plantilla\n","        text = prompts_train[tipo_prompt].format(\n","            lyrics=str(lyric),\n","            reasoning=str(reasoning),\n","            label=label_str\n","            ) + EOS_TOKEN  # Añade token de finalización\n","\n","        texts.append(text)  # Guarda el texto formateado\n","\n","    # Devuelve un diccionario con el texto ya preparado para el entrenamiento\n","    return {\n","        \"text\": texts\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1746530602577,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"ao_bD7U-_pDA","outputId":"161fe151-9344-4815-93b3-3476fcbd07a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Formateando datasets para entrenamiento CoT...\n","Columnas en trainData antes de mapear: ['id', 'lyrics', 'reasoning', 'label', 'labelconletras']\n","Columnas en validData antes de mapear: ['id', 'lyrics', 'reasoning', 'label', 'labelconletras', '__index_level_0__']\n"]}],"source":["print(\"\\nFormateando datasets para entrenamiento CoT...\")\n","print(f\"Columnas en trainData antes de mapear: {trainData.column_names}\")\n","print(f\"Columnas en validData antes de mapear: {validData.column_names}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"executionInfo":{"elapsed":187,"status":"ok","timestamp":1746530603840,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"S4quwbYvMOO0","outputId":"f49e993a-905a-4d7e-9dc7-4cf0e5eea38f"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"752ea53897ca407b8ac5587b9c9d90b9","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/891 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d197e2553bb47b6bc7e4efea21c90f2","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/281 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n            ### Instruccion\\n            Analiza la siguiente letra de canción y clasifícala en UNA de las siguientes cuatro categorías: Sexualización, Odio, Violencia, o No relacionado.\\n\\n            Evalúa si la letra contiene predominantemente:\\n              - Violencia: Descripciones explícitas de agresión física, uso de armas, peleas, asesinatos, daño físico, amenazas directas, o glorificación de la violencia física.\\n              - Odio: Expresiones de hostilidad intensa, desprecio profundo, animadversión, lenguaje deshumanizante o extremadamente denigrante hacia individuos o grupos (por cualquier motivo: rivalidad, raza, género, etc.), o incitación al rencor/venganza.\\n              - Sexualizacion: Reducción de personas a objetos sexuales, enfoque excesivo en atributos físicos de forma despersonalizada, descripciones sexuales explícitas con fin de objetivar/degradar, o lenguaje sexualmente sugerente de manera cosificadora.\\n              - No relacionado: Si la letra NO contiene predominantemente ninguno de los elementos anteriores (puede ser romántica, triste, festiva, narrativa sin estos enfoques, etc.).\\n\\n            Piensa cuidadosamente tu respuesta paso a paso, considerando cada categoría. Determina cuál es la categoría principal o más definitoria si hubiera solapamiento leve. Responde únicamente con la etiqueta de la categoría elegida: \"Violencia\", \"Odio\", \"Sexualizacion\", o \"No relacionado\". No proporciones ninguna explicación ni texto adicional fuera de la etiqueta final.\\n\\n            ### Letra:\\n            a a ti te quiero mujer no le hace que seas pasiada te quiero porque te quiero porque me nace del alma tu no sabias querer porque eras mujer pasiada y te burrlabas de mi cuando de amores te acraba pero, llegaste a saber que con mi amor no jugabas y con el tiempo supistes lo mucho que tu me amaba tu despresiabas me amor cuando en tus brasos lloraba pero llegaste a quererme asi como yo yo deseaba\\n\\n            ### Respuesta:\\n            <think> 1. Análisis de Componentes: La frase describe una relación entre dos personas. Palabras y frases clave incluyen \"mujer pasiada\", \"te burlabas de mí\", \"no sabías querer\", \"despreciabas mi amor\", y \"lloraba en tus brazos\". Estas expresiones sugieren conflictos en la relación y sentimientos de menosprecio.\\n\\n2. Interpretación y Connotación: La expresión \"mujer pasiada\" podría implicar que la mujer en cuestión tiene múltiples relaciones, lo que lleva a un conflicto. \"Te burlabas de mí\" y \"despreciabas mi amor\" indican una dinámica de desdén y rechazo. El narrador describe una transición desde el menosprecio hacia un eventual reconocimiento y reciprocidad del amor.\\n\\n3. Evaluación contra S (Sexualization): No hay indicios claros de lenguaje o insinuaciones de naturaleza sexual en la letra. La frase se centra más en el conflicto emocional y el cambio en la relación afectiva, sin descripciones sexuales.\\n\\n4. Evaluación contra V (Violence): Aunque no hay amenazas físicas explícitas, el lenguaje sugiere un tipo de violencia emocional o psicológica. Frases como \"te burlabas de mí\" y \"despreciabas mi amor\" implican un menosprecio emocional significativo. La idea de \"llorar en tus brazos\" denota sufrimiento emocional, que puede considerarse una forma de agresión psicológica.\\n\\n5. Evaluación contra H (Hate): No se evidencia hostilidad activa o desprecio hacia un grupo específico. El enfoque está en la interacción y el conflicto entre dos individuos, sin extenderse a una expresión de odio más generalizada.\\n\\n6. Síntesis y Justificación Final: La letra refleja una dinámica de poder emocional donde el narrador siente que ha sido objeto de burla y desprecio, lo que conlleva sufrimiento emocional. Aunque no hay violencia física, la violencia emocional se manifiesta a través de las expresiones de menosprecio y el impacto psicológico en el narrador. Este tipo de agresión justifica la clasificación bajo la etiqueta de violencia. </think>Violencia<｜end▁of▁sentence｜>'"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["dataset_finetune = trainData.map(formatting_prompts, batched = True, remove_columns=trainData.column_names) # remove_columns para dejar solo 'text'\n","eval_finetune = validData.map(formatting_prompts, batched=True, remove_columns=validData.column_names) # remove_columns para dejar solo 'text'\n","dataset_finetune[\"text\"][0]"]},{"cell_type":"markdown","metadata":{"id":"VK2eEmIhJDai"},"source":["# Calculo longitud de los prompts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9q54WStlr81"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","token_lengths = [len(tokenizer.encode(text, add_special_tokens=False)) for text in dataset_finetune[\"text\"]]\n","print(f\"Se calcularon las longitudes de {len(token_lengths)} prompts formateados.\")\n","\n","if not token_lengths:\n","    print(\"La lista de longitudes está vacía. Verifica el dataset y la columna 'text'.\")\n","else:\n","    # --- Ahora puedes proceder con el análisis ---\n","\n","    # 1. Encontrar la longitud máxima\n","    max_len = max(token_lengths)\n","    print(f\"Longitud máxima encontrada (incl. special tokens): {max_len}\")\n","\n","    # 2. Visualizar la distribución (como en tu ejemplo anterior)\n","    plt.figure(figsize=(12, 7))\n","    plt.hist(token_lengths, bins=50, alpha=0.7, label=f'Distribución (Máx={max_len})')\n","\n","    plt.axvline(1024, color='red', linestyle='dashed', linewidth=1, label='Max Length = 1024')\n","    plt.axvline(2048, color='orange', linestyle='dashed', linewidth=1, label='Max Length = 2048')\n","    plt.axvline(4096, color='green', linestyle='dashed', linewidth=1, label='Max Length = 4096')\n","    plt.axvline(8192, color='blue', linestyle='dashed', linewidth=1, label='Max Length = 8192')\n","\n","\n","    plt.xlabel('Número de Tokens (incl. special tokens)')\n","    plt.ylabel('Número de Ejemplos')\n","    plt.title(f'Distribución de Longitud de Tokens del Prompt Formateado\\n(Modelo: {tokenizer.name_or_path})') # Usa el nombre del tokenizer\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    print(f\"Porcentaje de canciones conS más de 1024 tokens: {sum(l > 1024 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")\n","    print(f\"Porcentaje de canciones con más de 2048 tokens: {sum(l > 2048 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")\n","    print(f\"Porcentaje de canciones con más de 4096 tokens: {sum(l > 4096 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")\n","    print(f\"Porcentaje de canciones con más de 8192 tokens: {sum(l > 8192 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"355czYm-o-QH"},"source":["# ELIMINO LOS PROMPTS QUE SUPEREN LOS 2048 TOKENS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gG983_Pfoc7B"},"outputs":[],"source":["def limit_tokens(example):\n","  return len(tokenizer.encode(str(example['text']), add_special_tokens=True)) <= max_seq_length"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":154},"executionInfo":{"elapsed":4943,"status":"ok","timestamp":1746530616762,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"5x5k-ByEopGc","outputId":"5e3b6da2-3b4e-4b5d-fb72-09efb51d567e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tamaño original de dataset_finetune: 891\n","Tamaño original de eval_finetune: 281\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"af3859103d4d46e9be244b29180ba784","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/891 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59a1ed7493684ff4a51c739086dd90af","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/281 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Tamaño filtrado de dataset_finetune: 844\n","Tamaño filtrado de eval_finetune: 263\n"]}],"source":["print(f\"Tamaño original de dataset_finetune: {len(dataset_finetune)}\")\n","print(f\"Tamaño original de eval_finetune: {len(eval_finetune)}\")\n","dataset_finetune = dataset_finetune.filter(limit_tokens, batched=False)\n","eval_finetune = eval_finetune.filter(limit_tokens, batched=False)\n","print(f\"Tamaño filtrado de dataset_finetune: {len(dataset_finetune)}\")\n","print(f\"Tamaño filtrado de eval_finetune: {len(eval_finetune)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6I0nyDAVo7J_"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","token_lengths = [len(tokenizer.encode(text, add_special_tokens=False)) for text in dataset_finetune[\"text\"]]\n","print(f\"Se calcularon las longitudes de {len(token_lengths)} prompts formateados.\")\n","\n","if not token_lengths:\n","    print(\"La lista de longitudes está vacía. Verifica el dataset y la columna 'text'.\")\n","else:\n","    # --- Ahora puedes proceder con el análisis ---\n","\n","    # 1. Encontrar la longitud máxima\n","    max_len = max(token_lengths)\n","    print(f\"Longitud máxima encontrada (incl. special tokens): {max_len}\")\n","\n","    # 2. Visualizar la distribución (como en tu ejemplo anterior)\n","    plt.figure(figsize=(12, 7))\n","    plt.hist(token_lengths, bins=50, alpha=0.7, label=f'Distribución (Máx={max_len})')\n","\n","    plt.axvline(1024, color='red', linestyle='dashed', linewidth=1, label='Max Length = 1024')\n","    plt.axvline(2048, color='orange', linestyle='dashed', linewidth=1, label='Max Length = 2048')\n","    plt.axvline(4096, color='green', linestyle='dashed', linewidth=1, label='Max Length = 4096')\n","    plt.axvline(8192, color='blue', linestyle='dashed', linewidth=1, label='Max Length = 8192')\n","\n","\n","    plt.xlabel('Número de Tokens (incl. special tokens)')\n","    plt.ylabel('Número de Ejemplos')\n","    plt.title(f'Distribución de Longitud de Tokens del Prompt Formateado\\n(Modelo: {tokenizer.name_or_path})') # Usa el nombre del tokenizer\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    print(f\"Porcentaje de canciones con más de 1024 tokens: {sum(l > 1024 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")\n","    print(f\"Porcentaje de canciones con más de 2048 tokens: {sum(l > 2048 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")\n","    print(f\"Porcentaje de canciones con más de 4096 tokens: {sum(l > 4096 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")\n","    print(f\"Porcentaje de canciones con más de 8192 tokens: {sum(l > 8192 for l in token_lengths) / len(token_lengths) * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"VJR6rC2uibOw"},"source":["# Parametros del train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1746530625939,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"sXMS7Nai_pDB","outputId":"85fa64a5-d627-4606-fee5-8058d0974fe2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Configuración de Entrenamiento:\n","  - Per Device Batch Size: 1\n","  - Gradient Accumulation Steps: 16\n","  - Effective Batch Size: 16\n","  - Num Train Examples: 843\n","  - Epochs: 8\n","  - Steps per Epoch: ~52\n","  - Total Steps: ~416\n","  - Warmup Steps: 41\n","  - Eval/Log/Save Steps: 25\n","  - LoRA R: 16\n","  - LoRA Alpha: 32\n"]}],"source":["# --- Ajustes para 14GB VRAM y cálculo de pasos ---\n","per_device_batch = 1\n","gradient_accum_steps = 16 # Si da OOM, bajar a 4.\n","effective_batch_size = per_device_batch * gradient_accum_steps # = 8\n","epochs = 8\n","num_train_examples = 843 # Tamaño de tu dataset de entrenamiento\n","steps_per_epoch = num_train_examples // effective_batch_size # ~177\n","total_steps = steps_per_epoch * epochs # ~296\n","warmup_steps_calculated = int(total_steps * 0.10) # ~30 (10% warmup)\n","# Evaluar/Loguear/Guardar cada 25 pasos\n","eval_logging_save_steps = 25\n","lora_r = 16\n","lora_alpha = 32\n","learning_rate = 1e-5\n","\n","print(f\"Configuración de Entrenamiento:\")\n","print(f\"  - Per Device Batch Size: {per_device_batch}\")\n","print(f\"  - Gradient Accumulation Steps: {gradient_accum_steps}\")\n","print(f\"  - Effective Batch Size: {effective_batch_size}\")\n","print(f\"  - Num Train Examples: {num_train_examples}\")\n","print(f\"  - Epochs: {epochs}\")\n","print(f\"  - Steps per Epoch: ~{steps_per_epoch}\")\n","print(f\"  - Total Steps: ~{total_steps}\")\n","print(f\"  - Warmup Steps: {warmup_steps_calculated}\")\n","print(f\"  - Eval/Log/Save Steps: {eval_logging_save_steps}\")\n","print(f\"  - LoRA R: {lora_r}\")\n","print(f\"  - LoRA Alpha: {lora_alpha}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7243,"status":"ok","timestamp":1746530634769,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"aZLxKZeJNOpW","outputId":"9ffbe42e-bde1-4c01-a969-a440291dd6c9"},"outputs":[{"name":"stderr","output_type":"stream","text":["Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n","Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n","Unsloth 2025.4.8 patched 48 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"]}],"source":["# Aplica LoRA al modelo base para permitir entrenamiento eficiente\n","model_lora = FastLanguageModel.get_peft_model(\n","    model,                          # Modelo base al que se le quiere aplicar LoRA\n","    r=lora_r,                           # Rango bajo de adaptación (más bajo = menos parámetros entrenables)\n","    target_modules=[               # Capas específicas del modelo donde se insertarán los adaptadores LoRA\n","        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",        # Proyecciones de atención\n","        \"gate_proj\", \"up_proj\", \"down_proj\"           # Capas del MLP\n","    ],\n","    lora_alpha=lora_alpha,                 # Escala del adaptador LoRA (afecta la fuerza de la adaptación)\n","    lora_dropout=0.05,                # Dropout en LoRA (0 = no dropout, útil para tener resultados estables)\n","    bias=\"none\",                   # No se entrena el sesgo de las capas originales\n","    use_gradient_checkpointing=\"unsloth\",  # Reduce el uso de memoria en GPU durante el entrenamiento\n","    random_state=3407,             # Fijar semilla aleatoria para reproducibilidad\n","    use_rslora=False,              # Si se quiere usar una versión más avanzada de LoRA (aquí se desactiva)\n","    loftq_config=None              # Solo se usa si se quiere aplicar cuantización LoFTQ (aquí no se usa)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"elapsed":1840,"status":"ok","timestamp":1746530636612,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"z1ydFsfzOueV","outputId":"67a559c5-0f3c-4f2d-ca10-83c338f59e15"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23cb2bb61b594145bcf39d1b84af562c","version_major":2,"version_minor":0},"text/plain":["Unsloth: Tokenizing [\"text\"]:   0%|          | 0/844 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d785ec90a0d47aa84b818431fa1cbc6","version_major":2,"version_minor":0},"text/plain":["Unsloth: Tokenizing [\"text\"]:   0%|          | 0/263 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["#Inicializar el fine tuning trainer\n","trainer = SFTTrainer(\n","    model = model_lora,                 # El modelo con adaptadores LoRA listos para entrenar\n","    tokenizer = tokenizer,              # El tokenizer asociado al modelo\n","    train_dataset = dataset_finetune,   # Dataset ya formateado con prompts\n","    eval_dataset = eval_finetune,\n","    dataset_text_field = \"text\",        # Campo del dataset que contiene el texto de entrada y salida\n","    max_seq_length = max_seq_length,    # Longitud máxima de secuencia para cada ejemplo\n","    dataset_num_proc = 1,               # Número de procesos paralelos para preparar el dataset\n","    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n","    args = TrainingArguments(\n","        per_device_train_batch_size = per_device_batch,       # Tamaño de batch por GPU (se combina con gradient_accumulation_steps)\n","        gradient_accumulation_steps = gradient_accum_steps,       # Acumula gradientes durante 4 pasos antes de actualizar pesos → batch efectivo = 8\n","        #max_steps=60,\n","        num_train_epochs = epochs,                  # Número de épocas completas sobre el dataset\n","        warmup_steps = warmup_steps_calculated,                      # Pasos de warmup donde la LR va subiendo poco a poco\n","        learning_rate = learning_rate,                  # Tasa de aprendizaje (bastante buena para LoRA)\n","        fp16 = not is_bfloat16_supported(),    # Usa FP16 si bfloat16 no está soportado\n","        bf16 = is_bfloat16_supported(),        # Usa BF16 si tu GPU lo soporta (mejor para Ampere y posteriores)\n","        logging_steps = eval_logging_save_steps,                    # Registra logs cada 10 pasos\n","        #logging_steps = 1,\n","        optim = \"adamw_8bit\",                  # Optimizador AdamW en 8 bits (menos memoria, ideal para GPUs pequeñas)\n","        weight_decay = 0.01,                   # Regularización para evitar overfitting\n","        lr_scheduler_type = \"linear\",          # Scheduler lineal de tasa de aprendizaje\n","        seed = 3407,                           # Semilla para reproducibilidad\n","        output_dir = f\"{path}/cuadernos/outputs\",                # Carpeta donde se guardan los checkpoints\n","        eval_strategy=\"steps\",\n","        eval_steps=eval_logging_save_steps,\n","        save_strategy=\"steps\",\n","        save_steps=eval_logging_save_steps,\n","        save_total_limit=4,                                   # Guardar solo los últimos 2\n","        load_best_model_at_end=True,                          # Carga el mejor modelo según eval_loss\n","        metric_for_best_model=\"eval_loss\",                    # Usa loss para decidir el \"mejor\"\n","        greater_is_better=False,                              # Menor loss es mejor\n","        report_to=\"wandb\",\n","    ),\n",")"]},{"cell_type":"markdown","metadata":{"id":"JGpCfVNfifkF"},"source":["#Iniciando training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":834},"executionInfo":{"elapsed":30715341,"status":"ok","timestamp":1746561351955,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"9JUZ4b6aQriG","outputId":"7d2a78a3-6cd7-4f07-f4ab-dee0af4185cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Iniciando entrenamiento...\n"]},{"name":"stderr","output_type":"stream","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 844 | Num Epochs = 8 | Total steps = 416\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n","\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n"," \"-____-\"     Trainable parameters = 68,812,800/14,000,000,000 (0.49% trained)\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"]},{"name":"stdout","output_type":"stream","text":["Unsloth: Will smartly offload gradients to save VRAM!\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='416' max='416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [416/416 8:30:43, Epoch 7/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>2.190000</td>\n","      <td>2.160296</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>2.103900</td>\n","      <td>1.901212</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>1.777100</td>\n","      <td>1.556898</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>1.465400</td>\n","      <td>1.297387</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>1.293300</td>\n","      <td>1.203350</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>1.215800</td>\n","      <td>1.157684</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>1.181900</td>\n","      <td>1.135752</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>1.186800</td>\n","      <td>1.120890</td>\n","    </tr>\n","    <tr>\n","      <td>225</td>\n","      <td>1.163900</td>\n","      <td>1.109648</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>1.146200</td>\n","      <td>1.101332</td>\n","    </tr>\n","    <tr>\n","      <td>275</td>\n","      <td>1.137500</td>\n","      <td>1.094796</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>1.127600</td>\n","      <td>1.090002</td>\n","    </tr>\n","    <tr>\n","      <td>325</td>\n","      <td>1.130000</td>\n","      <td>1.086341</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>1.114400</td>\n","      <td>1.083882</td>\n","    </tr>\n","    <tr>\n","      <td>375</td>\n","      <td>1.138600</td>\n","      <td>1.082101</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>1.110600</td>\n","      <td>1.081092</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n","Using gradient accumulation will be very slightly less accurate.\n","Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"]},{"name":"stdout","output_type":"stream","text":["Entrenamiento completado.\n"]}],"source":["# --- Ejecutar Entrenamiento ---\n","print(\"\\nIniciando entrenamiento...\")\n","trainer_stats = trainer.train()\n","print(\"Entrenamiento completado.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167},"executionInfo":{"elapsed":3602,"status":"ok","timestamp":1746561355558,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"E-O6TMCrITD-","outputId":"2a675c30-d1ff-46b8-d392-555a2534f31c"},"outputs":[{"name":"stderr","output_type":"stream","text":["==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n","   \\\\   /|    Num examples = 844 | Num Epochs = 8 | Total steps = 416\n","O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 16\n","\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 16 x 1) = 16\n"," \"-____-\"     Trainable parameters = 68,812,800/14,000,000,000 (0.49% trained)\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='416' max='416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [416/416 : < :, Epoch 7/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["trainer_stats = trainer.train(resume_from_checkpoint = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":763},"executionInfo":{"elapsed":1295,"status":"ok","timestamp":1746561356854,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"YFud0-khQvsB","outputId":"9b36bbc1-0f34-4d35-f743-a7dca452287c"},"outputs":[{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▂▁▁▂▁▂▂▂▁▁▂▂▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▆▇█▇▇▇▇▇▇█▇▇▇█▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▆▇█▇▇▇▇▇▇█▇▇▇█▇</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▇█▅▃▁▁▂▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▅█▇▇▇▆▅▅▄▄▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▅▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.08109</td></tr><tr><td>eval/runtime</td><td>310.4012</td></tr><tr><td>eval/samples_per_second</td><td>0.847</td></tr><tr><td>eval/steps_per_second</td><td>0.847</td></tr><tr><td>total_flos</td><td>7.92954674284376e+17</td></tr><tr><td>train/epoch</td><td>7.85308</td></tr><tr><td>train/global_step</td><td>416</td></tr><tr><td>train/grad_norm</td><td>0.1919</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1106</td></tr><tr><td>train_loss</td><td>0</td></tr><tr><td>train_runtime</td><td>0.3866</td></tr><tr><td>train_samples_per_second</td><td>17464.218</td></tr><tr><td>train_steps_per_second</td><td>1075.994</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">charmed-energy-5</strong> at: <a href='https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2/runs/c6f61n2p?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3' target=\"_blank\">https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2/runs/c6f61n2p?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3</a><br> View project at: <a href='https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3' target=\"_blank\">https://wandb.ai/atorojaen-university-of-huelva/Fine-tune-con-CoT-Tarea-2?apiKey=267f9c5e00c4482ef4fffbdae37a4e02ecad5eb3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20250506_111927-c6f61n2p/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["wandb.finish()"]},{"cell_type":"markdown","metadata":{"id":"ubt8DzWPijDT"},"source":["#Guardando el modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQe8V1Qjk_Xi"},"outputs":[],"source":["final_save_path = f'{path}/models/Model_{model_name}-CoT_{tipo_CoT}-R_{lora_r}-Alpha_{lora_alpha}-LR_{learning_rate}-Tarea_2'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11081,"status":"ok","timestamp":1746561367947,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"pry08J4u0vfN","outputId":"9d624d24-fb72-4f0d-812c-91fef2ffeb89"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Guardando el mejor modelo en: /content/drive/Othercomputers/Mi PC/TFG/MySonGyny/MySonGyny-2025/models/Model_DeepSeek-R1-Distill-Qwen-14B-unsloth-bnb-4bit-CoT_GPT4o-R_16-Alpha_32-LR_1e-05-Tarea_2\n","Modelo y tokenizer guardados.\n"]}],"source":["\n","print(f\"\\nGuardando el mejor modelo en: {final_save_path}\")\n","trainer.save_model(final_save_path)\n","tokenizer.save_pretrained(final_save_path)\n","print(\"Modelo y tokenizer guardados.\")"]},{"cell_type":"markdown","metadata":{"id":"HNsaiyuOila2"},"source":["# Probando el modelo entrenado :)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"1hVnVadOOqTC","executionInfo":{"status":"ok","timestamp":1746722102998,"user_tz":-120,"elapsed":8,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"}}},"outputs":[],"source":["final_save_path = f'{path}/models/Model-DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-CoT-GPT4o-R-16-Alpha-32-LR-2e-05-Tarea-2'"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"bkM2Q93s74fB","colab":{"base_uri":"https://localhost:8080/","height":499},"executionInfo":{"status":"error","timestamp":1746722809086,"user_tz":-120,"elapsed":27298,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"}},"outputId":"ad9ddada-9abd-410e-9f96-dc96533441cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.4.8: Fast Llama patching. Transformers: 4.51.3.\n","   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n"," \"-____-\"     Free license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"error","ename":"HFValidationError","evalue":"Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/Othercomputers/Mi PC/TFG/MySonGyny/MySonGyny-2025/models/Model-DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-CoT-GPT4o-R-16-Alpha-32-LR-2e-05-Tarea-2'. Use `repo_type` argument if needed.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-519023891ab2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_save_path\u001b[0m\u001b[0;34m,\u001b[0m                         \u001b[0;31m# Nombre del modelo en Hugging Face\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m                      \u001b[0;31m# Longitud máxima de secuencia que puede procesar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m                                        \u001b[0;31m# Tipo de precisión para los cálculos (torch.float16 o torch.bfloat16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;31m# From https://github.com/huggingface/peft/issues/184\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# Now add PEFT adapters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m             model = PeftModel.from_pretrained(\n\u001b[0m\u001b[1;32m    434\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0mold_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             )\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         load_result = model.load_adapter(\n\u001b[0m\u001b[1;32m    542\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mload_adapter\u001b[0;34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, **kwargs)\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m         \u001b[0madapters_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_peft_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhf_hub_download_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;31m# load the weights into the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py\u001b[0m in \u001b[0;36mload_peft_weights\u001b[0;34m(model_id, device, **hf_hub_download_kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0mhub_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_hub_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_safetensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         has_remote_safetensors_file = file_exists(\n\u001b[0m\u001b[1;32m    539\u001b[0m             \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhub_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         ):\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"token\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marg_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0;34mf\" '{repo_id}'. Use `repo_type` argument if needed.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/Othercomputers/Mi PC/TFG/MySonGyny/MySonGyny-2025/models/Model-DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-CoT-GPT4o-R-16-Alpha-32-LR-2e-05-Tarea-2'. Use `repo_type` argument if needed."]}],"source":["model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = final_save_path,                         # Nombre del modelo en Hugging Face\n","    max_seq_length = 2048,                      # Longitud máxima de secuencia que puede procesar\n","    dtype = dtype,                                        # Tipo de precisión para los cálculos (torch.float16 o torch.bfloat16)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NnSc7zDBsoFI","executionInfo":{"status":"aborted","timestamp":1746722031450,"user_tz":-120,"elapsed":0,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"}}},"outputs":[],"source":["prompts_test = [ \"\"\"\n","            ### Instruccion\n","            Analiza la siguiente letra de canción y clasifícala en UNA de las siguientes cuatro categorías: Sexualización, Odio, Violencia, o No relacionado.\n","\n","            Evalúa si la letra contiene predominantemente:\n","              - Violencia: Descripciones explícitas de agresión física, uso de armas, peleas, asesinatos, daño físico, amenazas directas, o glorificación de la violencia física.\n","              - Odio: Expresiones de hostilidad intensa, desprecio profundo, animadversión, lenguaje deshumanizante o extremadamente denigrante hacia individuos o grupos (por cualquier motivo: rivalidad, raza, género, etc.), o incitación al rencor/venganza.\n","              - Sexualizacion: Reducción de personas a objetos sexuales, enfoque excesivo en atributos físicos de forma despersonalizada, descripciones sexuales explícitas con fin de objetivar/degradar, o lenguaje sexualmente sugerente de manera cosificadora.\n","              - No relacionado: Si la letra NO contiene predominantemente ninguno de los elementos anteriores (puede ser romántica, triste, festiva, narrativa sin estos enfoques, etc.).\n","\n","            Piensa cuidadosamente tu respuesta paso a paso, considerando cada categoría. Determina cuál es la categoría principal o más definitoria si hubiera solapamiento leve. Responde únicamente con la etiqueta de la categoría elegida: \"Violencia\", \"Odio\", \"Sexualizacion\", o \"No relacionado\". No proporciones ninguna explicación ni texto adicional fuera de la etiqueta final.\n","\n","            ### Letra:\n","            {lyrics}\n","\n","            ### Respuesta:\n","            <think>\"\"\"\n","          ]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":49,"status":"ok","timestamp":1746563026762,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"MscjKW9x_pDC","outputId":"94ba4b8b-bb3a-4236-da5d-e48690371d64"},"outputs":[{"data":{"text/plain":["Qwen2ForCausalLM(\n","  (model): Qwen2Model(\n","    (embed_tokens): Embedding(152064, 5120, padding_idx=151654)\n","    (layers): ModuleList(\n","      (0-4): 5 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear(\n","            (base_layer): Linear(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (5-6): 2 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear(\n","            (base_layer): Linear(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (7-22): 16 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (23): Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (24-25): 2 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (26): Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear(\n","            (base_layer): Linear(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (27-42): 16 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (43-46): 4 x Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear(\n","            (base_layer): Linear(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","      (47): Qwen2DecoderLayer(\n","        (self_attn): Qwen2Attention(\n","          (q_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (k_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (v_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=1024, bias=True)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=1024, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (o_proj): lora.Linear(\n","            (base_layer): Linear(in_features=5120, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (rotary_emb): LlamaRotaryEmbedding()\n","        )\n","        (mlp): Qwen2MLP(\n","          (gate_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (up_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=5120, out_features=13824, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=5120, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=13824, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (down_proj): lora.Linear4bit(\n","            (base_layer): Linear4bit(in_features=13824, out_features=5120, bias=False)\n","            (lora_dropout): ModuleDict(\n","              (default): Dropout(p=0.05, inplace=False)\n","            )\n","            (lora_A): ModuleDict(\n","              (default): Linear(in_features=13824, out_features=16, bias=False)\n","            )\n","            (lora_B): ModuleDict(\n","              (default): Linear(in_features=16, out_features=5120, bias=False)\n","            )\n","            (lora_embedding_A): ParameterDict()\n","            (lora_embedding_B): ParameterDict()\n","            (lora_magnitude_vector): ModuleDict()\n","          )\n","          (act_fn): SiLU()\n","        )\n","        (input_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","        (post_attention_layernorm): Qwen2RMSNorm((5120,), eps=1e-05)\n","      )\n","    )\n","    (norm): Qwen2RMSNorm((5120,), eps=1e-05)\n","    (rotary_emb): LlamaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=5120, out_features=152064, bias=False)\n",")"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["# --- Asegurar que el modelo esté en modo evaluación e inferencia ---\n","model.eval() # Pone el modelo en modo evaluación (desactiva dropout, etc.)\n","FastLanguageModel.for_inference(model) # Prepara el modelo Unsloth para inferencia rápida"]},{"cell_type":"markdown","metadata":{"id":"B6DgHs_EKK4K"},"source":["#Prueba para ver los FP y FN"]},{"cell_type":"markdown","metadata":{"id":"JS_D54YZKQlC"},"source":["#Prueba para ejecutar solo 40 canciones"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":44,"status":"ok","timestamp":1746563033084,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"36YvpbvGDNVI","outputId":"7ba48aa1-8b27-4b6e-b12d-598bdf9093f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- Preparando evaluación con un subconjunto ---\n","Seleccionados 6 ejemplos con etiqueta '1'.\n","Seleccionados 6 ejemplos con etiqueta '0'.\n","Total ejemplos en el subconjunto de evaluación: 12\n"]}],"source":["import random\n","# --- SELECCIÓN DEL SUBCONJUNTO DE testData ---\n","\n","num_samples_label_1 = 6  # Número de ejemplos misóginos a seleccionar\n","num_samples_label_0 = 6  # Número de ejemplos no misóginos a seleccionar\n","\n","# 1. Separar los ejemplos por etiqueta\n","examples_label_1 = [ex for ex in testData if str(ex[COLUMN_ETIQUETA]).strip() == '2']\n","examples_label_0 = [ex for ex in testData if str(ex[COLUMN_ETIQUETA]).strip() == '3']\n","\n","# 2. Asegurarse de no pedir más ejemplos de los que hay\n","num_samples_label_1 = min(num_samples_label_1, len(examples_label_1))\n","num_samples_label_0 = min(num_samples_label_0, len(examples_label_0))\n","\n","# 3. Seleccionar aleatoriamente los ejemplos deseados\n","random.seed(42) # Para reproducibilidad\n","selected_examples_1 = random.sample(examples_label_1, num_samples_label_1)\n","selected_examples_0 = random.sample(examples_label_0, num_samples_label_0)\n","\n","# 4. Combinar los ejemplos seleccionados en un nuevo dataset para evaluar\n","sampled_testData = selected_examples_1 + selected_examples_0\n","random.shuffle(sampled_testData) # Mezclar las clases 0 y 1 para la evaluación\n","\n","testData = sampled_testData\n","print(f\"--- Preparando evaluación con un subconjunto ---\")\n","print(f\"Seleccionados {len(selected_examples_1)} ejemplos con etiqueta '1'.\")\n","print(f\"Seleccionados {len(selected_examples_0)} ejemplos con etiqueta '0'.\")\n","print(f\"Total ejemplos en el subconjunto de evaluación: {len(sampled_testData)}\")"]},{"cell_type":"markdown","metadata":{"id":"1tZaUff7Kiho"},"source":["#Ejecutar evaluación"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":1098031,"status":"ok","timestamp":1746564132748,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"z-HKvebm_pDC","outputId":"0c9bd0a9-7e9e-4dd6-fc92-b752b6c2e746"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Iniciando evaluación en el Test Set (12 ejemplos) ---\n"]},{"name":"stderr","output_type":"stream","text":["Evaluando Test Set:   8%|▊         | 1/12 [00:55<10:08, 55.27s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0460\n","True label Violencia\n","Salida\n","1. Análisis de Componentes: La frase clave es \"invito a las putas\" y \"el que no sabía de mí, me escucha y dice quien es ese\". Estas expresiones contienen palabras como \"putas\", que es una forma despectiva y vulgar de referirse a mujeres, y \"dice quien es ese\", que sugiere una actitud de menosprecio o discriminación hacia alguien desconocido.\n","\n","2. Interpretación y Connotación: La palabra \"putas\" es una expresión grosera y despectiva que implica una falta de respeto hacia mujeres, reduciéndolas a un estereotipo negativo. Además, la frase \"dice quien es ese\" puede interpretarse como una forma de burla o discriminación hacia alguien que no es conocido, reflejando una actitud de superioridad o indiferencia hacia ellos.\n","\n","3. Evaluación contra S (Sexualization): Aunque hay una insinuación de invitación a relaciones sexuales implícita en \"invito a las putas\", el contexto general de la canción no está centrado en contenido sexual. Más bien, esta parte del análisis se centra en la naturaleza despectiva y discriminatoria de la frase.\n","\n","4. Evaluación contra V (Violence): No hay referencias directas a agresión física, amenazas o actos violentos en la frase analizada. El tono es más bien de desdén y discriminación, pero no se menciona violencia física.\n","\n","5. Evaluación contra H (Hate): La frase contiene elementos de odio y discriminación. La palabra \"putas\" es una expresión despectiva y discriminatoria hacia mujeres, mientras que \"dice quien es ese\" sugiere una actitud de indiferencia o desprecio hacia aquellos que no son conocidos. Ambas expresiones reflejan una actitud de hostilidad y desprecio hacia otros individuos, lo cual es consistente con la categoría de odio.\n","\n","6. Síntesis y Justificación Final: La frase analizada contiene elementos de odio debido a su uso de lenguaje despectivo y discriminatorio hacia mujeres y hacia aquellos que no son conocidos. La palabra \"putas\" es una expresión grosera y despectiva que demuestra una falta de respeto hacia mujeres, mientras que \"dice quien es ese\" expresa una actitud de indiferencia o desprecio hacia desconocidos. Ambas expresiones reflejan una actitud de hostilidad y discriminación, lo cual justifica la clasificación bajo la categoría de odio. </think>Odio\n","-----------------------------------------------------------------------\n","ID T2_TRAIN_0701\n","True label Violencia\n","Salida\n","que se metió, lo mataron y a los otros les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un\n","---Reintento 1/4 para ID: T2_TRAIN_0701---\n","-----------------------------------------------------------------------\n","ID T2_TRAIN_0701\n","True label Violencia\n","Salida\n","que se metió, lo mataron y a los otros les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un\n","---Reintento 2/4 para ID: T2_TRAIN_0701---\n","-----------------------------------------------------------------------\n","ID T2_TRAIN_0701\n","True label Violencia\n","Salida\n","que se metió, lo mataron y a los otros les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un\n","---Reintento 3/4 para ID: T2_TRAIN_0701---\n","-----------------------------------------------------------------------\n","ID T2_TRAIN_0701\n","True label Violencia\n","Salida\n","que se metió, lo mataron y a los otros les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un\n","---Reintento 4/4 para ID: T2_TRAIN_0701---\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  17%|█▋        | 2/12 [09:26<53:52, 323.28s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0701\n","True label Violencia\n","Salida\n","que se metió, lo mataron y a los otros les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un balazo y a los que se salvaron, les dieron un\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  25%|██▌       | 3/12 [10:09<29:20, 195.66s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0839\n","True label Violencia\n","Salida\n","1. Análisis de Componentes: La frase clave es \"ahora si voy palante, ahora si voy paencima.\" El término \"paencima\" puede interpretarse literalmente como \"hacia arriba\" o figurativamente como \"muy lejos\" o \"de gran altura.\" Sin embargo, en este contexto, podría tener una connotación de dominancia o superioridad.\n","\n","2. Interpretación y Connotación: La expresión \"voy paencima\" sugiere una posición de ventaja o control sobre otra persona. En el contexto de la canción, esto se refiere a la relación entre dos personas, donde uno está en una posición de poder o control sobre el otro. Esto implica una dinámica de poder desequilibrada, lo cual puede ser visto como una forma de violencia emocional o psicológica.\n","\n","3. Evaluación contra S (Sexualization): Aunque hay referencias a amor y atracción, la frase no presenta elementos de sexualización. No hay insinuaciones sexuales ni descripciones de naturaleza sexual.\n","\n","4. Evaluación contra V (Violence): La frase \"voy paencima\" sugiere una posición de dominancia o control, lo cual puede implicar una forma de violencia emocional o psicológica. La idea de \"estar encima\" de alguien puede representar una dinámica de poder abusiva, que es consistente con la categoría de violencia.\n","\n","5. Evaluación contra H (Hate): No hay expresiones de odio o discriminación en la frase. La canción parece centrarse en el amor y la atracción, sin mostrar hostilidad o desprecio hacia individuos o grupos.\n","\n","6. Síntesis y Justificación Final: La frase \"voy paencima\" se ajusta a la categoría de violencia debido a su connotación de dominancia y control sobre otra persona. Este tipo de dinámica de poder desequilibrada es una forma de violencia emocional o psicológica, lo cual justifica la asignación de la etiqueta 'V'. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  33%|███▎      | 4/12 [11:10<18:59, 142.43s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0726\n","True label Odio\n","Salida\n","1. Análisis de Componentes: La frase contiene varias expresiones clave como \"estoy solo otra vez\", \"ahogado en mi soledad\", \"no es novedad\", \"antes ya me habían dejado\", \"antes ya me había enborrachado\", \"quiero saber si alguien puede salvarme\", \"si hay una mujer que me saque de esta perra soledad\", \"quiero gritar porque no puedo más\", \"tu adios me ba matar\", \"regresa por piedad\", \"me encontraras en el mismo lugar\", \"si quieres regresar ahi te esperare\". Estas frases reflejan emociones de tristeza, soledad, depresión y un deseo de compañía.\n","\n","2. Interpretación y Connotación: Las palabras \"solo\", \"soledad\" y \"ahogado\" sugieren una sensación de aislamiento y angustia emocional. \"Me habían dejado\" y \"me había enborrachado\" indican un patrón de fracaso en relaciones pasadas y posible autoabuso. \"Quiero gritar porque no puedo más\" expresa un estado de agotamiento emocional. \"Tu adios me ba matar\" podría interpretarse literalmente o metafóricamente, sugiriendo que el impacto del adiós ha sido tan fuerte que casi mata al hablante. \"Regresa por piedad\" muestra una súplica desesperada, mientras que \"si quieres regresar ahi te esperare\" refuerza el anhelo de compañía y estabilidad emocional.\n","\n","3. Evaluación contra S (Sexualization): No hay referencias o insinuaciones de naturaleza sexual en la letra. El enfoque está en el dolor emocional y la necesidad de compañía, no en temas sexuales.\n","\n","4. Evaluación contra V (Violence): Aunque hay una declaración de \"tu adios me ba matar\", esto se interpreta más bien como una metáfora poética para expresar el impacto emocional devastador del adiós, en lugar de una amenaza real de violencia física. No hay menciones de agresión o actos violentos.\n","\n","5. Evaluación contra H (Hate): No hay expresiones de odio, discriminación o hostilidad hacia individuos o grupos. La letra se centra en el sufrimiento personal y la búsqueda de consuelo, sin dirigir sentimientos negativos hacia otros.\n","\n","6. Síntesis y Justificación Final: La letra pertenece a la categoría 'NR' debido a que no presenta elementos de sexualización ('S'), violencia ('V') o odio ('H'). En cambio, se enfoca en el dolor emocional, la soledad y la súplica de ayuda, lo cual es consistente con la categoría 'NR'. </think>No relacionado\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  42%|████▏     | 5/12 [12:14<13:18, 114.09s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0403\n","True label Violencia\n","Salida\n","1. Análisis de Componentes: La letra describe una escena urbana donde Pedro Navaja, un personaje conocido por sus actos violentos, se enfrenta a una mujer que también porta un arma. Se mencionan detalles sobre cómo ambos portan armas (puñales y revólveres) y cómo Pedro Navaja utiliza estas armas contra la mujer, culminando en un homicidio. Además, se hace referencia a la violencia cotidiana en el barrio, con expresiones como \"el que no corre, vuela\" y \"la vida te da sorpresas\".\n","\n","2. Interpretación y Connotación: El texto presenta una historia cargada de violencia y peligro. Pedro Navaja es retratado como un criminal que no duda en usar su arma para resolver conflictos, y la mujer también resulta ser una figura violenta que intenta defenderse. La frase \"¡guiso fácil!\" sugiere una falta de empatía y una actitud de indiferencia hacia la víctima. Las referencias a la violencia en el barrio (\"cuidao en la acera\") y la metáfora de Kafka refuerzan la idea de una existencia marcada por la violencia y la fatalidad.\n","\n","3. Evaluación contra S (Sexualization): Aunque hay una interacción entre hombres y mujeres, la letra no contiene insinuaciones sexuales ni se centra en aspectos de naturaleza sexual. Los encuentros entre los personajes están centrados en la confrontación y la violencia, no en relaciones íntimas o seducción.\n","\n","4. Evaluación contra V (Violence): La letra está cargada de violencia. Pedro Navaja es descrito como un hombre que porta un puñal y no duda en usarlo contra la mujer. La escena incluye un homicidio perpetrado con un puñal, y la mujer también responde con un disparo de revólver. Además, el contexto general del barrio y las expresiones utilizadas (\"el que no corre, vuela\") refuerzan la percepción de un ambiente violento y peligroso.\n","\n","5. Evaluación contra H (Hate): Aunque hay tensiones entre los personajes, no hay evidencias claras de odio basado en características demográficas como raza, género u origen étnico. La violencia parece estar motivada por el conflicto personal y situacional, no por sentimientos de hostilidad generalizados hacia un grupo específico.\n","\n","6. Síntesis y Justificación Final: La letra pertenece a la categoría 'V' debido a su contenido extremadamente violento. La presencia constante de armas, el homicidio cometido por Pedro Navaja y la defensa violenta de la mujer son ejemplos claros de violencia. La narrativa refuerza la idea de un mundo lleno de peligros y amenazas, justificando así la clasificación bajo la etiqueta de violencia. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  50%|█████     | 6/12 [13:22<09:49, 98.22s/it] "]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0453\n","True label Odio\n","Salida\n","coger un avión pa llega a mi velocidad tu gente, par de día esperando, y mira qué asquerosidad\n","\n","            ### Respuesta:\n","            <think> 1. Análisis de Componentes: La frase incluye varias expresiones clave como \"yo te mato con lo tuyo, ratón, pero con limpieza de tu boca\", \"te entró a mordía\", \"te asesino a sangre fría\", \"el que gana es el que goza y yo sigo con mi alegría\", y \"yo te maté encima de una pista\". Estas expresiones sugieren actos violentos, amenazas y referencias a la muerte.\n","\n","2. Interpretación y Connotación: Las frases analizadas presentan un tono agresivo y hostil. \"Matar\" y \"asesinar\" son palabras fuertes que indican intencionalidad violenta. \"Te asesino a sangre fría\" sugiere una falta de empatía y una acción premeditada. \"El que gana es el que goza y yo sigo con mi alegría\" implica una justicia personal basada en la superioridad y la victoria sobre el otro. \"Yo te maté encima de una pista\" refuerza la idea de dominancia y control.\n","\n","3. Evaluación contra S (Sexualization): Aunque hay referencias a relaciones personales y situaciones íntimas (\"tu mujer te acepta do hijo ajeno\"), estas no constituyen la base de la letra. El énfasis está en la violencia y las amenazas, no en insinuaciones sexuales.\n","\n","4. Evaluación contra V (Violence): Las expresiones analizadas claramente apuntan a la violencia. Frases como \"yo te mato con lo tuyo, ratón, pero con limpieza de tu boca\" y \"te asesino a sangre fría\" son ejemplos claros de amenazas de agresión física. Además, \"yo te maté encima de una pista\" sugiere un acto de dominancia y control, que es consistente con la categoría de violencia.\n","\n","5. Evaluación contra H (Hate): Aunque hay cierto desprecio y hostilidad hacia el otro (\"lo lapicista\", \"volviste pa la suciedad\"), la letra no expresa odio en el sentido de hostilidad generalizada hacia un grupo o individuo por motivos de raza, género u otra discriminación. El foco está más en la competencia personal y la superioridad individual.\n","\n","6. Síntesis y Justificación Final: La letra pertenece a la categoría 'V' debido a las múltiples expresiones de amenazas y actos violentos. Las frases analizadas reflejan un tono agresivo y hostil, con referencias directas a la muerte y la dominancia. Esto se alinea con la definición de violencia proporcionada, que incluye amenazas y glorificaciones de agresión física. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  58%|█████▊    | 7/12 [14:14<06:56, 83.27s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0776\n","True label Odio\n","Salida\n","1. Análisis de Componentes: La frase clave es \"juegas al hombre fatal\" y \"sabes jugar a matar\". Estas expresiones implican un juego peligroso y potencialmente mortal, sugiriendo una dinámica de riesgo y control emocional. El término \"fatal\" implica resultados negativos o mortales, mientras que \"jugar a matar\" puede interpretarse literalmente o metafóricamente como una relación tóxica o destructiva.\n","\n","2. Interpretación y Connotación: La frase parece insinuar que el hablante está involucrado en una relación donde uno de los participantes ejerce control o manipulación emocional, similar a un juego de muerte. Esto puede connotar una dinámica de poder desequilibrada, donde el otro individuo es tratado como si estuviera jugando con su vida o su bienestar emocional.\n","\n","3. Evaluación contra S (Sexualization): Aunque la frase podría tener ciertos tonos seductores debido a la palabra \"fatal\", no hay referencias claras o explícitas a contenido sexual. El contexto parece estar centrado más en el conflicto emocional que en la sexualización.\n","\n","4. Evaluación contra V (Violence): La frase \"juegas al hombre fatal\" y \"sabes jugar a matar\" sugieren una dinámica de control y posible abuso emocional. La metáfora del juego a matar puede indicar una situación violenta o tóxica, aunque no necesariamente física. Sin embargo, la intención puede ser interpretar una violencia emocional o psicológica, que también cae bajo la categoría de violencia en este contexto.\n","\n","5. Evaluación contra H (Hate): No hay evidencias claras de odio en la frase proporcionada. Las palabras no contienen expresiones de hostilidad, discriminación o desprecio hacia individuos o grupos específicos.\n","\n","6. Síntesis y Justificación Final: La frase se centra en una dinámica de control y manipulación emocional, representada mediante la metáfora de un juego fatal. Aunque no hay violencia física explicita, la referencia a \"jugar a matar\" sugiere una violencia emocional o psicológica, lo cual justifica la categorización bajo 'V' por implicar un tipo de violencia no física pero significativa en el contexto de la letra. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  67%|██████▋   | 8/12 [14:58<04:43, 70.86s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_1130\n","True label Odio\n","Salida\n","1. Análisis de Componentes: La frase describe un conflicto emocional entre dos personas. Palabras clave incluyen \"tu amante\", \"tu amor\", \"tu vida\", \"tu retrato\", \"mala mujer\", \"matarlo\", \"llevarlo hasta tus pies\", \"me quedo y te tengo que matar\", \"te olvidas de mi nombre\". Estas palabras sugieren un contexto de traición y venganza.\n","\n","2. Interpretación y Connotación: El habla está cargada de emociones negativas como rabia, dolor y frustración. Se menciona la intención de \"matar\" a alguien, lo cual es una expresión clara de violencia. Además, hay una referencia a \"mala mujer\", lo cual puede implicar juicios de valor negativos sobre la otra persona.\n","\n","3. Evaluación contra S (Sexualization): No hay indicios de contenido sexual en la frase. Las referencias son más bien emocionales y violentas, sin insinuaciones o descripciones de naturaleza sexual.\n","\n","4. Evaluación contra V (Violence): La frase contiene varias indicaciones de violencia potencial. La repetida mención de querer \"matar\" a alguien es un ejemplo claro de violencia verbal y emocional. Además, la idea de \"llevarlo hasta tus pies\" implica una acción de dominancia o castigo, refuerzando la connotación de violencia.\n","\n","5. Evaluación contra H (Hate): Aunque hay un tono de hostilidad y desprecio (\"mala mujer\"), la frase no expresa odio en el sentido de hostilidad generalizada o discriminación hacia un grupo específico. El odio aquí es más personal y centrado en una situación particular de traición.\n","\n","6. Síntesis y Justificación Final: La frase pertenece a la categoría 'V' debido a las múltiples referencias a actos violentos, especialmente la intención de \"matar\" a alguien. Esto es un ejemplo directo de violencia verbal y emocional, que domina el análisis de la frase. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  75%|███████▌  | 9/12 [15:47<03:11, 63.92s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0069\n","True label Violencia\n","Salida\n","1. Análisis de Componentes: La frase analizada describe emocionalmente el proceso de pérdida de amor y dolor emocional. Palabras clave incluyen \"no vi las flores marchitar\", \"frío en tus ojos\", \"amor\", \"lloré\", \"perderte\", \"juré\", \"negar\", \"pasión\", \"esperando\" y \"nadie te ha amado\". Estas palabras sugieren sentimientos de tristeza, arrepentimiento y deseo de reconciliación.\n","\n","2. Interpretación y Connotación: El contexto de la frase implica una historia de amor que terminó mal, donde el hablante expresa su pena y arrepentimiento por haber permitido que el amor terminara. La repetición de \"como yo, como yo nadie te ha amado\" refuerza la idea de que el hablante ha puesto todo su corazón en el amor, pero ahora está solo y herido. Sin embargo, no hay insinuaciones de agresión, hostilidad o contenido sexual.\n","\n","3. Evaluación contra S (Sexualization): No hay referencias a actos sexuales, lenguaje erótico o insinuaciones de naturaleza sexual. Las expresiones se centran en el amor y el dolor emocional, sin ningún componente de sexualización.\n","\n","4. Evaluación contra V (Violence): Aunque se menciona el dolor emocional y el arrepentimiento, no hay descripciones de agresión física, amenazas o actos violentos. La frase se enfoca en el impacto emocional del amor perdido, no en la violencia física o verbal.\n","\n","5. Evaluación contra H (Hate): No hay expresiones de hostilidad, discriminación o desprecio hacia individuos o grupos. La letra se centra en el amor y el dolor personal, sin mostrar odio hacia otra persona o grupo.\n","\n","6. Síntesis y Justificación Final: Dada la ausencia de elementos de sexualización, violencia o odio en la frase, la letra pertenece claramente a la categoría 'NR' (No Related). La letra se centra en el dolor emocional y el arrepentimiento por el amor perdido, sin contener contenido sensible según las definiciones proporcionadas. </think>No relacionado\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  83%|████████▎ | 10/12 [16:34<01:57, 58.58s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0051\n","True label Odio\n","Salida\n","1. Análisis de Componentes: La frase contiene varias expresiones clave: \"me desespera\", \"me mata\", \"me enloquece\", \"hasta la vida diera por vencer el miedo de besarla a usted\". Estas frases sugieren emociones intensas y posibles actos de autolesividad o agresión emocional.\n","\n","2. Interpretación y Connotación: Las palabras \"me desespera\" y \"me mata\" pueden interpretarse literalmente o metafóricamente. En contexto, parecen indicar un estado emocional extremo, quizás refiriéndose a la intensidad del amor o atracción. Sin embargo, también podrían insinuar un impacto negativo en la salud mental del hablante, tal vez inducido por la relación con otra persona.\n","\n","3. Evaluación contra S (Sexualization): No hay indicios claros de contenido sexual en la frase. Aunque se menciona \"el miedo de besarla a usted\", esto está centrado en el deseo y la ansiedad emocional, no en un contexto sexual explícito.\n","\n","4. Evaluación contra V (Violence): La frase utiliza expresiones como \"me desespera\", \"me mata\", y \"me enloquece\", que pueden implicar un impacto emocional violento o extremo. \"Me mata\" puede interpretarse literalmente, lo cual implica un acto de violencia real, aunque probablemente sea una metáfora poética para expresar la intensidad emocional.\n","\n","5. Evaluación contra H (Hate): No hay evidencias de odio o discriminación en la frase. El tono es más bien de desesperación personal y afecto intenso hacia otra persona, sin mostrar hostilidad hacia individuos o grupos específicos.\n","\n","6. Síntesis y Justificación Final: La frase presenta elementos que podrían caer bajo la categoría de violencia debido a las expresiones extremadas como \"me desespera\", \"me mata\", y \"me enloquece\". Aunque estas palabras pueden ser interpretadas metafóricamente, su naturaleza potencialmente violenta justifica la asignación a esta categoría. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\rEvaluando Test Set:  92%|█████████▏| 11/12 [17:19<00:54, 54.45s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0675\n","True label Odio\n","Salida\n","1. Análisis de Componentes: La frase incluye varias expresiones clave como \"mi negra\", \"una poción\", \"mágica ilusión\", \"un toloache\" y \"vámonos\". Estas palabras sugieren un contexto de búsqueda emocional y posible consumo de sustancias. \"Toloache\" es una bebida tradicional en algunas culturas, pero también puede asociarse con efectos psicoactivos.\n","\n","2. Interpretación y Connotación: El uso de \"mi negra\" podría implicar una relación íntima o afectiva, mientras que \"una poción\" y \"mágica ilusión\" sugieren un intento de encontrar consuelo o escape emocional. \"Vámonos\" implica una urgencia o deseo de acción inmediata, posiblemente relacionada con el consumo de sustancias para alcanzar ese estado deseado.\n","\n","3. Evaluación contra S (Sexualization): Aunque hay referencias a \"mi negra\", no hay insinuaciones claras de naturaleza sexual. Las frases parecen centradas en el deseo emocional y el consumo de sustancias más que en interacciones sexuales.\n","\n","4. Evaluación contra V (Violence): No hay indicios de agresión, amenazas o actos violentos en la frase. El tono es más bien de buscar consuelo y escape, sin mencionar violencia.\n","\n","5. Evaluación contra H (Hate): No hay expresiones de hostilidad, discriminación o desprecio hacia individuos o grupos. La frase parece estar centrada en el deseo personal y no en el odio hacia otros.\n","\n","6. Síntesis y Justificación Final: La frase carece de elementos de sexualización, violencia u odio. Se centra en el deseo de encontrar consuelo emocional y escape a través del consumo de sustancias, lo cual no cae bajo las categorías de S, V o H. Por lo tanto, corresponde a la etiqueta 'NR' debido a la ausencia de contenido sensible en las categorías analizadas. </think>No relacionado\n"]},{"name":"stderr","output_type":"stream","text":["Evaluando Test Set: 100%|██████████| 12/12 [18:17<00:00, 91.50s/it]"]},{"name":"stdout","output_type":"stream","text":["-----------------------------------------------------------------------\n","ID T2_TRAIN_0159\n","True label Violencia\n","Salida\n","1. Análisis de Componentes: La frase incluye varias expresiones clave como \"yo te voy a raptar\", \"va a tocarme arrestarte\", \"un ak en la nalga va a tatuarse\", \"ella e la preferida\", \"bandida\", \"stripper\", y \"tre letra\". Estas palabras y frases sugieren referencias a secuestros, actos de posesión, violencia, y comportamientos asociados con la cultura delincuencial.\n","\n","2. Interpretación y Connotación: Las expresiones analizadas tienen connotaciones claras de violencia y hostilidad. \"Raptar\" implica secuestro, mientras que \"va a tocarme arrestarte\" sugiere un acto de dominancia o agresión. \"Un ak en la nalga va a tatuarse\" puede interpretarse como una amenaza de usar un arma contra alguien. Además, \"bandida\" y \"tre letra\" son términos comunes en la cultura del crimen organizado, refiriéndose a actividades ilegales y peligrosas.\n","\n","3. Evaluación contra S (Sexualization): Aunque hay menciones de \"stripper\" y \"ropa interior\", estas no constituyen la parte central de la letra. El contexto general de la frase no se centra en la sexualización, sino más bien en la violencia y el crimen organizado.\n","\n","4. Evaluación contra V (Violence): La frase contiene múltiples indicadores de violencia. \"Yo te voy a raptar\" y \"va a tocarme arrestarte\" son claramente amenazantes y sugieren actos violentos. \"Un ak en la nalga va a tatuarse\" también implica el uso de armas y la intimidación. Además, \"bandida\" y \"tre letra\" están ligados a la cultura delictuosa, refuerzando la presencia de violencia en la letra.\n","\n","5. Evaluación contra H (Hate): No hay evidencias claras de odio dirigido a individuos o grupos específicos en la frase. Las expresiones se centran más en actos violentos y criminales en general, sin apuntar a un grupo determinado.\n","\n","6. Síntesis y Justificación Final: La frase pertenece a la categoría 'V' debido a su contenido explícito de violencia. Las expresiones como \"yo te voy a raptar\", \"va a tocarme arrestarte\", y \"un ak en la nalga va a tatuarse\" son claras referencias a actos violentos y amenazas. La combinación de estos elementos justifica la clasificación bajo la etiqueta de violencia. </think>Violencia\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# --- Listas para guardar resultados ---\n","true_labels = []\n","predicted_labels = []\n","failed_extractions_count = 0\n","test_results_details = []\n","\n","from tqdm import tqdm\n","import re\n","print(f\"\\n--- Iniciando evaluación en el Test Set ({len(testData)} ejemplos) ---\")\n","\n","for example in tqdm(testData, desc=\"Evaluando Test Set\"):\n","    try:\n","        lyrics = str(example[COLUMN_LETRA])\n","        true_label = str(example[COLUNM_ETIQUETALETRA]).strip()\n","\n","        if true_label == 'NR':\n","            text_label = 'No relacionado'\n","        elif true_label == 'V':\n","            text_label = 'Violencia'\n","        elif true_label == 'H':\n","            text_label = 'Odio'\n","        elif true_label == 'S':\n","            text_label = 'Sexualizacion'\n","\n","        # 1. Formatear el prompt de inferencia\n","        input_text = prompts_test[tipo_prompt].format(lyrics=str(lyrics))\n","\n","        # 2. Tokenizar\n","        inputs = tokenizer([input_text], return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(device)\n","\n","        pred_label = None\n","        retries = 0\n","\n","        while retries <= 4 and pred_label not in [\"NR\", \"V\", \"H\", \"S\"]:\n","            if retries > 0:\n","                print(f'---Reintento {retries}/4 para ID: {example[COLUMN_ID]}---')\n","            # 3. Generar la salida del modelo\n","            with torch.no_grad():\n","                outputs = model.generate(\n","                    input_ids=inputs.input_ids,\n","                    attention_mask=inputs.attention_mask,\n","                    temperature = 0.6,\n","                    max_new_tokens=1024,  # Suficiente para CoT + </think> + label\n","                    eos_token_id=tokenizer.eos_token_id,\n","                    pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id,\n","                    do_sample=False,     # Forzar salida determinista para evaluación\n","                    use_cache=True,\n","                    repetition_penalty=1.1,\n","                )\n","\n","            # 4. Decodificar SOLO los tokens generados\n","            prompt_length = inputs.input_ids.shape[1]\n","            generated_ids = outputs[0, prompt_length:]\n","            decoded_output = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n","            print(\"-----------------------------------------------------------------------\")\n","            print(\"ID \"+example[COLUMN_ID])\n","            print(\"True label \" + text_label)\n","            print(\"Salida\\n\" + decoded_output)\n","            target_labels = [\"Sexualizacion\", \"Odio\", \"Violencia\", \"No relacionado\"]\n","            regex_pattern = r\"(?is)<\\/think>\\s*(.*)\"\n","            #regex_pattern = r\"(?i)</think>\\s*(\" + \"|\".join(target_labels) + r\")\\s*$\"\n","            match = re.search(regex_pattern, decoded_output)\n","            if match:\n","                pred_label = match.group(1).strip().capitalize()\n","            if pred_label not in target_labels:\n","                pred_label = None  # Forzar otro intento\n","                if retries == 4:\n","                    pred_label = 'No relacionado'\n","                retries += 1\n","                continue  # Ir al siguiente intento\n","\n","\n","            if pred_label == 'No relacionado':\n","                pred_label = 'NR'\n","            elif pred_label == 'Violencia':\n","                pred_label = 'V'\n","            elif pred_label == 'Odio':\n","                pred_label = 'H'\n","            elif pred_label == 'Sexualizacion':\n","                pred_label = 'S'\n","\n","            # 6. Almacenar resultados si la extracción fue válida\n","            if pred_label in ('NR', 'V', 'H', 'S'):\n","                true_labels.append(true_label)\n","                predicted_labels.append(pred_label)\n","                test_results_details.append({\n","                    'id': example[COLUMN_ID],\n","                    'lyrics': lyrics,\n","                    'true_label': true_label,\n","                    'predicted_label': pred_label,\n","                    'full_output_decoded': decoded_output # Guardar salida completa para análisis\n","                })\n","\n","    except Exception as e:\n","        print(f\"\\nError procesando un ejemplo durante la evaluación: {e}\")\n","        failed_extractions_count += 1 # Contarlo como fallo"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1438,"status":"ok","timestamp":1746564153656,"user":{"displayName":"Antonio Toro Jaén","userId":"09221236473454991270"},"user_tz":-120},"id":"1GQ4MA8Y_pDC","outputId":"8789b724-2ffb-4f59-8449-fdcc2fab25ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","--- Resultados de la Evaluación (Test Set) ---\n","Total de ejemplos evaluados: 11\n","Ejemplos donde falló la extracción de la etiqueta: 0\n","\n","Accuracy: 0.2727\n","F1 Score (Macro): 0.1667\n","F1 Score (Weighted): 0.2273\n","\n","Reporte de Clasificación Detallado:\n","                     precision    recall  f1-score   support\n","\n","No relacionado (NR)       0.00      0.00      0.00         0\n","  Sexualizacion (S)       0.00      0.00      0.00         0\n","      Violencia (V)       0.43      0.60      0.50         5\n","           Odio (H)       0.00      0.00      0.00         6\n","\n","           accuracy                           0.27        11\n","          macro avg       0.11      0.15      0.12        11\n","       weighted avg       0.19      0.27      0.23        11\n","\n"]},{"data":{"image/png":"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\n","text/plain":["<Figure size 800x600 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","# --- Calcular y mostrar métricas ---\n","print(f\"\\n--- Resultados de la Evaluación (Test Set) ---\")\n","print(f\"Total de ejemplos evaluados: {len(true_labels)}\")\n","print(f\"Ejemplos donde falló la extracción de la etiqueta: {failed_extractions_count}\")\n","\n","class_labels = ['NR', 'S', 'V', 'H']\n","class_labels_display = [\"No relacionado (NR)\", \"Sexualizacion (S)\", \"Violencia (V)\", \"Odio (H)\"]\n","\n","if not true_labels or not predicted_labels:\n","    print(\"\\nNo se pudieron calcular las métricas (no hay predicciones válidas).\")\n","else:\n","    # Calcular métricas usando sklearn\n","    accuracy = accuracy_score(true_labels, predicted_labels)\n","    f1_macro = f1_score(true_labels, predicted_labels, average='macro', zero_division=0)\n","    f1_weighted = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n","    report = classification_report(true_labels, predicted_labels, labels=class_labels, target_names=class_labels_display, zero_division=0)\n","    conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=class_labels)\n","\n","    print(f\"\\nAccuracy: {accuracy:.4f}\")\n","    print(f\"F1 Score (Macro): {f1_macro:.4f}\")\n","    print(f\"F1 Score (Weighted): {f1_weighted:.4f}\")\n","    print(\"\\nReporte de Clasificación Detallado:\")\n","    print(report)\n","\n","    # Grafico de la matriz de confusión\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Oranges', xticklabels=class_labels_display, yticklabels=class_labels_display)\n","    plt.ylabel('Etiqueta verdadera')\n","    plt.xlabel('Etiqueta predecida')\n","    plt.title('Matriz de confusión Tarea 2')\n","    matriz_path = f\"{path}/matrices/prueba.png\"\n","    plt.savefig(matriz_path)\n","    plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["57hgk8HW_pC_","MBzYEoW5WjG_","VK2eEmIhJDai","355czYm-o-QH","VJR6rC2uibOw","JGpCfVNfifkF"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0752a00183c64d9b86c2979d73ee25e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dae58497ab641b09eaa2a253dd393c3","placeholder":"​","style":"IPY_MODEL_ff2ba07063ee427299a1430f9fbfc6ea","value":"model-00003-of-00003.safetensors: 100%"}},"0824a865c0ea4f52a763742f6ced64c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a07b601e5694cf48f25a316f76d7a76":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ccbee7735904b5d82f197b6d0747c98":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dae58497ab641b09eaa2a253dd393c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0efa7482772f4b74b52701c14de3176c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f2510c730e24e52941ce325080592cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1096ee5ce1ef4f74a4b1cc9143127403":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12cd7047a5c54d6486e83e2c0a01f015":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"178246b4898f4474beaf28a8f944e7cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17bfd2e0cb3d459e9ad58834b3f6869e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2359a187b3394c09b8031707aa974ebc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6c9ebf42670480b93b95056cef6983a","placeholder":"​","style":"IPY_MODEL_eb8490d21bac4a75bb5a777e8d22bfab","value":"model-00002-of-00003.safetensors: 100%"}},"23d53214d58c41eea95355ae35a56eda":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca65e1341deb42c98db598f9134c3303","placeholder":"​","style":"IPY_MODEL_9c8b77fa1841456c81f72ecb197b63fb","value":"model-00001-of-00003.safetensors: 100%"}},"253ed70d5b4149cdb58619b1974ed1ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"29da4326c15d4b92b83870960d387010":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a07b601e5694cf48f25a316f76d7a76","max":4967447568,"min":0,"orientation":"horizontal","style":"IPY_MODEL_253ed70d5b4149cdb58619b1974ed1ee","value":4967447568}},"2a91de26d6aa49fdb29ac9f4200fc25f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35e8c3ae84ee4722bcc932a21bbae8a0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0efa7482772f4b74b52701c14de3176c","placeholder":"​","style":"IPY_MODEL_db0a2ba6f43343449e2b9dce9611fb28","value":" 11.4M/11.4M [00:01&lt;00:00, 6.65MB/s]"}},"38270cf0ec3c43608ee6e443dcd0a893":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a4809f0001646c0ab767ef0bd017fef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db309406b1254fc6811f02a2aaf0a057","IPY_MODEL_f56a778c3d2a45d08a34489d1c21d7fc","IPY_MODEL_cba192c28db44cec9434691a77dfe79c"],"layout":"IPY_MODEL_b6fbfbed0211459a8fc0d9d2b1d607c2"}},"40a0b050829940c3b593ab6ff2e5d1e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4135157c1d804e94947ee0b8eed4b9ed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_79b6995f9e5c4d8c8bdaf6df1c75c29a","max":236,"min":0,"orientation":"horizontal","style":"IPY_MODEL_af57b503abee4a7b843a1e531c1aefbd","value":236}},"43c2dcb9dfa24d08b220c0a6148f77c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6ea58ddcce84a25ad9af005b2510cb2","placeholder":"​","style":"IPY_MODEL_609cfffdb0604df29ff4e1fd71b6c578","value":"model.safetensors.index.json: 100%"}},"43d7def3f91447a8a159198a79a56d2a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44b050dc6ac842feb9d7e5836f95429b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ccbee7735904b5d82f197b6d0747c98","placeholder":"​","style":"IPY_MODEL_0f2510c730e24e52941ce325080592cb","value":"Loading checkpoint shards: 100%"}},"4c610cbdd3ef4ab380561ff181c694c6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f90974ee917b4ac6b32f8eec0dea6b39","max":11422778,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8bfddbe72ee94433b3674a5c27ea8ceb","value":11422778}},"4f107ec4d633447a9326fd7e6c8fb813":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"532da8e4c46145e5b3c334d787e73184":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c902d3d624e24f629a77ac7a0de612bc","placeholder":"​","style":"IPY_MODEL_7dc9c7909dd048dd81d9db1a5adc0bbd","value":" 236/236 [00:00&lt;00:00, 28.4kB/s]"}},"55194d41ce7242e294584b09aaa33859":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"563e94e1db65459b86db501ab5ad29de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17bfd2e0cb3d459e9ad58834b3f6869e","placeholder":"​","style":"IPY_MODEL_9cbb6745209b4b4a8872a3a803c330f3","value":" 4.94G/4.94G [00:23&lt;00:00, 555MB/s]"}},"5656321d931f460cbf9bb816755dda4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57058c2b65a1490782c98279fa518a09":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0752a00183c64d9b86c2979d73ee25e4","IPY_MODEL_7dd73dba05f94fbf99f849e20259cbb6","IPY_MODEL_d71bc31b01364c7ebb6af35fa6ac2de4"],"layout":"IPY_MODEL_a410f703ec494b24a0d22fcad7d2c718"}},"59ac5eb554214e56a88de281fef2728f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"609cfffdb0604df29ff4e1fd71b6c578":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62cd6f4b08cc4de88d8da6bb118f9c22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_44b050dc6ac842feb9d7e5836f95429b","IPY_MODEL_a087c2a202114633b1f5327c0e968737","IPY_MODEL_de6f8f182c4d4ea0843cf4a03f4e9771"],"layout":"IPY_MODEL_4f107ec4d633447a9326fd7e6c8fb813"}},"6f587a743ecb412f8a8baf700a648848":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fac066a886d4b6d9c00314b04424706":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"720228dc54bc46d1b29205410850b033":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75a8313258614396be18ace8509582b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a91de26d6aa49fdb29ac9f4200fc25f","placeholder":"​","style":"IPY_MODEL_6fac066a886d4b6d9c00314b04424706","value":"generation_config.json: 100%"}},"76c064c70b9a4872a30eae66f02f4753":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf56cd4e1d6f4d8f907d57c0914fbfa2","placeholder":"​","style":"IPY_MODEL_40a0b050829940c3b593ab6ff2e5d1e7","value":" 182k/182k [00:00&lt;00:00, 739kB/s]"}},"79b6995f9e5c4d8c8bdaf6df1c75c29a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b293025a01c487184e3923b9750153e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7dc9c7909dd048dd81d9db1a5adc0bbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7dd73dba05f94fbf99f849e20259cbb6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2c15a0b46be43daa8c8e45225bf9f09","max":4350046915,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1096ee5ce1ef4f74a4b1cc9143127403","value":4350046915}},"7f8950f8e14f47bf834f00f1feef2f4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"898c7aa44a61421896f9dc52b32c4f6c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89e21db494784a13bb3c0f1ac3cd3f8d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8bfddbe72ee94433b3674a5c27ea8ceb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d13ce5ab8424161aa3b0f5a5e3da0e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d75a97d43b7449ebb6de38c73df4cf7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9928e6506d144835bb23099ac4273c5e","placeholder":"​","style":"IPY_MODEL_59ac5eb554214e56a88de281fef2728f","value":"special_tokens_map.json: 100%"}},"8daa6bf63e6f4fd3abfd1a047f5857a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9031da16fc8241c8bcc5ff9faa1c25e6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_43d7def3f91447a8a159198a79a56d2a","max":472,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b293025a01c487184e3923b9750153e","value":472}},"9111563aceda495ea3373f406c42e52d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75a8313258614396be18ace8509582b5","IPY_MODEL_4135157c1d804e94947ee0b8eed4b9ed","IPY_MODEL_532da8e4c46145e5b3c334d787e73184"],"layout":"IPY_MODEL_d48e0838e8944a1099df1bf49af9902e"}},"9394d1bd90514806b1d36bbd8d7226d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9803dd27aef04ed3b9824bb7102886ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23d53214d58c41eea95355ae35a56eda","IPY_MODEL_b780a36a3ea246aa9f8dc6dc4100b367","IPY_MODEL_563e94e1db65459b86db501ab5ad29de"],"layout":"IPY_MODEL_55194d41ce7242e294584b09aaa33859"}},"9928e6506d144835bb23099ac4273c5e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c8b77fa1841456c81f72ecb197b63fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9cbb6745209b4b4a8872a3a803c330f3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f5f356f76e146b4aaf4d97ce97a6270":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a087c2a202114633b1f5327c0e968737":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8daa6bf63e6f4fd3abfd1a047f5857a1","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab656ff5575940d3a30c8cf7e239631a","value":3}},"a15419a9a4e04d92a5ff9b0208c94d48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2359a187b3394c09b8031707aa974ebc","IPY_MODEL_29da4326c15d4b92b83870960d387010","IPY_MODEL_e5871d00596346c8bda863a97e106fc5"],"layout":"IPY_MODEL_e30074f251ee42fca775950cb0073988"}},"a410f703ec494b24a0d22fcad7d2c718":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5c6ab15b4fa485496d010863539ec52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_720228dc54bc46d1b29205410850b033","max":182401,"min":0,"orientation":"horizontal","style":"IPY_MODEL_89e21db494784a13bb3c0f1ac3cd3f8d","value":182401}},"ab656ff5575940d3a30c8cf7e239631a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af57b503abee4a7b843a1e531c1aefbd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b1249e86d93043d38854703cd1289dfa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa23568eac23446aa17e36b670db2ce8","placeholder":"​","style":"IPY_MODEL_178246b4898f4474beaf28a8f944e7cd","value":"tokenizer.json: 100%"}},"b6c9ebf42670480b93b95056cef6983a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6fbfbed0211459a8fc0d9d2b1d607c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b780a36a3ea246aa9f8dc6dc4100b367":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e3c2d518fee34df187e83afc3224fd3c","max":4941037119,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e8cc49716c2e44a0bc81aa3e7759f6dd","value":4941037119}},"bf56cd4e1d6f4d8f907d57c0914fbfa2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2c15a0b46be43daa8c8e45225bf9f09":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c442ee20120d411aac058cb14ecd1d50":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12cd7047a5c54d6486e83e2c0a01f015","placeholder":"​","style":"IPY_MODEL_daa4842fa31b4ae2b339c24e668e9e19","value":" 472/472 [00:00&lt;00:00, 57.1kB/s]"}},"c6ea58ddcce84a25ad9af005b2510cb2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c902d3d624e24f629a77ac7a0de612bc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c997e3afe6af48d78a43bcb0f9d805b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca65e1341deb42c98db598f9134c3303":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cba192c28db44cec9434691a77dfe79c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf589c973cfe4d92a527243bcd7ac737","placeholder":"​","style":"IPY_MODEL_0824a865c0ea4f52a763742f6ced64c1","value":" 6.78k/6.78k [00:00&lt;00:00, 861kB/s]"}},"cf589c973cfe4d92a527243bcd7ac737":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d261bc71b45d40df94e1ff0038ffc45d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d331a9a675a74c7faee91bad2659246c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d48e0838e8944a1099df1bf49af9902e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d71bc31b01364c7ebb6af35fa6ac2de4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e685393f4d6e46b6bfeb08a3b2674e8b","placeholder":"​","style":"IPY_MODEL_c997e3afe6af48d78a43bcb0f9d805b0","value":" 4.35G/4.35G [00:19&lt;00:00, 221MB/s]"}},"d8ce0dad3c8548708cd7722ea2e256df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d75a97d43b7449ebb6de38c73df4cf7","IPY_MODEL_9031da16fc8241c8bcc5ff9faa1c25e6","IPY_MODEL_c442ee20120d411aac058cb14ecd1d50"],"layout":"IPY_MODEL_8d13ce5ab8424161aa3b0f5a5e3da0e1"}},"daa4842fa31b4ae2b339c24e668e9e19":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db0a2ba6f43343449e2b9dce9611fb28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db309406b1254fc6811f02a2aaf0a057":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d331a9a675a74c7faee91bad2659246c","placeholder":"​","style":"IPY_MODEL_9394d1bd90514806b1d36bbd8d7226d9","value":"tokenizer_config.json: 100%"}},"de6f8f182c4d4ea0843cf4a03f4e9771":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d261bc71b45d40df94e1ff0038ffc45d","placeholder":"​","style":"IPY_MODEL_898c7aa44a61421896f9dc52b32c4f6c","value":" 3/3 [00:04&lt;00:00,  1.60s/it]"}},"deb48c3656054abd8bb022564fd2940c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e30074f251ee42fca775950cb0073988":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3c2d518fee34df187e83afc3224fd3c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5871d00596346c8bda863a97e106fc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f587a743ecb412f8a8baf700a648848","placeholder":"​","style":"IPY_MODEL_7f8950f8e14f47bf834f00f1feef2f4a","value":" 4.97G/4.97G [00:47&lt;00:00, 369MB/s]"}},"e685393f4d6e46b6bfeb08a3b2674e8b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8cc49716c2e44a0bc81aa3e7759f6dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8fc7a0dd21840c4aa7e95fd37d312c8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b1249e86d93043d38854703cd1289dfa","IPY_MODEL_4c610cbdd3ef4ab380561ff181c694c6","IPY_MODEL_35e8c3ae84ee4722bcc932a21bbae8a0"],"layout":"IPY_MODEL_9f5f356f76e146b4aaf4d97ce97a6270"}},"eb8490d21bac4a75bb5a777e8d22bfab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f56a778c3d2a45d08a34489d1c21d7fc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_deb48c3656054abd8bb022564fd2940c","max":6782,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5656321d931f460cbf9bb816755dda4c","value":6782}},"f90974ee917b4ac6b32f8eec0dea6b39":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa23568eac23446aa17e36b670db2ce8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fec76fe1626f4faa986607fecd5bb6f9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_43c2dcb9dfa24d08b220c0a6148f77c1","IPY_MODEL_a5c6ab15b4fa485496d010863539ec52","IPY_MODEL_76c064c70b9a4872a30eae66f02f4753"],"layout":"IPY_MODEL_38270cf0ec3c43608ee6e443dcd0a893"}},"ff2ba07063ee427299a1430f9fbfc6ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}